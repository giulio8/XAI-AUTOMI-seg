{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12289924,"sourceType":"datasetVersion","datasetId":7037513}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import Packages for the Environment","metadata":{"id":"THifmOYu9Cip"}},{"cell_type":"code","source":"# Import basic packages for later use\nimport os\nimport shutil\nfrom collections import OrderedDict\n\nimport json\nimport matplotlib.pyplot as plt\nimport nibabel as nib\n\nimport numpy as np\nimport torch","metadata":{"id":"k-hj38QV_raZ","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:27.123369Z","iopub.execute_input":"2025-07-16T22:36:27.123717Z","iopub.status.idle":"2025-07-16T22:36:27.128322Z","shell.execute_reply.started":"2025-07-16T22:36:27.123692Z","shell.execute_reply":"2025-07-16T22:36:27.127523Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"!pip install nnunetv2\n!pip install captum","metadata":{"id":"c4KbMYcEWZjU","outputId":"ba9a5149-e800-477a-9717-4f4796903bc1","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:27.129733Z","iopub.execute_input":"2025-07-16T22:36:27.129996Z","iopub.status.idle":"2025-07-16T22:36:34.522338Z","shell.execute_reply.started":"2025-07-16T22:36:27.129975Z","shell.execute_reply":"2025-07-16T22:36:34.521281Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nnunetv2 in /usr/local/lib/python3.11/dist-packages (2.6.2)\nRequirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.6.0+cu124)\nRequirement already satisfied: acvl-utils<0.3,>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.2.5)\nRequirement already satisfied: dynamic-network-architectures<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (4.67.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.15.2)\nRequirement already satisfied: batchgenerators>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.25.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.2.2)\nRequirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.25.2)\nRequirement already satisfied: SimpleITK>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.5.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.2.3)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.20.3)\nRequirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2025.3.30)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.32.3)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (5.3.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.12.2)\nRequirement already satisfied: imagecodecs in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2025.3.30)\nRequirement already satisfied: yacs in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.1.8)\nRequirement already satisfied: batchgeneratorsv2>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.3.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.8.1)\nRequirement already satisfied: blosc2>=3.0.0b1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (3.2.1)\nRequirement already satisfied: connected-components-3d in /usr/local/lib/python3.11/dist-packages (from acvl-utils<0.3,>=0.2.3->nnunetv2) (3.24.0)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (11.1.0)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (1.0.0)\nRequirement already satisfied: unittest2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (1.1.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (3.6.0)\nRequirement already satisfied: fft-conv-pytorch in /usr/local/lib/python3.11/dist-packages (from batchgeneratorsv2>=0.3.0->nnunetv2) (1.2.0)\nRequirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (1.9.2)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (1.1.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (4.3.8)\nRequirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (2.10.2)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (9.0.0)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.0.15)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2.4.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (2.37.0)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (25.0)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (4.13.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.2->nnunetv2) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (2.9.0.post0)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->nnunetv2) (6.5.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunetv2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunetv2) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (2025.4.26)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nnunetv2) (1.5.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs->nnunetv2) (6.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->nnunetv2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->nnunetv2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->nnunetv2) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->nnunetv2) (2024.2.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.21.0+cu124)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.31.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.5.3)\nCollecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: traceback2 in /usr/local/lib/python3.11/dist-packages (from unittest2->batchgenerators>=0.25.1->nnunetv2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->nnunetv2) (2024.2.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.1.0)\nRequirement already satisfied: linecache2 in /usr/local/lib/python3.11/dist-packages (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2) (1.0.0)\nUsing cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nInstalling collected packages: argparse\nSuccessfully installed argparse-1.4.0\nRequirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.8.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.7.2)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (25.0)\nRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->captum) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->captum) (2024.2.0)\n","output_type":"stream"}],"execution_count":125},{"cell_type":"markdown","source":"# 2. Mount the dataset","metadata":{"id":"tgssjiuNVvq5"}},{"cell_type":"code","source":"from batchgenerators.utilities.file_and_folder_operations import join\n\n# Google Colab\n\"\"\"# for colab users only - mounting the drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive',force_remount = True)\n\ndrive_dir = \"/content/drive/My Drive\"\nmount_dir = join(drive_dir, \"tesi\", \"automi\")\nbase_dir = os.getcwd()\"\"\"\n\n# Kaggle\nmount_dir = \"/kaggle/input/\"\nbase_dir = os.getcwd()\nprint(base_dir)\n!ls '/kaggle/input'\n!cd \"/kaggle/input/automi-seg\" ; ls","metadata":{"id":"_WLi-mVRjbfb","outputId":"e8440113-7302-4a08-9b64-d35179d4e3b4","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:34.523673Z","iopub.execute_input":"2025-07-16T22:36:34.523926Z","iopub.status.idle":"2025-07-16T22:36:35.107997Z","shell.execute_reply.started":"2025-07-16T22:36:34.523902Z","shell.execute_reply":"2025-07-16T22:36:35.106852Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\ncode  nnunet_raw  preprocessed_files  results  segmentation-masked-ROI.nii\n/bin/bash: line 1: cd: /kaggle/input/automi-seg: No such file or directory\nattribution_map-FCC-TP.nii.gz\t\t      dataset-fp.pkl\nattribution_map-FP-Lasso.nii.gz\t\t      dataset-kernel-shap.pkl\nattribution_map-provaLASSO.nii.gz\t      dataset-tp.pkl\nattribution_map-TP-Lasso.nii.gz\t\t      monitor_log.jsonl\ncropped_baseline_output_dictionary_cache.pkl  organ_mask_resampled_to_ct.nii.gz\ncropped_mask_with_RF.nii.gz\t\t      ROI_binary_mask.nii.gz\ncropped_volume_with_RF.nii.gz\t\t      state.db\n","output_type":"stream"}],"execution_count":126},{"cell_type":"markdown","source":"# 3. Setting up nnU-Nets folder structure and environment variables\nnnUnet expects a certain folder structure and environment variables.\n\nRoughly they tell nnUnet:\n1. Where to look for stuff\n2. Where to put stuff\n\nFor more information about this please check: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md","metadata":{"id":"d-CV4Jc4W77P"}},{"cell_type":"markdown","source":"## 3.1 Set environment Variables and creating folders","metadata":{"id":"JAvjVPF0_7t3"}},{"cell_type":"code","source":"# ===========================\n# ðŸ“¦ SETUP nnUNet ENVIRONMENT\n# ===========================\n\n# Definisci i path da settare\npath_dict = {\n    \"nnUNet_raw\": join(mount_dir, \"nnunet_raw\"),\n    \"nnUNet_preprocessed\": join(mount_dir, \"preprocessed_files\"),#\"nnUNet_preprocessed\"),\n    \"nnUNet_results\": join(mount_dir, \"results\"),#\"nnUNet_results\"),\n    # \"RAW_DATA_PATH\": join(mount_dir, \"RawData\"),  # Facoltativo, se ti serve salvare zips\n}\n\n# Scrivi i path nelle variabili di ambiente, che vengono lette dal modulo paths di nnunetv2\nfor env_var, path in path_dict.items():\n    os.environ[env_var] = path\n\nfrom nnunetv2.paths import nnUNet_results, nnUNet_raw\n\nif nnUNet_raw == None:\n    nnUNet_raw = \"/kaggle/input/nnunet_raw\"\nif nnUNet_results == None:\n    nnUNet_results = \"/kaggle/input/results\"\n# Kaggle has some very unconsistent behaviors in dataset mounting...\n#nnUNet_raw = \"/kaggle/input/automi-seg/nnunet_raw\"\n#nnUNet_results = \"/kaggle/input/automi-seg/results\"\nprint(\"nnUNet_raw:\", nnUNet_raw)\nprint(\"nnUNet_results:\", nnUNet_results)","metadata":{"id":"3rlqq-V-CWh8","outputId":"e16ea0fc-57b8-4478-e8df-14942c962c9c","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:35.110437Z","iopub.execute_input":"2025-07-16T22:36:35.110749Z","iopub.status.idle":"2025-07-16T22:36:35.117652Z","shell.execute_reply.started":"2025-07-16T22:36:35.110724Z","shell.execute_reply":"2025-07-16T22:36:35.116922Z"}},"outputs":[{"name":"stdout","text":"nnUNet_raw: /kaggle/input/nnunet_raw\nnnUNet_results: /kaggle/input/results\n","output_type":"stream"}],"execution_count":127},{"cell_type":"markdown","source":"### Some tests","metadata":{}},{"cell_type":"code","source":"ct_img_path = join(nnUNet_raw, \"imagesTr\", \"AUTOMI_00039_0000.nii\")\norgan_mask_path = join(nnUNet_raw, \"total_segmentator_structures\", \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\")\nct_img = nib.load(ct_img_path)\norgan_mask = nib.load(organ_mask_path)","metadata":{"id":"RO5hP5Ao79QG","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:35.118304Z","iopub.execute_input":"2025-07-16T22:36:35.118487Z","iopub.status.idle":"2025-07-16T22:36:35.157170Z","shell.execute_reply.started":"2025-07-16T22:36:35.118467Z","shell.execute_reply":"2025-07-16T22:36:35.156654Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"print(\"CT shape:\", ct_img.shape)\nprint(\"Organ shape:\", organ_mask.shape)\nprint(\"Spacing:\", ct_img.header.get_zooms())\nprint(\"Organ spacing:\", organ_mask.header.get_zooms())","metadata":{"id":"0RGc0dNi9Wop","outputId":"770675ee-5426-493d-a037-b7fdf58afe5d","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:35.157891Z","iopub.execute_input":"2025-07-16T22:36:35.158097Z","iopub.status.idle":"2025-07-16T22:36:35.163065Z","shell.execute_reply.started":"2025-07-16T22:36:35.158082Z","shell.execute_reply":"2025-07-16T22:36:35.162300Z"}},"outputs":[{"name":"stdout","text":"CT shape: (512, 512, 283)\nOrgan shape: (512, 512, 283)\nSpacing: (1.171875, 1.171875, 5.0)\nOrgan spacing: (1.171875, 1.171875, 5.0)\n","output_type":"stream"}],"execution_count":129},{"cell_type":"markdown","source":"## Re-align CT scan with its own organ segmentation mask","metadata":{}},{"cell_type":"code","source":"import SimpleITK as sitk\n\n# Load CT and misaligned organ mask\nct = sitk.ReadImage(ct_img_path, sitk.sitkFloat32)\norgan_mask = sitk.ReadImage(organ_mask_path, sitk.sitkUInt8)\n\n# Resample organ mask to match CT space\nresampler = sitk.ResampleImageFilter()\nresampler.SetReferenceImage(ct)\nresampler.SetInterpolator(sitk.sitkNearestNeighbor)\norgan_resampled = resampler.Execute(organ_mask)\n\n# Save aligned output\nsitk.WriteImage(organ_resampled, \"organ_mask_resampled_to_ct.nii.gz\")","metadata":{"id":"2UGYxNOXBhmN","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:35.163837Z","iopub.execute_input":"2025-07-16T22:36:35.164057Z","iopub.status.idle":"2025-07-16T22:36:36.303609Z","shell.execute_reply.started":"2025-07-16T22:36:35.164043Z","shell.execute_reply":"2025-07-16T22:36:36.303015Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"#organ_mask_path = join(nnUNet_raw, \"organ_mask_resampled_to_ct.nii.gz\")\norgan_mask_path = \"organ_mask_resampled_to_ct.nii.gz\"\nct_img = nib.load(ct_img_path)\norgan_mask = nib.load(organ_mask_path)\nprint(\"CT shape:\", ct_img.shape)\nprint(\"Organ shape:\", organ_mask.shape)\nprint(\"Spacing:\", ct_img.header.get_zooms())\nprint(\"Organ spacing:\", organ_mask.header.get_zooms())","metadata":{"id":"Fq-CUTS2Omeq","outputId":"1ab8dd02-93d3-4430-9a08-ec72940d36a0","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.304418Z","iopub.execute_input":"2025-07-16T22:36:36.304712Z","iopub.status.idle":"2025-07-16T22:36:36.313535Z","shell.execute_reply.started":"2025-07-16T22:36:36.304692Z","shell.execute_reply":"2025-07-16T22:36:36.313023Z"}},"outputs":[{"name":"stdout","text":"CT shape: (512, 512, 283)\nOrgan shape: (512, 512, 283)\nSpacing: (1.171875, 1.171875, 5.0)\nOrgan spacing: (1.171875, 1.171875, 5.0)\n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"# model directory; note that this is readonly in Kaggle environment\nmodel_dir = join(nnUNet_results, 'Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.315861Z","iopub.execute_input":"2025-07-16T22:36:36.316324Z","iopub.status.idle":"2025-07-16T22:36:36.327838Z","shell.execute_reply.started":"2025-07-16T22:36:36.316307Z","shell.execute_reply":"2025-07-16T22:36:36.327112Z"}},"outputs":[],"execution_count":132},{"cell_type":"markdown","source":"## Utility to export logits to a visualizable segmentation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nfrom typing import Union\nfrom pathlib import Path\nfrom nnunetv2.configuration import default_num_processes\nfrom nnunetv2.inference.export_prediction import export_prediction_from_logits\n\ndef export_logits_to_nifty_segmentation(\n    predictor,\n    volume_file: Path,\n    model_dir: str,\n    logits: Union[str, np.ndarray, torch.Tensor],\n    npz_dir: str | None,\n    output_dir: str = \"\",\n    fold: int = 0,\n    save_probs: bool = False,\n    from_file: bool = True\n):\n    \"\"\"\n    Converts a saved .npz logits file into a native-space NIfTI segmentation using nnU-Net's helper.\n\n    Args:\n        predictor: An instantiated nnU-Net predictor object with loaded plans/configs.\n        volume_file: An Path object pointing to the raw image file.\n        model_dir (str): Path to the nnU-Net mode1Introductionl directory containing dataset.json.\n        logits (str or tensor): Base name of the .npz logits file (no extension), when from_file=True\n        npz_dir (str): Directory where the .npz file is stored.\n        output_dir (str): Directory where the .nii.gz segmentation will be saved.\n        fold (int): The fold number used for prediction (default is 0).\n        save_probs (bool): Whether to save softmax probabilities as a .npz file.\n        from_file (bool). Whether to convert from a file instead of from the logits (default true)\n    \"\"\"\n    if from_file:\n        npz_logits = Path(npz_dir) / f\"{logits}.npz\"\n        output_nii = Path(output_dir) / f\"{logits}_seg.nii.gz\"\n        logits = np.load(npz_logits)[\"logits\"]\n    else:\n        output_nii = Path(output_dir) / \"exported_seg.nii.gz\"\n\n    plans_manager = predictor.plans_manager\n    configuration_manager = predictor.configuration_manager\n    dataset_json = Path(model_dir) / \"dataset.json\"\n\n    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n    rw = plans_manager.image_reader_writer_class()\n    if callable(rw) and not hasattr(rw, \"read_images\"):\n        rw = rw()\n    img_np, img_props = rw.read_images([str(volume_file)])\n\n    _, _, data_props = preprocessor.run_case_npy(\n        img_np, seg=None, properties=img_props,\n        plans_manager=plans_manager,\n        configuration_manager=configuration_manager,\n        dataset_json=dataset_json\n    )\n\n\n    export_prediction_from_logits(\n        predicted_array_or_file=logits,\n        properties_dict=data_props,\n        configuration_manager=configuration_manager,\n        plans_manager=plans_manager,\n        dataset_json_dict_or_file=str(dataset_json),\n        output_file_truncated=os.path.splitext(str(output_nii))[0],\n        save_probabilities=save_probs,\n        num_threads_torch=default_num_processes\n    )\n\n    print(f\"âœ…  NIfTI segmentation written â†’ {output_nii}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.328508Z","iopub.execute_input":"2025-07-16T22:36:36.328689Z","iopub.status.idle":"2025-07-16T22:36:36.339849Z","shell.execute_reply.started":"2025-07-16T22:36:36.328676Z","shell.execute_reply":"2025-07-16T22:36:36.339353Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"\"\"\"export_logits_to_nifty_segmentation(\n    predictor=predictor,\n    plan=plan,\n    model_dir=Path(model_dir),\n    logits_filename=\"pred_00007\",\n    npz_dir=\"SHAP/shap_run\",\n    output_dir=\"SHAP/shap_run\",\n    fold=0,\n    save_probs=False\n)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.340683Z","iopub.execute_input":"2025-07-16T22:36:36.341017Z","iopub.status.idle":"2025-07-16T22:36:36.359139Z","shell.execute_reply.started":"2025-07-16T22:36:36.340994Z","shell.execute_reply":"2025-07-16T22:36:36.358577Z"}},"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"'export_logits_to_nifty_segmentation(\\n    predictor=predictor,\\n    plan=plan,\\n    model_dir=Path(model_dir),\\n    logits_filename=\"pred_00007\",\\n    npz_dir=\"SHAP/shap_run\",\\n    output_dir=\"SHAP/shap_run\",\\n    fold=0,\\n    save_probs=False\\n)'"},"metadata":{}}],"execution_count":134},{"cell_type":"markdown","source":"## We define a sliding window caching for faster multi-inference scenario, like SHAP","metadata":{}},{"cell_type":"markdown","source":"### Try to override the sliding_window_function","metadata":{}},{"cell_type":"code","source":"from typing import Union\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom queue import Queue\nfrom threading import Thread\nfrom acvl_utils.cropping_and_padding.padding import pad_nd_image\nfrom nnunetv2.utilities.helpers import empty_cache, dummy_context\nfrom nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\nfrom nnunetv2.inference.sliding_window_prediction import compute_gaussian, compute_steps_for_sliding_window\n\nclass CustomNNUNetPredictor(nnUNetPredictor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n    \n    @torch.inference_mode()\n    def predict_sliding_window_return_logits_with_caching(self, input_image: torch.Tensor,\n                                                          perturbation_mask: torch.BoolTensor | None,\n                                                          baseline_prediction_dict: dict) \\\n            -> Union[np.ndarray, torch.Tensor]:\n        \"\"\"\n        Method predict_sliding_window_return_logits taken from official nnunetv2 documentation:\n        https://github.com/MIC-DKFZ/nnUNet/blob/58a3b121a6d1846a978306f6c79a7c005b7d669b/nnunetv2/inference/predict_from_raw_data.py\n        We add a perturbation_mask parameter to check each patch for the actual presence of a perturbation\n        \"\"\"\n        # fallback to original method if perturbation_mask is None\n        if perturbation_mask is None:\n            return self.predict_sliding_window_return_logits(input_image)\n                \n        assert isinstance(input_image, torch.Tensor)\n        self.network = self.network.to(self.device)\n        self.network.eval()\n\n        empty_cache(self.device)\n\n        # DEBUG --------------\n        \"\"\"voxels  = np.prod(input_image.shape[1:])          # (X*Y*Z)\n        bytes_per_voxel = 2                        # fp16\n        needed  = voxels * self.label_manager.num_segmentation_heads * bytes_per_voxel\n        print(f\"â‰ˆ{needed/1e9:.1f} GB per predicted_logits\")\"\"\"\n\n        # Autocast can be annoying\n        # If the device_type is 'cpu' then it's slow as heck on some CPUs (no auto bfloat16 support detection)\n        # and needs to be disabled.\n        # If the device_type is 'mps' then it will complain that mps is not implemented, even if enabled=False\n        # is set. Whyyyyyyy. (this is why we don't make use of enabled=False)\n        # So autocast will only be active if we have a cuda device.\n        with torch.autocast(self.device.type, enabled=True) if self.device.type == 'cuda' else dummy_context():\n            assert input_image.ndim == 4, 'input_image must be a 4D np.ndarray or torch.Tensor (c, x, y, z)'\n\n            if self.verbose:\n                print(f'Input shape: {input_image.shape}')\n                print(\"step_size:\", self.tile_step_size)\n                print(\"mirror_axes:\", self.allowed_mirroring_axes if self.use_mirroring else None)\n                print(f'Perturbation mask shape: {perturbation_mask.shape}')\n\n\n            # if input_image is smaller than tile_size we need to pad it to tile_size.\n            data, slicer_revert_padding = pad_nd_image(input_image, self.configuration_manager.patch_size,\n                                                       'constant', {'value': 0}, True,\n                                                       None)\n\n            # slicers can be applied to both perturbed volume and \n            slicers = self._internal_get_sliding_window_slicers(data.shape[1:])\n\n            if self.perform_everything_on_device and self.device != 'cpu':\n                # behavior changed\n                try:\n                    predicted_logits = self._internal_predict_sliding_window_return_logits(\n                        data, slicers, True, perturbation_mask, baseline_prediction_dict, caching=True\n                    )\n                except RuntimeError as e:\n                    if \"CUDA out of memory\" in str(e):\n                        print(\"âš ï¸  CUDA OOM, cambiare batch size o patch size!\")\n                        raise\n                    else:\n                        # Mostra l'errore reale e aborta: niente CPU fallback\n                        raise\n            else:\n                predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n                                                                                       self.perform_everything_on_device)\n\n            empty_cache(self.device)\n            # revert padding\n            predicted_logits = predicted_logits[(slice(None), *slicer_revert_padding[1:])]\n        return predicted_logits\n                \n\n    def _slice_key(self, slicer_tuple):\n        # make slicer object hashable to use it for cache lookup\n        return tuple((s.start, s.stop, s.step) for s in slicer_tuple)\n\n\n    @torch.inference_mode()\n    def _internal_predict_sliding_window_return_logits(self,\n                                                       data: torch.Tensor,\n                                                       slicers,\n                                                       do_on_device: bool = True,\n                                                       perturbation_mask: torch.BoolTensor | None = None,\n                                                       baseline_prediction_dict: dict | None = None,\n                                                       caching: bool = False,\n                                                       ):\n        \"\"\"\n        Modified to manage the caching of patches\n        \"\"\"\n        predicted_logits = n_predictions = prediction = gaussian = workon = None\n        results_device = self.device if do_on_device else torch.device('cpu')\n        if next(self.network.parameters()).device != results_device:\n            self.network = self.network.to(results_device)\n\n        def producer(d, slh, q):\n            for s in slh:\n                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(results_device), s))\n            q.put('end')\n\n        try:\n            empty_cache(self.device)\n\n            # move data to device\n            if self.verbose:\n                print(f'move image to device {results_device}')\n            data = data.to(results_device)\n            queue = Queue(maxsize=2)\n            t = Thread(target=producer, args=(data, slicers, queue))\n            t.start()\n\n            # preallocate arrays\n            if self.verbose:\n                print(f'preallocating results arrays on device {results_device}')\n            predicted_logits = torch.zeros((self.label_manager.num_segmentation_heads, *data.shape[1:]),\n                                           dtype=torch.half,\n                                           device=results_device)\n            n_predictions = torch.zeros(data.shape[1:], dtype=torch.half, device=results_device)\n\n            if self.use_gaussian:\n                gaussian = compute_gaussian(tuple(self.configuration_manager.patch_size), sigma_scale=1. / 8,\n                                            value_scaling_factor=10,\n                                            device=results_device)\n            else:\n                gaussian = 1\n\n        \n\n            if not self.allow_tqdm and self.verbose:\n                print(f'running prediction: {len(slicers)} steps')\n\n            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n                cache_hits = 0\n                while True:\n                    item = queue.get()\n                    if item == 'end':\n                        queue.task_done()\n                        break\n                    workon, sl = item\n                    try:\n                        if caching and not self.check_overlapping(sl, perturbation_mask):\n                            prediction = baseline_prediction_dict[self._slice_key(sl)].to(results_device)\n                            cache_hits += 1\n                        else:\n                            prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n                    except Exception as e:\n                        raise RuntimeError(\"Errore nella predizione del patch\") from e\n\n                    # 2) sanity-check device\n                    assert prediction.device == predicted_logits.device\n\n                    if self.use_gaussian:\n                        prediction *= gaussian\n                    predicted_logits[sl] += prediction\n                    n_predictions[sl[1:]] += gaussian\n\n                    # free up gpu memory\n                    del prediction, workon\n                    \n                    queue.task_done()\n                    pbar.set_postfix(\n                        cache=f\"{cache_hits}\",\n                        mem=f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\"\n                    )\n                    pbar.update(1)\n            queue.join()\n            if self.verbose and not self.allow_tqdm:\n                print(f\"Cache hits: {cache_hits}\\\\{len(slicers)}\")\n            \n\n            # predicted_logits /= n_predictions\n            torch.div(predicted_logits, n_predictions, out=predicted_logits)\n            # check for infs\n            if torch.any(torch.isinf(predicted_logits)):\n                raise RuntimeError('Encountered inf in predicted array. Aborting... If this problem persists, '\n                                   'reduce value_scaling_factor in compute_gaussian or increase the dtype of '\n                                   'predicted_logits to fp32')\n        except Exception as e:\n            del predicted_logits, n_predictions, prediction, gaussian, workon\n            empty_cache(self.device)\n            empty_cache(results_device)\n            raise e\n        return predicted_logits\n  \n\n\n    def get_output_dictionary_sliding_window(self, data: torch.Tensor, slicers,\n                                            do_on_device: bool = True,\n                                            ) -> torch.Tensor:\n        \"\"\"\n        # create a dictionary that associates the output of the inference, to each slicer of the sliding window module\n        # this way we can set ready for cache the output for the untouched patches.\n        \"\"\"\n        \n        dictionary = dict()\n        prediction = workon = None\n        results_device = self.device if do_on_device else torch.device('cpu')\n        if next(self.network.parameters()).device != results_device:\n            self.network = self.network.to(results_device)\n\n        def producer(d, slh, q):\n            for s in slh:\n                #tqdm.write(f\"put patch {s} on queue\")    # dentro producer\n                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(self.device), s))\n            q.put('end')\n\n        try:\n            empty_cache(self.device)\n\n            # move data and network to device\n            if self.verbose:\n                print(f'move image and model to device {results_device}')\n\n            self.network = self.network.to(results_device)\n            data = data.to(results_device)\n            queue = Queue(maxsize=2)\n            t = Thread(target=producer, args=(data, slicers, queue))\n            t.start()\n\n            if not self.allow_tqdm and self.verbose:\n                print(f'running prediction: {len(slicers)} steps')\n\n            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n                while True:\n                    item = queue.get()\n                    if item == 'end':\n                        queue.task_done()\n                        break\n                    workon, sl = item\n                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n\n                    pred_cpu = pred_gpu.cpu()\n                    # save prediction in the dictionary\n                    dictionary[self._slice_key(sl)] = pred_cpu\n                    # immediately free gpu memory\n                    del pred_gpu\n                    \n                    queue.task_done()\n                    pbar.update()\n            queue.join()\n\n        except Exception as e:\n            del workon#, prediction\n            empty_cache(self.device)\n            empty_cache(results_device)\n            raise e\n        return dictionary\n\n        try:\n            empty_cache(self.device)\n\n            # move data and network to device\n            if self.verbose:\n                print(f'move image and model to device {results_device}')\n\n            self.network = self.network.to(results_device)\n            data = data.to(results_device)\n            queue = Queue(maxsize=2)\n            t = Thread(target=producer, args=(data, slicers, queue))\n            t.start()\n\n            if not self.allow_tqdm and self.verbose:\n                print(f'running prediction: {len(slicers)} steps')\n\n            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n                while True:\n                    item = queue.get()\n                    if item == 'end':\n                        queue.task_done()\n                        break\n                    workon, sl = item\n                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n\n                    pred_cpu = pred_gpu.cpu()\n                    # save prediction in the dictionary\n                    dictionary[self._slice_key(sl)] = pred_cpu\n                    # immediately free gpu memory\n                    del pred_gpu\n                    \n                    queue.task_done()\n                    pbar.update()\n            queue.join()\n\n        except Exception as e:\n            del workon#, prediction\n            empty_cache(self.device)\n            empty_cache(results_device)\n            raise e\n        return dictionary\n\n    def check_overlapping(self, slicer, perturbation_mask: torch.BoolTensor) -> bool:\n        \"\"\"\n        Restituisce True se la patch definita da `slicer`\n        contiene almeno un voxel perturbato.\n    \n        Parameters\n        ----------\n        slicer : tuple\n            Quello prodotto da `_internal_get_sliding_window_slicers`,\n            cioÃ¨ (slice(None), slice(x0,x1), slice(y0,y1), slice(z0,z1)).\n        perturbation_mask : torch.BoolTensor\n            Maschera (C, X, Y, Z) con True nei voxel da perturbare\n            (di solito C==1 o replicata sui canali).\n    \n        Returns\n        -------\n        bool\n            True â†” almeno un voxel True nella patch.out\n        \"\"\"\n        # NB: il primo elemento del tuple Ã¨ sempre slice(None) (canali).\n        #     Lo manteniamo: non ha overhead e semplifica.\n        return perturbation_mask[slicer].any().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.360041Z","iopub.execute_input":"2025-07-16T22:36:36.360644Z","iopub.status.idle":"2025-07-16T22:36:36.388994Z","shell.execute_reply.started":"2025-07-16T22:36:36.360586Z","shell.execute_reply":"2025-07-16T22:36:36.388429Z"}},"outputs":[],"execution_count":135},{"cell_type":"markdown","source":"## Try Captum's kernel SHAP on the organ mask","metadata":{}},{"cell_type":"markdown","source":"### first derive a customized class from Captum library, to use sliding window caching","metadata":{}},{"cell_type":"markdown","source":"## Try to customize KernelShap as a \"sibling\", so let's inherit the parent, LimeBase\nthat's because we need to override (to-and-from)/interpret_rep_transform methods used to map the (1,M) binary mask vector with the perturbed volume AND the perturbation mask we need for caching","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n\n# pyre-strict\nimport inspect\nimport math\nimport typing\nimport warnings\nfrom collections.abc import Iterator\nfrom typing import Any, Callable, cast, List, Literal, Optional, Tuple, Union\n\nimport torch\nfrom captum._utils.common import (\n    _expand_additional_forward_args,\n    _expand_target,\n    _flatten_tensor_or_tuple,\n    _format_output,\n    _format_tensor_into_tuples,\n    _get_max_feature_index,\n    _is_tuple,\n    _reduce_list,\n    _run_forward,\n)\nfrom captum._utils.models.linear_model import SkLearnLasso\nfrom captum._utils.models.model import Model\nfrom captum._utils.progress import progress\nfrom captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\nfrom captum.attr._utils.attribution import PerturbationAttribution\nfrom captum.attr._utils.batching import _batch_example_iterator\nfrom captum.attr._utils.common import (\n    _construct_default_feature_mask,\n    _format_input_baseline,\n)\nfrom captum.log import log_usage\nfrom torch import Tensor, BoolTensor\nfrom torch.nn import CosineSimilarity\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nclass LimeBaseWithCustomArgumentToForwardFunc(PerturbationAttribution):\n    r\"\"\"\n    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_func: Callable[..., Tensor],\n        interpretable_model: Model,\n        similarity_func: Callable[\n            ...,\n            Union[float, Tensor],\n        ],\n        perturb_func: Callable[..., object],\n        perturb_interpretable_space: bool,\n        from_interp_rep_transform: Optional[\n            Callable[..., Union[Tensor, Tuple[Tensor, ...]]]\n        ],\n        to_interp_rep_transform: Optional[Callable[..., Tensor]],\n    ) -> None:\n        r\"\"\"\n\n        Args:\n\n\n            forward_func (Callable): The forward function of the model or any\n                    modification of it. If a batch is provided as input for\n                    attribution, it is expected that forward_func returns a scalar\n                    representing the entire batch.\n            interpretable_model (Model): Model object to train interpretable model.\n                    A Model object provides a `fit` method to train the model,\n                    given a dataloader, with batches containing three tensors:\n\n                    - interpretable_inputs: Tensor\n                      [2D num_samples x num_interp_features],\n                    - expected_outputs: Tensor [1D num_samples],\n                    - weights: Tensor [1D num_samples]\n\n                    The model object must also provide a `representation` method to\n                    access the appropriate coefficients or representation of the\n                    interpretable model after fitting.\n                    Some predefined interpretable linear models are provided in\n                    captum._utils.models.linear_model including wrappers around\n                    SkLearn linear models as well as SGD-based PyTorch linear\n                    models.\n\n                    Note that calling fit multiple times should retrain the\n                    interpretable model, each attribution call reuses\n                    the same given interpretable model object.\n            similarity_func (Callable): Function which takes a single sample\n                    along with its corresponding interpretable representation\n                    and returns the weight of the interpretable sample for\n                    training interpretable model. Weight is generally\n                    determined based on similarity to the original input.\n                    The original paper refers to this as a similarity kernel.\n\n                    The expected signature of this callable is:\n\n                    >>> similarity_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_interpretable_input:\n                    >>>        Tensor [2D 1 x num_interp_features],\n                    >>>    **kwargs: Any\n                    >>> ) -> float or Tensor containing float scalar\n\n                    perturbed_input and original_input will be the same type and\n                    contain tensors of the same shape (regardless of whether or not\n                    the sampling function returns inputs in the interpretable\n                    space). original_input is the same as the input provided\n                    when calling attribute.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n            perturb_func (Callable): Function which returns a single\n                    sampled input, generally a perturbation of the original\n                    input, which is used to train the interpretable surrogate\n                    model. Function can return samples in either\n                    the original input space (matching type and tensor shapes\n                    of original input) or in the interpretable input space,\n                    which is a vector containing the intepretable features.\n                    Alternatively, this function can return a generator\n                    yielding samples to train the interpretable surrogate\n                    model, and n_samples perturbations will be sampled\n                    from this generator.\n\n                    The expected signature of this callable is:\n\n                    >>> perturb_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor, tuple[Tensor, ...], or\n                    >>>    generator yielding tensor or tuple[Tensor, ...]\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n\n                    Returned sampled input should match the input type (Tensor\n                    or Tuple of Tensor and corresponding shapes) if\n                    perturb_interpretable_space = False. If\n                    perturb_interpretable_space = True, the return type should\n                    be a single tensor of shape 1 x num_interp_features,\n                    corresponding to the representation of the\n                    sample to train the interpretable model.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n            perturb_interpretable_space (bool): Indicates whether\n                    perturb_func returns a sample in the interpretable space\n                    (tensor of shape 1 x num_interp_features) or a sample\n                    in the original space, matching the format of the original\n                    input. Once sampled, inputs can be converted to / from\n                    the interpretable representation with either\n                    to_interp_rep_transform or from_interp_rep_transform.\n            from_interp_rep_transform (Callable): Function which takes a\n                    single sampled interpretable representation (tensor\n                    of shape 1 x num_interp_features) and returns\n                    the corresponding representation in the input space\n                    (matching shapes of original input to attribute).\n\n                    This argument is necessary if perturb_interpretable_space\n                    is True, otherwise None can be provided for this argument.\n\n                    The expected signature of this callable is:\n\n                    >>> from_interp_rep_transform(\n                    >>>    curr_sample: Tensor [2D 1 x num_interp_features]\n                    >>>    original_input: Tensor or Tuple of Tensors,\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor or tuple[Tensor, ...]\n\n                    Returned sampled input should match the type of original_input\n                    and corresponding tensor shapes.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n\n            to_interp_rep_transform (Callable): Function which takes a\n                    sample in the original input space and converts to\n                    its interpretable representation (tensor\n                    of shape 1 x num_interp_features).\n\n                    This argument is necessary if perturb_interpretable_space\n                    is False, otherwise None can be provided for this argument.\n\n                    The expected signature of this callable is:\n\n                    >>> to_interp_rep_transform(\n                    >>>    curr_sample: Tensor or Tuple of Tensors,\n                    >>>    original_input: Tensor or Tuple of Tensors,\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor [2D 1 x num_interp_features]\n\n                    curr_sample will match the type of original_input\n                    and corresponding tensor shapes.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n        \"\"\"\n        PerturbationAttribution.__init__(self, forward_func)\n        self.interpretable_model = interpretable_model\n        self.similarity_func = similarity_func\n        self.perturb_func = perturb_func\n        self.perturb_interpretable_space = perturb_interpretable_space\n        self.from_interp_rep_transform = from_interp_rep_transform\n        self.to_interp_rep_transform = to_interp_rep_transform\n\n        if self.perturb_interpretable_space:\n            assert (\n                self.from_interp_rep_transform is not None\n            ), \"Must provide transform from interpretable space to original input space\"\n            \" when sampling from interpretable space.\"\n        else:\n            assert (\n                self.to_interp_rep_transform is not None\n            ), \"Must provide transform from original input space to interpretable space\"\n\n    @log_usage(part_of_slo=True)\n    @torch.no_grad()\n    def attribute(\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        target: TargetType = None,\n        additional_forward_args: Optional[Tuple[object, ...]] = None,\n        n_samples: int = 50,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n        # --- MONITOR CONVERGENCE QUALITY\n        monitor_log_path: str | None = None,\n        monitor_convergence_step: int | None = 20,\n        monitor_local_accuracy_step: int | None = 50,\n        **kwargs: object,\n    ) -> Tensor:\n        r\"\"\"\n        This method attributes the output of the model with given target index\n        (in case it is provided, otherwise it assumes that output is a\n        scalar) to the inputs of the model using the approach described above.\n        It trains an interpretable model and returns a representation of the\n        interpretable model.\n\n        It is recommended to only provide a single example as input (tensors\n        with first dimension or batch size = 1). This is because LIME is generally\n        used for sample-based interpretability, training a separate interpretable\n        model to explain a model's prediction on each individual example.\n\n        A batch of inputs can be provided as inputs only if forward_func\n        returns a single value per batch (e.g. loss).\n        The interpretable feature representation should still have shape\n        1 x num_interp_features, corresponding to the interpretable\n        representation for the full batch, and perturbations_per_eval\n        must be set to 1.\n\n        Args:\n\n            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n                        is computed. If forward_func takes a single\n                        tensor as input, a single input tensor should be provided.\n                        If forward_func takes multiple tensors as input, a tuple\n                        of the input tensors should be provided. It is assumed\n                        that for all given input tensors, dimension 0 corresponds\n                        to the number of examples, and if multiple input tensors\n                        are provided, the examples must be aligned appropriately.\n            target (int, tuple, Tensor, or list, optional): Output indices for\n                        which surrogate model is trained\n                        (for classification cases,\n                        this is usually the target class).\n                        If the network returns a scalar value per example,\n                        no target index is necessary.\n                        For general 2D outputs, targets can be either:\n\n                        - a single integer or a tensor containing a single\n                          integer, which is applied to all input examples\n\n                        - a list of integers or a 1D tensor, with length matching\n                          the number of examples in inputs (dim 0). Each integer\n                          is applied as the target for the corresponding example.\n\n                        For outputs w            except --------ith > 2 dimensions, targets can be either:\n\n                        - A single tuple, which contains #output_dims - 1\n                          elements. This target index is applied to all examples.\n\n                        - A list of tuples with length equal to the number of\n                          examples in inputs (dim 0), and each tuple containing\n                          #output_dims - 1 elements. Each tuple is applied as the\n                          target for the corresponding example.\n\n                        Default: None\n            additional_forward_args (Any, optional): If the forward function\n                        requires additional arguments other than the inputs for\n                        which attributions should not be computed, this argument\n                        can be provided. It must be either a single additional\n                        argument of a Tensor or arbitrary (non-tuple) type or a\n                        tuple containing multiple additional arguments including\n                        tensors or any arbitrary python types. These arguments\n                        are provided to forward_func in order following the\n                        arguments in inputs.\n                        For a tensor, the first dimension of the tensor must\n                        correspond to the number of examples. For all other types,\n                        the given argument is used for all forward evaluations.\n                        Note that attributions are not computed with respect\n                        to these arguments.\n                        Default: None\n            n_samples (int, optional): The number of samples of the original\n                        model used to train the surrogate interpretable model.\n                        Default: `50` if `n_samples` is not provided.\n            perturbations_per_eval (int, optional): Allows multiple samples\n                        to be processed simultaneously in one call to forward_fn.\n                        Each forward pass will contain a maximum of\n                        perturbations_per_eval * #examples samples.\n                        For DataParallel models, each batch is split among the\n                        available devices, so evaluations on each available\n                        device contain at most\n                        (perturbations_per_eval * #examples) / num_devices\n                        samples.\n                        If the forward function returns a single scalar per batch,\n                        perturbations_per_eval must be set to 1.\n                        Default: 1\n            show_progress (bool, optional): Displays the progress of computation.\n                        It will try to use tqdm if available for advanced features\n                        (e.g. time estimation). Otherwise, it will fallback to\n                        a simple output of progress.\n                        Default: False\n            monitor_log_path (str, optional): Path to the log file for monitoring convergence.\n                        if None, no monitoring is performed.\n                        Default: None\n            monitor_convergence_step (int, optional): Number of iterations over which\n                        the difference among two attribution is computerd.\n                        Default: 20\n            monitor_local_accuracy_step (int, optional): Number of iterations over which\n                        the local accuracy of an attribution is computerd.\n                        Default: 50\n            **kwargs (Any, optional): Any additional arguments necessary for\n                        sampling and transformation functions (provided to\n                        constructor).\n                        Default: None\n\n        Returns:\n            **interpretable model representation**:\n            - **interpretable model representation** (*Any*):\n                    A representation of the interpretable model trained. The return\n                    type matches the return type of train_interpretable_model_func.\n                    For example, this could contain coefficients of a\n                    linear surrogate model.\n\n        Examples::\n\n            >>> # SimpleClassifier takes a single input tensor of\n            >>> # float features with size N x 5,\n            >>> # and returns an Nx3 tensor of class probabilities.\n            >>> net = SimpleClassifier()\n            >>>\n            >>> # We will train an interpretable model with the same\n            >>> # features by simply sampling with added Gaussian noise\n            >>> # to the inputs and training a model to predict the\n            >>> # score of the target class.\n            >>>\n            >>> # For interpretable model training, we will use sklearn\n            >>> # linear model in this example. We have provided wrappers\n            >>> # around sklearn linear models to fit the Model interface.\n            >>> # Any arguments provided to the sklearn constructor can also\n            >>> # be provided to the wrapper, e.g.:\n            >>> # SkLearnLinearModel(\"linear_model.Ridge\", alpha=2.0)\n            >>> from captum._utils.models.linear_model import SkLearnLinearModel\n            >>>\n            >>>\n            >>> # Define similarity kernel (exponential kernel based on L2 norm)\n            >>> def similarity_kernel(\n            >>>     original_input: Tensor,\n            >>>     perturbed_input: Tensor,\n            >>>     perturbed_interpretable_input: Tensor,\n            >>>     **kwargs)->Tensor:\n            >>>         # kernel_width will be provided to attribute as a kwarg\n            >>>         kernel_width = kwargs[\"kernel_width\"]\n            >>>         l2_dist = torch.norm(original_input - perturbed_input)\n            >>>         return torch.exp(- (l2_dist**2) / (kernel_width**2))\n            >>>\n            >>>\n            >>> # Define sampling function\n            >>> # This function samples in original input space\n            >>> def perturb_func(\n            >>>     original_input: Tensor,\n            >>>     **kwargs)->Tensor:\n            >>>         return original_input + torch.randn_like(original_input)\n            >>>\n            >>> # For this example, we are setting the interpretable input to\n            >>> # match the model input, so the to_interp_rep_transform\n            >>> # function simply returns the input. In most cases, the interpretable\n            >>> # input will be different and may have a smaller feature set, so\n            >>> # an appropriate transformation function should be provided.\n            >>>\n            >>> def to_interp_transform(curr_sample, original_inp,\n            >>>                                      **kwargs):\n            >>>     return curr_sample\\\n            >>>\n            >>> # Generating random input with size 1 x 5\n            >>> input = torch.randn(1, 5)\n            >>> # Defining LimeBase interpreter\n            >>> lime_attr = LimeBase(net,\n                                     SkLearnLinearModel(\"linear_model.Ridge\"),\n                                     similarity_func=similarity_kernel,\n                                     perturb_func=perturb_func,\n                                     perturb_interpretable_space=False,\n                                     from_interp_rep_transform=None,\n                                     to_interp_rep_transform=to_interp_transform)\n            >>> # Computes interpretable model, returning coefficients of linear\n            >>> # model.\n            >>> attr_coefs = lime_attr.attribute(input, target=1, kernel_width=1.1)\n        \"\"\"\n        inp_tensor = cast(Tensor, inputs) if isinstance(inputs, Tensor) else inputs[0]\n        device = inp_tensor.device\n        \n        # --------------------------------------------------------------------- #\n        # 1.  Lists that grow while we sample                                   #\n        # --------------------------------------------------------------------- #\n        interpretable_inps, similarities, outputs = [], [], []\n        \n        curr_model_inputs = []\n        expanded_additional_args = None\n        expanded_target = None\n        gen_perturb_func = self._get_perturb_generator_func(inputs, **kwargs)\n\n        # --------------------------------------------------------------------- #\n        # 2.  Monitoring initialisation                                         #\n        # --------------------------------------------------------------------- #\n        MONITOR = monitor_log_path is not None\n        if MONITOR:\n            beta_prev = None\n            k_monitor_conv   = monitor_convergence_step\n            k_monitor_delta  = monitor_local_accuracy_step\n            logf = open(monitor_log_path, \"a\")\n        \n        if show_progress:\n            attr_progress = progress(\n                total=math.ceil(n_samples / perturbations_per_eval),\n                desc=f\"{self.get_name()} attribution\",\n            )\n            attr_progress.update(0)\n        \n        feature_mask = kwargs[\"feature_mask\"]\n        batch_count = 0\n\n        # --------------------------------------------------------------------- #\n        # 3.  Main sampling loop                                                #\n        # --------------------------------------------------------------------- #\n        for _ in range(n_samples):\n            try:\n                interpretable_inp, curr_model_input = gen_perturb_func()\n                perturbation_mask = self._get_perturbation_mask(\n                    interpretable_inp, curr_model_input, feature_mask\n                )\n            except StopIteration:\n                warnings.warn(\n                    \"Generator completed prior to given n_samples iterations!\",\n                    stacklevel=1,\n                )\n                break\n            except Exception:\n                print(\"error in the perturbation mask generation\")\n                raise\n        \n            # ------------ Build forward args with mask ------------------------ #\n            if additional_forward_args is None:\n                additional_forward_args_with_mask = (perturbation_mask,)\n            elif isinstance(additional_forward_args, tuple):\n                additional_forward_args_with_mask = additional_forward_args + (perturbation_mask,)\n            else:\n                additional_forward_args_with_mask = (additional_forward_args, perturbation_mask)\n\n            # ------------ Book-keeping per sample ----------------------------- #\n            batch_count += 1\n            interpretable_inps.append(interpretable_inp)\n            curr_model_inputs.append(curr_model_input)\n        \n            curr_sim = self.similarity_func(inputs, curr_model_input, interpretable_inp, **kwargs)\n            similarities.append(\n                curr_sim.flatten()\n                if isinstance(curr_sim, Tensor)\n                else torch.tensor([curr_sim], device=device)\n            )\n\n            # ------------ When we have one evaluation batch ready ------------- #\n            if len(curr_model_inputs) == perturbations_per_eval:\n                expanded_additional_args = _expand_additional_forward_args(\n                    additional_forward_args_with_mask, len(curr_model_inputs)\n                )\n                if expanded_target is None:\n                    expanded_target = _expand_target(target, len(curr_model_inputs))\n        \n                model_out = self._evaluate_batch(\n                    curr_model_inputs,\n                    expanded_target,\n                    expanded_additional_args,\n                    device,\n                )\n                if show_progress:\n                    attr_progress.update()\n                outputs.append(model_out)\n        \n                curr_model_inputs = []\n\n                # =============================================================== #\n                # === MONITORING: re-fit & log when a checkpoint is reached ===== #\n                # =============================================================== #\n                if MONITOR and (\n                    len(interpretable_inps) % k_monitor_conv == 0\n                    or len(interpretable_inps) % k_monitor_delta == 0\n                ):\n                    ### MONITOR BEGIN\n                    # build DataLoader with *all* samples so far\n                    X = torch.cat(interpretable_inps).float()\n                    y = (\n                        torch.cat(outputs)\n                        if len(outputs[0].shape) > 0\n                        else torch.stack(outputs)\n                    ).float()\n                    w = torch.cat(similarities).float()\n        \n                    dl_mon = DataLoader(\n                        TensorDataset(X, y, w), batch_size=len(X)\n                    )\n        \n                    # one API call â†’ fast enough for â‰¤ few 100 samples\n                    self.interpretable_model.fit(dl_mon)\n        \n                    # ---------- obtain coefficients as a clean 1-D tensor -----------------\n                    rep = self.interpretable_model.representation()        # may be Tensor / np / list\n                    \n                    beta_cur = (\n                        rep.flatten().to(\"cpu\")                         # if already a Tensor\n                        if isinstance(rep, torch.Tensor)\n                        else torch.as_tensor(rep, dtype=torch.float32, device=\"cpu\").flatten() # keep these small vectors in CPU\n                    )\n                    phi0, phis = beta_cur[0].item(), beta_cur[1:]          # scalar + 1-D tensor\n                    # ----------------------------------------------------------------------\n                    \n                    # ---- Convergence distance -------------------------------------------\n                    if len(interpretable_inps) % k_monitor_conv == 0 and beta_prev is not None:\n                        dist = torch.norm(beta_cur - beta_prev, p=1).item()\n                        logf.write(json.dumps({\n                            \"iter\": len(interpretable_inps),\n                            \"conv_dist_L1\": dist\n                        }) + \"\\n\")\n\n        \n                    # ---- Local-accuracy residual ------------------------------ #\n                    if len(interpretable_inps) % k_monitor_delta == 0:\n                        # take the output of the unperturbed input (ASSUMES first sample is unperturbed, only true for KernelSHAP)\n                        model_fwd_original = outputs[0]\n                        fx = model_fwd_original.item() if torch.is_tensor(model_fwd_original) else model_fwd_original\n                        delta = abs(fx - (phi0 + sum(phis))).item()\n                        logf.write(json.dumps({\n                            \"iter\": len(interpretable_inps),\n                            \"delta_shap\": delta\n                        }) + \"\\n\")\n        \n                    beta_prev = beta_cur.detach().cpu()\n                    ### MONITOR END\n\n\n        # --------------------------------------------------------------------- #\n        # 4.  Flush any leftover mini-batch                                     #\n        # --------------------------------------------------------------------- #\n        if len(curr_model_inputs) > 0:\n            expanded_additional_args = _expand_additional_forward_args(\n                additional_forward_args_with_mask, len(curr_model_inputs)\n            )\n            expanded_target = _expand_target(target, len(curr_model_inputs))\n        \n            model_out = self._evaluate_batch(\n                curr_model_inputs,\n                expanded_target,\n                expanded_additional_args,\n                device,\n            )\n            if show_progress:\n                attr_progress.update()\n            outputs.append(model_out)\n        \n        if show_progress:\n            attr_progress.close()\n\n        # --------------------------------------------------------------------- #\n        # 5.  Final fit on *all* samples                                        #\n        # --------------------------------------------------------------------- #\n        combined_interp_inps = torch.cat(interpretable_inps).float()\n        combined_outputs = (\n            torch.cat(outputs) if len(outputs[0].shape) > 0 else torch.stack(outputs)\n        ).float()\n        combined_sim = (\n            torch.cat(similarities)\n            if len(similarities[0].shape) > 0\n            else torch.stack(similarities)\n        ).float()\n        \n        self.dataset = TensorDataset(combined_interp_inps, combined_outputs, combined_sim)\n        self.interpretable_model.fit(DataLoader(self.dataset, batch_size=batch_count))\n        \n        if MONITOR:\n            logf.close()\n        \n        return self.interpretable_model.representation()\n\n\n\n    def _get_perturbation_mask(\n        self,\n        interpretable_input: torch.Tensor,       # shape = (B, M)\n        original_inputs: TensorOrTupleOfTensorsGeneric, # shape = (B, C, D, W, H) or tuple thereof\n        feature_mask,\n    ) -> Union[torch.BoolTensor, Tuple[torch.BoolTensor, ...]]:\n        \"\"\"\n        Build a Boolean mask of shape (B, *input_dims) indicating which\n        elements should be perturbed (True) vs. left untouched (False).\n        \"\"\"\n    \n        # Case 1: singleâ€Tensor input\n        if isinstance(feature_mask, torch.Tensor):\n            # advanced indexing over the batch dimension\n            # result has shape (B, *feature_mask.shape)\n            mask = interpretable_input[:, feature_mask]\n            mask = ~mask.bool()\n\n            return mask\n    \n        # Case 2: multiâ€input (tuple) model\n        else:\n            masks = []\n            for fm_i in feature_mask:\n                mask_i = interpretable_input[:, fm_i]  # â†’ (B, *fm_i.shape)\n                masks.append(~mask_i.bool())\n            return tuple(masks)\n\n    \n\n    def _get_perturb_generator_func(\n        self, inputs: TensorOrTupleOfTensorsGeneric, **kwargs: Any\n    ) -> Callable[\n        [], Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n    ]:\n        perturb_generator: Optional[Iterator[TensorOrTupleOfTensorsGeneric]]\n        perturb_generator = None\n        if inspect.isgeneratorfunction(self.perturb_func):\n            perturb_generator = self.perturb_func(inputs, **kwargs)\n\n        def generate_perturbation() -> (\n            Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n        ):\n            if perturb_generator:\n                curr_sample = next(perturb_generator)\n            else:\n                curr_sample = self.perturb_func(inputs, **kwargs)\n\n            if self.perturb_interpretable_space:\n                interpretable_inp = curr_sample\n                curr_model_input = self.from_interp_rep_transform(  # type: ignore\n                    curr_sample, inputs, **kwargs\n                )\n            else:\n                curr_model_input = curr_sample\n                interpretable_inp = self.to_interp_rep_transform(  # type: ignore\n                    curr_sample, inputs, **kwargs\n                )\n\n            return interpretable_inp, curr_model_input  # type: ignore\n\n        return generate_perturbation\n\n    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.)\n    def attribute_future(self) -> Callable:\n        r\"\"\"\n        This method is not implemented for LimeBase.\n        \"\"\"\n        raise NotImplementedError(\n            \"LimeBase does not support attribution of future samples.\"\n        )\n\n    def _evaluate_batch(\n        self,\n        curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n        expanded_target: TargetType,\n        expanded_additional_args: object,\n        device: torch.device,\n    ) -> Tensor:\n        model_out = _run_forward(\n            self.forward_func,\n            #MOMENTANEAOUS---> sliding_window forward function only works with single items (no batch) --> take first\n            #_reduce_list(curr_model_inputs),\n            _reduce_list(curr_model_inputs)[0],\n            expanded_target,\n            expanded_additional_args,\n        )\n        if isinstance(model_out, Tensor):\n            assert model_out.numel() == len(curr_model_inputs), (\n                \"Number of outputs is not appropriate, must return \"\n                \"one output per perturbed input\"\n            )\n        if isinstance(model_out, Tensor):\n            return model_out.flatten()\n        return torch.tensor([model_out], device=device)\n\n    def has_convergence_delta(self) -> bool:\n        return False\n\n    @property\n    def multiplies_by_inputs(self) -> bool:\n        return False\n\n\n# Default transformations and methods\n# for Lime child implementation.\n\n\n# pyre-fixme[3]: Return type must be annotated.\n# pyre-fixme[2]: Parameter must be annotated.\ndef default_from_interp_rep_transform(curr_sample, original_inputs, **kwargs):\n    assert (\n        \"feature_mask\" in kwargs\n    ), \"Must provide feature_mask to use default interpretable representation transform\"\n    assert (\n        \"baselines\" in kwargs\n    ), \"Must provide baselines to use default interpretable representation transform\"\n    feature_mask = kwargs[\"feature_mask\"]\n    if isinstance(feature_mask, Tensor):\n        binary_mask = curr_sample[0][feature_mask].bool()\n        input_space_transformed = (\n            binary_mask.to(original_inputs.dtype) * original_inputs\n            + (~binary_mask).to(original_inputs.dtype) * kwargs[\"baselines\"]\n        )\n        \n        return input_space_transformed\n    else:\n        binary_mask = tuple(\n            curr_sample[0][feature_mask[j]].bool() for j in range(len(feature_mask))\n        )\n        input_space_transformed = tuple(\n            binary_mask[j].to(original_inputs[j].dtype) * original_inputs[j]\n            + (~binary_mask[j]).to(original_inputs[j].dtype) * kwargs[\"baselines\"][j]\n            for j in range(len(feature_mask))\n        )\n        return input_space_transformed\n\n\ndef get_exp_kernel_similarity_function(\n    distance_mode: str = \"cosine\",\n    kernel_width: float = 1.0,\n) -> Callable[..., float]:\n    r\"\"\"\n    This method constructs an appropriate similarity function to compute\n    weights for perturbed sample in LIME. Distance between the original\n    and perturbed inputs is computed based on the provided distance mode,\n    and the distance is passed through an exponential kernel with given\n    kernel width to convert to a range between 0 and 1.\n\n    The callable returned can be provided as the similarity_fn for\n    Lime or LimeBase.\n\n    Args:\n\n        distance_mode (str, optional): Distance mode can be either \"cosine\" or\n                    \"euclidean\" corresponding to either cosine distance\n                    or Euclidean distance respectively. Distance is computed\n                    by flattening the original inputs and perturbed inputs\n                    (concatenating tuples of inputs if necessary) and computing\n                    distances between the resulting vectors.\n                    Default: \"cosine\"\n        kernel_width (float, optional):\n                    Kernel width for exponential kernel applied to distance.\n                    Default: 1.0\n\n    Returns:\n\n        *Callable*:\n        - **similarity_fn** (*Callable*):\n            Similarity function. This callable can be provided as the\n            similarity_fn for Lime or LimeBase.\n    \"\"\"\n\n    # pyre-fixme[3]: Return type must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def default_exp_kernel(original_inp, perturbed_inp, __, **kwargs):\n        flattened_original_inp = _flatten_tensor_or_tuple(original_inp).float()\n        flattened_perturbed_inp = _flatten_tensor_or_tuple(perturbed_inp).float()\n        if distance_mode == \"cosine\":\n            cos_sim = CosineSimilarity(dim=0)\n            distance = 1 - cos_sim(flattened_original_inp, flattened_perturbed_inp)\n        elif distance_mode == \"euclidean\":\n            distance = torch.norm(flattened_original_inp - flattened_perturbed_inp)\n        else:\n            raise ValueError(\"distance_mode must be either cosine or euclidean.\")\n        return math.exp(-1 * (distance**2) / (2 * (kernel_width**2)))\n\n    return default_exp_kernel\n\n\ndef default_perturb_func(\n    original_inp: TensorOrTupleOfTensorsGeneric, **kwargs: object\n) -> Tensor:\n    assert (\n        \"num_interp_features\" in kwargs\n    ), \"Must provide num_interp_features to use default interpretable sampling function\"\n    if isinstance(original_inp, Tensor):\n        device = original_inp.device\n    else:\n        device = original_inp[0].device\n\n    probs = torch.ones(1, cast(int, kwargs[\"num_interp_features\"])) * 0.5\n    return torch.bernoulli(probs).to(device=device).long()\n\n\ndef construct_feature_mask(\n    feature_mask: Union[None, Tensor, Tuple[Tensor, ...]],\n    formatted_inputs: Tuple[Tensor, ...],\n) -> Tuple[Tuple[Tensor, ...], int]:\n    feature_mask_tuple: Tuple[Tensor, ...]\n    if feature_mask is None:\n        feature_mask_tuple, num_interp_features = _construct_default_feature_mask(\n            formatted_inputs\n        )\n    else:\n        feature_mask_tuple = _format_tensor_into_tuples(feature_mask)\n        min_interp_features = int(\n            min(\n                torch.min(single_mask).item()\n                for single_mask in feature_mask_tuple\n                if single_mask.numel()\n            )\n        )\n        if min_interp_features != 0:\n            warnings.warn(\n                \"Minimum element in feature mask is not 0, shifting indices to\"\n                \" start at 0.\",\n                stacklevel=2,\n            )\n            feature_mask_tuple = tuple(\n                single_mask - min_interp_features for single_mask in feature_mask_tuple\n            )\n\n        num_interp_features = _get_max_feature_index(feature_mask_tuple) + 1\n    return feature_mask_tuple, num_interp_features\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.389958Z","iopub.execute_input":"2025-07-16T22:36:36.390202Z","iopub.status.idle":"2025-07-16T22:36:36.434914Z","shell.execute_reply.started":"2025-07-16T22:36:36.390183Z","shell.execute_reply":"2025-07-16T22:36:36.434268Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"class LimeWithCustomArgumentToForwardFunc(LimeBaseWithCustomArgumentToForwardFunc):\n    r\"\"\"\n    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n    This will just inherit our modified LimeBase class\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_func: Callable[..., Tensor],\n        interpretable_model: Optional[Model] = None,\n        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n        similarity_func: Optional[Callable] = None,\n        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n        perturb_func: Optional[Callable] = None,\n    ) -> None:\n        r\"\"\"\n\n        Args:\n\n\n            forward_func (Callable): The forward function of the model or any\n                    modification of it\n            interpretable_model (Model, optional): Model object to train\n                    interpretable model.\n\n                    This argument is optional and defaults to SkLearnLasso(alpha=0.01),\n                    which is a wrapper around the Lasso linear model in SkLearn.\n                    This requires having sklearn version >= 0.23 available.\n\n                    Other predefined interpretable linear models are provided in\n                    captum._utils.models.linear_model.\n\n                    Alternatively, a custom model object must provide a `fit` method to\n                    train the model, given a dataloader, with batches containing\n                    three tensors:\n\n                    - interpretable_inputs: Tensor\n                      [2D num_samples x num_interp_features],\n                    - expected_outputs: Tensor [1D num_samples],\n                    - weights: Tensor [1D num_samples]\n\n                    The model object must also provide a `representation` method to\n                    access the appropriate coefficients or representation of the\n                    interpretable model after fitting.\n\n                    Note that calling fit multiple times should retrain the\n                    interpretable model, each attribution call reuses\n                    the same given interpretable model object.\n            similarity_func (Callable, optional): Function which takes a single sample\n                    along with its corresponding interpretable representation\n                    and returns the weight of the interpretable sample for\n                    training the interpretable model.\n                    This is often referred to as a similarity kernel.\n\n                    This argument is optional and defaults to a function which\n                    applies an exponential kernel to the cosine distance between\n                    the original input and perturbed input, with a kernel width\n                    of 1.0.\n\n                    A similarity function applying an exponential\n                    kernel to cosine / euclidean distances can be constructed\n                    using the provided get_exp_kernel_similarity_function in\n                    captum.attr._core.lime.\n\n                    Alternately, a custom callable can also be provided.\n                    The expected signature of this callable is:\n\n                    >>> def similarity_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_interpretable_input:\n                    >>>        Tensor [2D 1 x num_interp_features],\n                    >>>    **kwargs: Any\n                    >>> ) -> float or Tensor containing float scalar\n\n                    perturbed_input and original_input will be the same type and\n                    contain tensors of the same shape, with original_input\n                    being the same as the input provided when calling attribute.\n\n                    kwargs includes baselines, feature_mask, num_interp_features\n                    (integer, determined from feature mask).\n            perturb_func (Callable, optional): Function which returns a single\n                    sampled input, which is a binary vector of length\n                    num_interp_features, or a generator of such tensors.\n\n                    This function is optional, the default function returns\n                    a binary vector where each element is selected\n                    independently and uniformly at LimeWithCustomArgumentToForwardFuncrandom. Custom\n                    logic for selecting sampled binary vectors can\n                    be implemented by providing a function with the\n                    following expected signature:\n\n                    >>> perturb_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor [Binary 2D Tensor 1 x num_interp_features]\n                    >>>  or generator yielding such tensors\n\n                    kwargs includes baselines, feature_mask, num_interp_features\n                    (integer, determined from feature mask).\n\n        \"\"\"\n        if interpretable_model is None:\n            interpretable_model = SkLearnLasso(alpha=0.01)\n\n        if similarity_func is None:\n            similarity_func = get_exp_kernel_similarity_function()\n\n        if perturb_func is None:\n            perturb_func = default_perturb_func\n\n        LimeBaseWithCustomArgumentToForwardFunc.__init__(\n            self,\n            forward_func,\n            interpretable_model,\n            similarity_func,\n            perturb_func,\n            True,\n            default_from_interp_rep_transform,\n            None,\n        )\n\n    @log_usage(part_of_slo=True)\n    def attribute(  # type: ignore\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        baselines: BaselineType = None,\n        target: TargetType = None,\n        additional_forward_args: Optional[object] = None,\n        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n        n_samples: int = 25,\n        perturbations_per_eval: int = 1,\n        return_input_shape: bool = True,\n        show_progress: bool = False,\n    ) -> TensorOrTupleOfTensorsGeneric:\n        r\"\"\"\n        This method attributes the output of the model with given target index\n        (in case it is provided, otherwise it assumes that output is a\n        scalar) to the inputs of the model using the approach described above,\n        training an interpretable model and returning a representation of the\n        interpretable model.\n\n        It is recommended to only provide a single example as input (tensors\n        with first dimension or batch size = 1). This is because LIME is generally\n        used for sample-based interpretability, training a separate interpretable\n        model to explain a model's prediction on each individual example.\n\n        A batch of inputs can also be provided as inputs, similar to\n        other perturbation-based attribution methods. In this case, if forward_fn\n        returns a scalar per example, attributions will be computed for each\n        example independently, with a separate interpretable model trained for each\n        example. Note that provided similarity and pertforward_funcurbation functions will be\n        provided each example separately (first dimension = 1) in this case.\n        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n        still be computed using a single interpretable model for the full batch.\n        In this case, similarity and perturbation functions will be provided the\n        same original input containing the full batch.\n\n        The number of interpretable features is determined from the provided\n        feature mask, or if none is provided, from the default feature mask,\n        which considers each scalar input as a separate feature. It is\n        generally recommended to provide a feature mask which groups features\n        into a small number of interpretable features / components (e.g.\n        superpixels in images).\n\n        Args:\n\n            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n                        is computed. If forward_func takes a single\n                        tensor as input, a single input tensor should be provided.\n                        If forward_func takes multiple tensors as input, a tuple\n                        of the input tensors should be provided. It is assumed\n                        that for all given input tensors, dimension 0 corresponds\n                        to the number of examples, and if multiple input tensors\n                        are provided, the examples must be aligned appropriately.\n            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n                        Baselines define reference value which replaces each\n                        feature when the corresponding interpretable feature\n                        is set to 0.\n                        Baselines can be provided as:\n\n                        - a single tensor, if inputs is a single tensor, with\n                          exactly the same dimensions as inputs or the first\n                          dimension is one and the remaining dimensions match\n                          with inputs.\n\n                        - a single scalar, if inputs is a single tensor, which will\n                          be broadcasted for each input value in input tensor.\n\n                        - a tuple of tensors or scalars, the baseline corresponding\n                          to each tensor in the inputs' tuple can be:\n\n                          - either a tensor with matching dimensions to\n                            corresponding tensor in the inputs' tuple\n                            or the first dimension is one and the remaining\n                            dimensions match with the corresponding\n                            input tensor.\n\n                          - or a scalar, corresponding to a tensor in the\n                            inputs' tuple. This scalar value is broadcasted\n                            for corresponding input tensor.\n\n                        In the cases when `baselines` iforward_funcs not provided, we internally\n                        use zero scalar corresponding to each input tensor.\n                        Default: None\n            target (int, tuple, Tensor, or list, optional): Output indices for\n                        which surrogate model is trained\n                        (for classification cases,\n                        this is usually the target class).\n                        If the network returns a scalar value per example,\n                        no target index is necessary.\n                        For general 2D outputs, targets can be either:\n\n                        - a single integer or a tensor containing a single\n                          integer, which is applied to all input examples\n\n                        - a list of integers or a 1D tensor, with length matching\n                          the number of examples in inputs (dim 0). Each integer\n                          is applied as the target for the corresponding example.\n\n                        For outputs with > 2 dimensions, targets can be either:\n\n                        - A single tuple, which contains #output_dims - 1\n                          elements. This target index is applied to all examples.\n\n                        - A list of tuples with length equal to the number of\n                          examples in inputs (dim 0), and each tuple containing\n                          #output_dims - 1 elements. Each tuple is applied as the\n                          target for the corresponding example.\n\n                        Default: None\n            additional_forward_args (Any, optional): If the forward function\n                        requires additional arguments other than the inputs for\n                        which attributions should not be computed, this argument\n                        can be provided. It must be either a single additional\n                        argument of a Tensor or arbitrary (non-tuple) type or a\n                        tuple containing multiple additional arguments including\n                        tensors or any arbitrary python types. These arguments\n                        are provided to forward_func in order following the\n                        arguments in inputs.\n                        For a tensor, the first dimension of the tensor must\n                        correspond to the number of examples. It will be\n                        repeated for each of `n_steps` along the integrated\n                        path. For all other types, the given argument is used\n                        for all forward evaluations.\n                        Note that attributions are not computed with respect\n                        to these arguments.\n                        Default: None\n            feature_mask (Tensor or tuple[Tensor, ...], optional):\n                        feature_mask defines a mask for the input, grouping\n                        features which correspond to the same\n                        interpretable feature. feature_mask\n                        should contain the same number of tensors as inputs.\n                        Each tensor should\n                        be the same size as the corresponding input or\n                        broadcastable to match the inpuforward_funct tensor. Values across\n                        all tensors should be integers in the range 0 to\n                        num_interp_features - 1, and indices corresponding to the\n                        same feature should have the same value.\n                        Note that features are grouped across tensors\n                        (unlike feature ablation and occlusion), so\n                        if the same index is used in different tensors, those\n                        features are still grouped and added simultaneously.\n                        If None, then a feature mask is constructed which assigns\n                        each scalar within a tensor as a separate feature.\n                        Default: None\n            n_samples (int, optional): The number of samples of the original\n                        model used to train the surrogate interpretable model.\n                        Default: `50` if `n_samples` is not provided.\n            perturbations_per_eval (int, optional): Allows multiple samples\n                        to be processed simultaneously in one call to forward_fn.\n                        Each forward pass will contain a maximum of\n                        perturbations_per_eval * #examples samples.\n                        For DataParallel models, each batch is split among the\n                        available devices, so evaluations on each available\n                        device contain at most\n                        (perturbations_per_eval * #examples) / num_devices\n                        samples.\n                        If the forward function returns a single scalar per batch,\n                        perturbations_per_eval must be set to 1.\n                        Default: 1\n            return_input_shape (bool, optional): Determines whether the returned\n                        tensor(s) only contain the coefficients for each interp-\n                        retable feature from the trained surrogate model, or\n                        whether the returned attributions match the input shape.\n                        When return_input_shape is True, the return type of attribute\n                        matches the input shape, with each element containing the\n                        coefficient of the corresponding interpretale feature.\n                        All elements with the same value in the feature mask\n                        will contain the same coefficient in the returned\n                        attributions.\n                        If forward_func returns a single element per batch, then the\n                        first dimension of each tensor will be 1, and the remaining\n                        dimensions will have the same shape as the original input\n                        tensor.\n                        If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpreatable models, with length\n                        num_interp_features.\n            show_progress (bool, optional): Displays the progress of computation.\n                        It will try to use tqdm if available for advanced features\n                        (e.g. time estimation). Otherwise, it will fallback to\n                        a simple output of progress.\n                        Default: False\n\n        Returns:\n            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n                        The attributions with respect to each input feature.\n                        If return_input_shape = True, attributions will be\n                        the same size as the provided inputs, with each value\n                        providing the coefficient of the corresponding\n                        interpretale feature.\n                        If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpreatable models, with length\n                        num_interp_features.\n        Examples::\n\n            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n            >>> # and returns an Nx3 tensor of class probabilities.\n            >>> net = SimpleClassifier()\n\n            >>> # Generating random input with size 1 x 4 x 4\n            >>> input = torch.randn(1, 4, 4)\n\n            >>> # Defining Lime interpreter\n            >>> lime = Lime(net)\n            >>> # Computes attribution, with each of the 4 x 4 = 16\n            >>> # features as a separate interpretable feature\n            >>> attr = lime.attribute(input, target=1, n_samples=200)\n\n            >>> # Alternatively, we can group each 2x2 square of the inputs\n            >>> # as one 'interpretable' feature and perturb them together.\n            >>> # This can be done by creating a feature mask as follows, which\n            >>> # defines the feature groups, e.g.:\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # With this mask, all inputs with the same value are set to their\n            >>> # baseline value, when the corresponding binary interpretable\n            >>> # feature is set to 0.\n            >>> # The attributions can be calculated as follows:\n            >>> # feature mask has dimensions 1 x 4 x 4\n            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n            >>>                             [2,2,3,3],[2,2,3,3]]])\n\n            >>> # Computes interpretable model and returning attributions\n            >>> # matching input shape.\n            >>> attr = lime.attribute(input, target=1, feature_mask=feature_mask)\n        \"\"\"\n        return self._attribute_kwargs(\n            inputs=inputs,\n            baselines=baselines,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            feature_mask=feature_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            return_input_shape=return_input_shape,\n            show_progress=show_progress,\n        )\n\n    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n    def attribute_future(self) -> Callable:\n        return super().attribute_future()\n\n    def _attribute_kwargs(  # type: ignore\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        baselines: BaselineType = None,\n        target: TargetType = None,\n        additional_forward_args: Optional[object] = None,\n        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n        n_samples: int = 25,\n        perturbations_per_eval: int = 1,\n        return_input_shape: bool = True,\n        monitor_log_path: str | None = None,\n        monitor_convergence_step: int | None = 20,\n        monitor_local_accuracy_step: int | None = 50,\n        show_progress: bool = False,\n        **kwargs: object,\n    ) -> TensorOrTupleOfTensorsGeneric:\n        is_inputs_tuple = _is_tuple(inputs)\n        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n        bsz = formatted_inputs[0].shape[0]\n\n        feature_mask, num_interp_features = construct_feature_mask(\n            feature_mask, formatted_inputs\n        )\n\n        if num_interp_features > 10000:\n            warnings.warn(\n                \"Attempting to construct interpretable model with > 10000 features.\"\n                \"This can be very slow or lead to OOM issues. Please provide a feature\"\n                \"mask which groups input features to reduce the number of interpretable\"\n                \"features. \",\n                stacklevel=1,\n            )\n\n        coefs: Tensor\n        if bsz > 1:\n            test_output = _run_forward(\n                self.forward_func, inputs, target, additional_forward_args\n            )\n            if isinstance(test_output, Tensor) and torch.numel(test_output) > 1:\n                if torch.numel(test_output) == bsz:\n                    warnings.warn(\n                        \"You are providing multiple inputs for Lime / Kernel SHAP \"\n                        \"attributions. This trains a separate interpretable model \"\n                        \"for each example, which can be time consuming. It is \"\n                        \"recommended to compute attributions for one example at a \"\n                        \"time.\",\n                        stacklevel=1,\n                    )\n                    output_list = []\n                    for (\n                        curr_inps,\n                        curr_target,\n                        curr_additional_args,\n                        curr_baselines,\n                        curr_feature_mask,\n                    ) in _batch_example_iterator(\n                        bsz,\n                        formatted_inputs,\n                        target,\n                        additional_forward_args,# -----> CAN BE ALSO BATCHED AUTOMATICALLY BY THE LIBRARY ITERATOR\n                        baselines,\n                        feature_mask,\n                    ):\n                        coefs = super().attribute.__wrapped__(\n                            self,\n                            inputs=curr_inps if is_inputs_tuple else curr_inps[0],\n                            target=curr_target,\n                            additional_forward_args=curr_additional_args,\n                            n_samples=n_samples,\n                            perturbations_per_eval=perturbations_per_eval,\n                            baselines=(\n                                curr_baselines if is_inputs_tuple else curr_baselines[0]\n                            ),\n                            feature_mask=(\n                                curr_feature_mask\n                                if is_inputs_tuple\n                                else curr_feature_mask[0]\n                            ),\n                            num_interp_features=num_interp_features,\n                            show_progress=show_progress,\n                            **kwargs,\n                        )\n                        if return_input_shape:\n                            output_list.append(\n                                self._convert_output_shape(\n                                    curr_inps,\n                                    curr_feature_mask,\n                                    coefs,\n                                    num_interp_features,\n                                    is_inputs_tuple,\n                                )\n                            )\n                        else:\n                            output_list.append(coefs.reshape(1, -1))  # type: ignore\n\n                    return _reduce_list(output_list)\n                else:\n                    raise AssertionError(\n                        \"Invalid number of outputs, forward function should return a\"\n                        \"scalar per example or a scalar per input batch.\"\n                    )\n            else:\n                assert perturbations_per_eval == 1, (\n                    \"Perturbations per eval must be 1 when forward function\"\n                    \"returns single value per batch!\"\n                )\n\n        coefs = super().attribute.__wrapped__(\n            self,\n            inputs=inputs,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            baselines=baselines if is_inputs_tuple else baselines[0],\n            feature_mask=feature_mask if is_inputs_tuple else feature_mask[0],\n            num_interp_features=num_interp_features,\n            monitor_log_path = monitor_log_path,\n            monitor_convergence_step = monitor_convergence_step,\n            monitor_local_accuracy_step = monitor_local_accuracy_step,\n            show_progress=show_progress,\n            **kwargs,\n        )\n        if return_input_shape:\n            # pyre-fixme[7]: Expected `TensorOrTupleOfTensorsGeneric` but got\n            #  `Tuple[Tensor, ...]`.\n            return self._convert_output_shape(\n                formatted_inputs,\n                feature_mask,\n                coefs,\n                num_interp_features,\n                is_inputs_tuple,\n    \n            leading_dim_one=(bsz > 1),\n            )\n        else:\n            return coefs\n\n    @typing.overload\n    def _convert_output_shape(\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: Literal[True],\n        leading_dim_one: bool = False,\n    ) -> Tuple[Tensor, ...]: ...\n\n    @typing.overload\n    def _convert_output_shape(  # type: ignore\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: Literal[False],\n        leading_dim_one: bool = False,\n    ) -> Tensor: ...\n\n    @typing.overload\n    def _convert_output_shape(\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: bool,\n        leading_dim_one: bool = False,\n    ) -> Union[Tensor, Tuple[Tensor, ...]]: ...\n\n    def _convert_output_shape(\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: bool,\n        leading_dim_one: bool = False,\n    ) -> Union[Tensor, Tuple[Tensor, ...]]:\n        coefs = coefs.flatten()\n        attr = [\n            torch.zeros_like(single_inp, dtype=torch.float)\n            for single_inp in formatted_inp\n        ]\n        for tensor_ind in range(len(formatted_inp)):\n            for single_feature in range(num_interp_features):\n                attr[tensor_ind] += (\n                    coefs[single_feature].item()\n                    * (feature_mask[tensor_ind] == single_feature).float()\n                )\n        if leading_dim_one:\n            for i in range(len(attr)):\n                attr[i] = attr[i][0:1]\n        return _format_output(is_inputs_tuple, tuple(attr))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.435859Z","iopub.execute_input":"2025-07-16T22:36:36.436067Z","iopub.status.idle":"2025-07-16T22:36:36.463039Z","shell.execute_reply.started":"2025-07-16T22:36:36.436051Z","shell.execute_reply":"2025-07-16T22:36:36.462226Z"}},"outputs":[],"execution_count":137},{"cell_type":"code","source":"#!/usr/bin/env python3\n\n# pyre-strict\n\nfrom typing import Callable, cast, Generator, Optional, Tuple, Union\n\nimport torch\nfrom captum._utils.models.linear_model import SkLearnLinearRegression\nfrom captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\nfrom captum.attr._core.lime import construct_feature_mask, Lime\nfrom captum.attr._utils.common import _format_input_baseline\nfrom captum.log import log_usage\nfrom torch import Tensor\nfrom torch.distributions.categorical import Categorical\n\n\nclass KernelShapWithMask(LimeWithCustomArgumentToForwardFunc):\n    r\"\"\"\n    Kernel SHAP is a method that uses the LIME framework to compute\n    Shapley Values. Setting the loss function, weighting kernel and\n    regularization terms appropriately in the LIME framework allows\n    theoretically obtaining Shapley Values more efficiently than\n    directly computing Shapley Values.\n\n    More information regarding this method and proof of equivalence\n    can be found in the original paper here:\n    https://arxiv.org/abs/1705.07874\n    \"\"\"\n\n    def __init__(self, \n                 forward_func: Callable[..., Tensor],\n                 surrogate_model: str = \"linear regression\",\n                 alpha_surrogate: float = 0.01,\n                 max_iter_surrogate: int = 1000\n                ) -> None:\n        r\"\"\"\n        Args:\n\n            forward_func (Callable): The forward function of the model or\n                        any modification of it.\n        \"\"\"\n        if surrogate_model == \"linear regression\":\n            interpretable_model = SkLearnLinearRegression()\n        elif surrogate_model == \"lasso\": \n            interpretable_model = SkLearnLasso(alpha=alpha_surrogate, max_iter=max_iter_surrogate)\n            \n        LimeWithCustomArgumentToForwardFunc.__init__(\n            self,\n            forward_func,\n            interpretable_model=interpretable_model,\n            similarity_func=self.kernel_shap_similarity_kernel,\n            perturb_func=self.kernel_shap_perturb_generator,\n        )\n        self.inf_weight = 1000000.0\n\n    @log_usage(part_of_slo=True)\n    def attribute(  # type: ignore\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        baselines: BaselineType = None,\n        target: TargetType = None,\n        additional_forward_args: Optional[object] = None,\n        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n        n_samples: int = 25,\n        perturbations_per_eval: int = 1,\n        return_input_shape: bool = True,\n        monitor_log_path: str | None = None,\n        monitor_convergence_step: int | None = 20,\n        monitor_local_accuracy_step: int | None = 50,\n        show_progress: bool = False,\n    ) -> TensorOrTupleOfTensorsGeneric:\n        r\"\"\"\n        This method attributes the output of the model with given target index\n        (in case it is provided, otherwise it assumes that output is a\n        scalar) to the inputs of the model using the approach described above,\n        training an interpretable model based on KernelSHAP and returning a\n        representation of the interpretable model.\n\n        It is recommended to only provide a single example as input (tensors\n        with first dimension or batch size = 1). This is because LIME / KernelShap\n        is generally used for sample-based interpretability, training a separate\n        interpretable model to explain a model's prediction on each individual example.\n\n        A batch of inputs can also be provided as inputs, similar to\n        other perturbation-based attribution methods. In this case, if forward_fn\n        returns a scalar per example, attributions will be computed for each\n        example independently, with a separate interpretable model trained for each\n        example. Note that provided similarity and perturbation functions will be\n        provided each example separately (first dimension = 1) in this case.\n        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n        still be computed using a single interpretable model for the full batch.\n        In this case, similarity and perturbation functions will be provided the\n        same original input containing the full batch.\n\n        The number of interpretable features is determined from the provided\n        feature mask, or if none is provided, from the default feature mask,\n        which considers each scalar input as a separate feature. It is\n        generally recommended to provide a feature mask which groups features\n        into a small number of interpretable features / components (e.g.\n        superpixels in images).\n\n        Args:\n\n            inputs (Tensor or tuple[Tensor, ...]): Input for which KernelShap\n                        is computed. If forward_func takes a single\n                        tensor as input, a single input tensor should be provided.\n                        If forward_func takes multiple tensors as input, a tuple\n                        of the input tensors should be provided. It is assumed\n                        that for all given input tensors, dimension 0 corresponds\n                        to the number of examples, and if multiple input tensors\n                        are provided, the examples must be aligned appropriately.\n            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n                        Baselines define the reference value which replaces eachconv_dist_L1 and delta_shap\n                        feature when the corresponding interpretable feature\n                        is set to 0.\n                        Baselines can be provided as:\n\n                        - a single tensor, if inputs is a single tensor, with\n                          exactly the same dimensions as inputs or the first\n                          dimension is one and the remaining dimensions match\n                          with inputs.\n\n                        - a single scalar, if inputs is a single tensor, which will\n                          be broadcasted for each input value in input tensor.\n\n                        - a tuple of tensors or scalars, the baseline corresponding\n                          to each tensor in the inputs' tuple can be:\n\n                          - either a tensor with matching dimensions to\n                            corresponding tensor in the inputs' tuple\n                            or the first dimension is one and the remaining\n                            dimensions match with the corresponding\n                            input tensor.\n\n                          - or a scalar, corresponding to a tensor in the\n                            inputs' tuple. This scalar value is broadcasted\n                            for corresponding input tensor.\n\n                        In the cases when `baselines` is not provided, we internally\n                        use zero scalar corresponding to each input tensor.\n                        Default: None\n            target (int, tuple, Tensor, or list, optional): Output indices for\n                        which surrogate model is trained\n                        (for classification cases,\n                        this is usually the target class).\n                        If the network returns a scalar value per example,\n                        no target index is necessary.\n                        For general 2D outputs, targets can be either:\n\n                        - a single integer or a tensor containing a single\n                          integer, which is applied to all input examples\n\n                        - a list of integers or a 1D tensor, with length matching\n                          the number of examples in inputs (dim 0). Each integer\n                          is applied as the target for the corresponding example.\n\n                        For outputs with > 2 dimensions, targets can be either:\n\n                        - A single tuple, which contains #output_dims - 1\n                          elements. This target index is applied to all examples.\n\n                        - A list of tuples with length equal to the number of\n                          examples in inputs (dim 0), and each tuple containing\n                          #output_dims - 1 elements. Each tuple is applied as the\n                          target for the corresponding example.\n\n                        Default: None\n            additional_forward_args (Any, optional): If the forward function\n                        requires additional arguments other than the inputs for\n                        which attributions should not be computed, this argument\n                        can be provided. It must be either a single additional\n                        argument of a Tensor or arbitrary (non-tuple) type or a\n                        tuple containing multiple additional arguments including\n                        tensors or any arbitrary python types. These arguments\n                        are provided to forward_func in order following the\n                        arguments in inputs.\n                        For a tensor, the first dimension of the tensor must\n                        correspond to the number of examples. It will be\n                        repeated for each of `n_steps` along the integrated\n                        path. For all other types, the given argument is used\n                        for all forward evaluations.\n                        Note that attributions are not computed with respect\n                        to these arguments.\n                        Default: None\n            feature_mask (Tensor or tuple[Tensor, ...], optional):\n                        feature_mask defines a mask for the input, grouping\n                        features which correspond to the same\n                        interpretable feature. feature_mask\n                        should contain the same number of tensors as inputs.\n                        Each tensor should\n                        be the same size as the corresponding input or\n                        broadcastable to match the input tensor. Values across\n                        all tensors should be integers in the range 0 to\n                        num_interp_features - 1, and indices corresponding to the\n                        same feature should have the same value.\n                        Note that features are grouped across tensors\n                        (unlike feature ablation and occlusion), so\n                        if the same index is used in different tensors, those\n                        features are still grouped and added simultaneously.\n                        If None, then a feature mask is constructed which assigns\n                        each scalar within a tensor as a separate feature.\n                        Default: None\n            n_samples (int, optional): The number of samples of the original\n                        model used to train the surrogate interpretable model.\n                        Default: `50` if `n_samples` is not provided.\n            perturbations_per_eval (int, optional): Allows multiple samples\n                        to be processed simultaneously in one call to forward_fn.\n                        Each forward pass will contain a maximum of\n                        perturbations_per_eval * #examples samples.\n                        For DataParallel models, each batch is split among the\n                        available devices, so evaluations on each available\n                        device contain at most\n                        (perturbations_per_eval * #examples) / num_devices\n                        samples.\n                        If the forward function returns a single scalar per batch,\n                        perturbations_per_eval must be set to 1.\n                        Default: 1\n            return_input_shape (bool, optional): Determines whether the returned\n                        tensor(s) only contain the coefficients for each interp-\n                        retable feature from the trained surrogate model, or\n                        whether the returned attributions match the input shape.\n                        When return_input_shape is True, the return type of attribute\n                        matches the input shape, with each element containing the\n                        coefficient of the corresponding interpretable feature.\n                        All elements with the same value in the feature mask\n                        will contain the same coefficient in the returned\n                        attributions. If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpretable model, with length\n                        num_interp_features.\n            monitor_log_path (str, optional): Path to the log file for monitoring convergence.\n                        if None, no monitoring is performed.\n                        Default: None\n            monitor_convergence_step (int, optional): Number of iterations over which\n                        the difference among two attribution is computerd.\n                        Default: 20\n            monitor_local_accuracy_step (int, optional): Number of iterations over which\n                        the local accuracy of an attribution is computerd.\n                        Default: 50\n            show_progress (bool, optional): Displays the progress of computation.\n                        It will try to use tqdm if available for advanced features\n                        (e.g. time estimation). Otherwise, it will fallback to\n                        a simple output of progress.\n                        Default: False\n\n        Returns:\n            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n                        The attributions with respect to each input feature.\n                        If return_input_shape = True, attributions will be\n                        the same size as the provided inputs, with each value\n                        providing the coefficient of the corresponding\n                        interpretale feature.\n                        If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpreatable models, with length\n                        num_interp_features.\n        Examples::\n            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n            >>> # and returns an Nx3 tensor of class probabilities.\n            >>> net = SimpleClassifier()\n\n            >>> # Generating random input with size 1 x 4 x 4\n            >>> input = torch.randn(1, 4, 4)\n\n            >>> # Defining KernelShap interpreter\n            >>> ks = KernelShap(net)\n            >>> # Computes attribution, with each of the 4 x 4 = 16\n            >>> # features as a separate interpretable feature\n            >>> attr = ks.attribute(input, target=1, n_samples=200)\n\n            >>> # Alternatively, we can group each 2x2 square of the inputs\n            >>> # as one 'interpretable' feature and perturb them together.\n            >>> # This can be done by creating a feature mask as follows, which\n            >>> # defines the feature groups, e.g.:\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # With this mask, all inputs with the same value are set to their\n            >>> # baseline value, when the corresponding binary interpretable\n            >>> # feature is set to 0.\n            >>> # The attributions can be calculated as follows:\n            >>> # feature mask has dimensions 1 x 4 x 4\n            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n            >>>                             [2,2,3,3],[2,2,3,3]]])\n\n            >>> # Computes KernelSHAP attributions with feature mask.\n            >>> attr = ks.attribute(input, target=1, feature_mask=feature_mask)\n        \"\"\"\n        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n        feature_mask, num_interp_features = construct_feature_mask(\n            feature_mask, formatted_inputs\n        )\n        num_features_list = torch.arange(num_interp_features, dtype=torch.float)\n        denom = num_features_list * (num_interp_features - num_features_list)\n        probs = torch.tensor((num_interp_features - 1)) / denom\n        probs[0] = 0.0\n        return self._attribute_kwargs(\n            inputs=inputs,\n            baselines=baselines,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            feature_mask=feature_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            return_input_shape=return_input_shape,\n            num_select_distribution=Categorical(probs),\n            monitor_log_path=monitor_log_path,\n            monitor_convergence_step=monitor_convergence_step,\n            monitor_local_accuracy_step=monitor_local_accuracy_step,\n            show_progress=show_progress,\n        )\n\n    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n    def attribute_future(self) -> Callable:\n        r\"\"\"\n        This method is not implemented for KernelShap.\n        \"\"\"\n        raise NotImplementedError(\"attribute_future is not implemented for KernelShap\")\n\n    def kernel_shap_similarity_kernel(\n        self,\n        _,\n        __,\n        interpretable_sample: Tensor,\n        **kwargs: object,\n    ) -> Tensor:\n        assert (\n            \"num_interp_features\" in kwargs\n        ), \"Must provide num_interp_features to use default similarity kernel\"\n        num_selected_features = int(interpretable_sample.sum(dim=1).item())\n        num_features = kwargs[\"num_interp_features\"]\n        if num_selected_features == 0 or num_selected_features == num_features:\n            # weight should be theoretically infinite when\n            # num_selected_features = 0 or num_features\n            # enforcing that trained linear model must satisfy\n            # end-point criteria. In practice, it is sufficient to\n            # make this weight substantially larger so setting this\n            # weight to 1000000 (all other weights are 1).\n            similarities = self.inf_weight\n        else:\n            similarities = 1.0\n        return torch.tensor([similarities])\n\n    def kernel_shap_perturb_generator(\n        self,\n        original_inp: Union[Tensor, Tuple[Tensor, ...]],\n        **kwargs: object,\n    ) -> Generator[Tensor, None, None]:\n        r\"\"\"\n        Perturbations are sampled by the following process:\n         - Choose k (number of selected features), based on the distribution\n                p(k) = (M - 1) / (k * (M - k))\n\n            where M is the total number of features in the interpretable space\n\n         - Randomly select a binary vector with k ones, each sample is equally\n            likely. This is done by generating a random vector of normal\n            values and thresholding based on the top k elements.\n\n         Since there are M choose k vectors with k ones, this weighted sampling\n         is equivalent to applying the Shapley kernel for the sample weight,\n         defined as:\n         k(M, k) = (M - 1) / (k * (M - k) * (M choose k))\n        \"\"\"\n        assert (\n            \"num_select_distribution\" in kwargs and \"num_interp_features\" in kwargs\n        ), (\n            \"num_select_distribution and num_interp_features are necessary\"\n            \" to use kernel_shap_perturb_func\"\n        )\n        if isinstance(original_inp, Tensor):\n            device = original_inp.device\n        else:\n            device = original_inp[0].device\n        num_features = cast(int, kwargs[\"num_interp_features\"])\n        yield torch.ones(1, num_features, device=device, dtype=torch.long)\n        yield torch.zeros(1, num_features, device=device, dtype=torch.long)\n        while True:\n            num_selected_features = cast(\n                Categorical, kwargs[\"num_select_distribution\"]\n            ).sample()\n            rand_vals = torch.randn(1, num_features)\n            threshold = torch.kthvalue(\n                rand_vals, num_features - num_selected_features\n            ).values.item()\n            yield (rand_vals > threshold).to(device=device).long()\n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.463813Z","iopub.execute_input":"2025-07-16T22:36:36.464011Z","iopub.status.idle":"2025-07-16T22:36:36.491529Z","shell.execute_reply.started":"2025-07-16T22:36:36.463997Z","shell.execute_reply":"2025-07-16T22:36:36.491027Z"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"# define an utility for annoying nnunetv2 preprocessing\ndef nnunetv2_default_preprocessing(ct_img_path, predictor, dataset_json_path) -> np.ndarray:\n    plans_manager = predictor.plans_manager\n    configuration_manager = predictor.configuration_manager\n    \n    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n    rw = plans_manager.image_reader_writer_class()\n    if callable(rw) and not hasattr(rw, \"read_images\"):\n        rw = rw()\n    img_np, img_props = rw.read_images([str(ct_img_path)])\n    \n    preprocessed, _, _ = preprocessor.run_case_npy(\n        img_np, seg=None, properties=img_props,\n        plans_manager=plans_manager,\n        configuration_manager=configuration_manager,\n        dataset_json=dataset_json_path\n    )\n    return preprocessed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.492200Z","iopub.execute_input":"2025-07-16T22:36:36.492410Z","iopub.status.idle":"2025-07-16T22:36:36.515334Z","shell.execute_reply.started":"2025-07-16T22:36:36.492388Z","shell.execute_reply":"2025-07-16T22:36:36.514661Z"}},"outputs":[],"execution_count":139},{"cell_type":"markdown","source":"# Next step: define regular, fixed size superpixels and try to compute the attributions of each of them\nSo we also need to define a metric to compare, since segmentation explanations, differently from classification, is intrinsically ambiguous. For example, let's select a priori a single region of the segmentation output, and use the average of these pixels to compute the impact of perturbations.","metadata":{}},{"cell_type":"markdown","source":"### 4. Face-centered cubic (FCC) lattice induced supervoxel assignment\n-> more *isotropic* than simple cubes","metadata":{}},{"cell_type":"markdown","source":"### 4.1 affine transformation to translate isotropy from voxel space into the original geometrical space (measured in mm)","metadata":{}},{"cell_type":"markdown","source":"### 4.2 Try to apply the original algorithm FCC it to an affine transformed volume that has the same proportion as the .nii in the physical space. Transform->apply the algorithm to derive the map-> back transform the map onto the voxel space\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nfrom scipy.spatial import cKDTree\n\ndef generate_supervoxel_map(img, S=200.0):\n    \"\"\"\n    Generate a supervoxel map using FCC tessellation in physical space (original version).\n    \n    Args:\n        img: Nibabel NIfTI image object\n        S (float): Desired supervoxel size in millimeters (default: 200.0)\n    \n    Returns:\n        supervoxel_map: 3D NumPy array with integer labels for supervoxels\n    \"\"\"\n    # Load volume and affine from image\n    volume = img.get_fdata()\n    affine = img.affine\n    W, H, D = volume.shape\n\n    # Compute the physical bounding box of the volume\n    corners_voxel = np.array([\n        [0, 0, 0],\n        [W-1, 0, 0],\n        [0, H-1, 0],\n        [0, 0, D-1],\n        [W-1, H-1, 0],\n        [W-1, 0, D-1],\n        [0, H-1, D-1],\n        [W-1, H-1, D-1]\n    ])\n    corners_hom = np.hstack((corners_voxel, np.ones((8, 1))))\n    corners_physical = (affine @ corners_hom.T).T[:, :3]\n    min_xyz = corners_physical.min(axis=0)\n    max_xyz = corners_physical.max(axis=0)\n\n    # Generate FCC lattice centers in physical space\n    a = S * np.sqrt(2)\n    factor = 2 / a\n    p_min = int(np.floor(factor * min_xyz[0])) - 1\n    p_max = int(np.ceil(factor * max_xyz[0])) + 1\n    q_min = int(np.floor(factor * min_xyz[1])) - 1\n    q_max = int(np.ceil(factor * max_xyz[1])) + 1\n    r_min = int(np.floor(factor * min_xyz[2])) - 1\n    r_max = int(np.ceil(factor * max_xyz[2])) + 1\n\n    # Create grid of possible indices\n    p_vals = np.arange(p_min, p_max + 1)\n    q_vals = np.arange(q_min, q_max + 1)\n    r_vals = np.arange(r_min, r_max + 1)\n    P, Q, R = np.meshgrid(p_vals, q_vals, r_vals, indexing='ij')\n    P = P.flatten()\n    Q = Q.flatten()\n    R = R.flatten()\n\n    # Filter for FCC lattice points (sum of indices is even)\n    mask = (P + Q + R) % 2 == 0\n    P = P[mask]\n    Q = Q[mask]\n    R = R[mask]\n\n    # Compute physical coordinates of centers\n    centers = np.column_stack((P * a / 2, Q * a / 2, R * a / 2))\n\n    # Keep only centers within the bounding box\n    inside = ((centers[:, 0] >= min_xyz[0]) & (centers[:, 0] <= max_xyz[0]) &\n              (centers[:, 1] >= min_xyz[1]) & (centers[:, 1] <= max_xyz[1]) &\n              (centers[:, 2] >= min_xyz[2]) & (centers[:, 2] <= max_xyz[2]))\n    centers = centers[inside]\n\n    # Check if any centers were generated\n    print(f\"Number of supervoxel centers: {len(centers)}\")\n    if len(centers) == 0:\n        raise ValueError(\"No supervoxel centers generated. Try reducing S.\")\n\n    # Generate voxel indices and transform to physical coordinates\n    voxel_indices = np.indices((W, H, D)).reshape(3, -1).T  # shape (W*H*D, 3)\n    voxel_indices_hom = np.hstack((voxel_indices, np.ones((voxel_indices.shape[0], 1))))  # shape (W*H*D, 4)\n    physical_coords = (affine @ voxel_indices_hom.T).T[:, :3]  # shape (W*H*D, 3)\n\n    # Assign each voxel to the nearest supervoxel center\n    tree = cKDTree(centers)\n    _, labels = tree.query(physical_coords)\n\n    # Create the supervoxel map\n    supervoxel_map = labels.reshape((W, H, D)).astype(np.int32)\n\n    return supervoxel_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.516060Z","iopub.execute_input":"2025-07-16T22:36:36.516272Z","iopub.status.idle":"2025-07-16T22:36:36.541012Z","shell.execute_reply.started":"2025-07-16T22:36:36.516258Z","shell.execute_reply":"2025-07-16T22:36:36.540291Z"}},"outputs":[],"execution_count":140},{"cell_type":"markdown","source":"## Try SLIC for visual context aware supervoxels","metadata":{}},{"cell_type":"markdown","source":"### define a preprocessing routine to enhance SLIC results","metadata":{}},{"cell_type":"code","source":"from skimage import exposure\nfrom skimage.restoration import denoise_nl_means, estimate_sigma\nfrom scipy.ndimage import gaussian_filter\nfrom skimage.exposure import equalize_adapthist\n\ndef preprocessing_for_SLIC(data: np.ndarray):\n    # 2ï¸âƒ£ Clip extreme intensities (e.g. 0.5%â€“99.5% quantiles) for contrast enhancement\n    vmin, vmax = np.quantile(data, (0.005, 0.995))\n    data = np.clip(data, vmin, vmax)\n    data = exposure.rescale_intensity(data, in_range=(vmin, vmax), out_range=(0, 1))  #  [oai_citation:0â€¡scikit-image.org](https://scikit-image.org/docs/0.25.x/api/skimage.segmentation.html?utm_source=chatgpt.com) [oai_citation:1â€¡researchgate.net](https://www.researchgate.net/publication/330691413_A_novel_technique_for_analysing_histogram_equalized_medical_images_using_superpixels?utm_source=chatgpt.com) [oai_citation:2â€¡arxiv.org](https://arxiv.org/abs/2204.05278?utm_source=chatgpt.com) [oai_citation:3â€¡scikit-image.org](https://scikit-image.org/skimage-tutorials/lectures/three_dimensional_image_processing.html?utm_source=chatgpt.com)\n\n    sigma_vox = np.array([1.0 / s for s in spacing])  # blur by 1 mm across axes\n    data = gaussian_filter(data, sigma=sigma_vox)\n\n    data = exposure.equalize_hist(data)\n\n    # Apply slice-wise CLAHE for 3D volume\n    data = np.stack([equalize_adapthist(slice_, clip_limit=0.03)\n                       for slice_ in data], axis=0)\n\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.541582Z","iopub.execute_input":"2025-07-16T22:36:36.541834Z","iopub.status.idle":"2025-07-16T22:36:36.563498Z","shell.execute_reply.started":"2025-07-16T22:36:36.541818Z","shell.execute_reply":"2025-07-16T22:36:36.563003Z"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":"from skimage.segmentation import slic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.564211Z","iopub.execute_input":"2025-07-16T22:36:36.564423Z","iopub.status.idle":"2025-07-16T22:36:36.587085Z","shell.execute_reply.started":"2025-07-16T22:36:36.564402Z","shell.execute_reply":"2025-07-16T22:36:36.586387Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"# Parametri SLIC: n_segments definisce quanti supervoxels circa si voglion\napply_SLIC = lambda data, spacing, n_supervoxels: slic(\n                preprocessing_for_SLIC(data), \n                n_segments=n_supervoxels, \n                compactness=0.2,\n                spacing=spacing,\n                start_label=0,\n                max_num_iter=10, \n                channel_axis=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.587710Z","iopub.execute_input":"2025-07-16T22:36:36.587904Z","iopub.status.idle":"2025-07-16T22:36:36.603055Z","shell.execute_reply.started":"2025-07-16T22:36:36.587889Z","shell.execute_reply":"2025-07-16T22:36:36.602482Z"}},"outputs":[],"execution_count":143},{"cell_type":"markdown","source":"### Initialize predictor","metadata":{}},{"cell_type":"code","source":"# 2) Initialise predictor ------------------------\npredictor = CustomNNUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=False, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=False,\n    verbose_preprocessing=False,\n    allow_tqdm=False #it interfere with SHAP loading bar\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:36.603910Z","iopub.execute_input":"2025-07-16T22:36:36.604166Z","iopub.status.idle":"2025-07-16T22:36:37.642219Z","shell.execute_reply.started":"2025-07-16T22:36:36.604145Z","shell.execute_reply":"2025-07-16T22:36:37.641528Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n","output_type":"stream"}],"execution_count":144},{"cell_type":"markdown","source":"# Define a ROI to explain segmentation in. \nMaybe this will provide a more useful attribution map, highlighting nearby organs","metadata":{}},{"cell_type":"code","source":"# get the manually derived ROI mask from the dataset, where we manually added it\nROI_segmentation_mask_path = \"/kaggle/input/segmentation-masked-ROI.nii\"\nROI_segmentation_mask = nib.load(ROI_segmentation_mask_path)\n\nprint(ROI_segmentation_mask.get_fdata().shape)\nprint(ROI_segmentation_mask.affine)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:37.643001Z","iopub.execute_input":"2025-07-16T22:36:37.643265Z","iopub.status.idle":"2025-07-16T22:36:37.816825Z","shell.execute_reply.started":"2025-07-16T22:36:37.643239Z","shell.execute_reply":"2025-07-16T22:36:37.816064Z"}},"outputs":[{"name":"stdout","text":"(512, 512, 283)\n[[-1.17187500e+00  0.00000000e+00  0.00000000e+00  3.00000000e+02]\n [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.86100006e+02]\n [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.73950000e+03]\n [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n","output_type":"stream"}],"execution_count":145},{"cell_type":"markdown","source":"### ROI mask is a binary mask highlighting the lymphnodes of interest. We need a bounding box to crop the volume accordingly","metadata":{}},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\n\ndef get_mask_bbox_slices(mask_nii_path):\n    \"\"\"\n    Load a binary ROI mask NIfTI and compute the minimal 3D bounding\n    box slices containing all positive voxels.\n\n    Parameters\n    ----------\n    mask_nii_path : str or Path\n        Path to the input binary ROI mask NIfTI (.nii or .nii.gz).\n\n    Returns\n    -------\n    bbox_slices : tuple of slice\n        A 3-tuple of Python slice objects (x_slice, y_slice, z_slice)\n        defining the minimal bounding box.\n    \"\"\"\n    # 1) Load mask\n    nii = nib.load(str(mask_nii_path))\n    data = nii.get_fdata()\n    if data.ndim != 3:\n        raise ValueError(\"Input NIfTI must be a 3D volume\")\n    \n    # 2) Find indices of positive voxels\n    pos_voxels = np.argwhere(data > 0)\n    if pos_voxels.size == 0:\n        raise ValueError(\"No positive voxels found in mask\")\n    \n    # 3) Compute min/max per axis\n    x_min, y_min, z_min = pos_voxels.min(axis=0)\n    x_max, y_max, z_max = pos_voxels.max(axis=0)\n    \n    # 4) Build slice objects (end is exclusive, hence +1)\n    bbox_slices = (\n        slice(int(x_min), int(x_max) + 1),\n        slice(int(y_min), int(y_max) + 1),\n        slice(int(z_min), int(z_max) + 1),\n    )\n    \n    return bbox_slices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:37.817521Z","iopub.execute_input":"2025-07-16T22:36:37.817756Z","iopub.status.idle":"2025-07-16T22:36:37.823499Z","shell.execute_reply.started":"2025-07-16T22:36:37.817739Z","shell.execute_reply":"2025-07-16T22:36:37.822752Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"x_slice, y_slice, z_slice = get_mask_bbox_slices(ROI_segmentation_mask_path)\nprint(\"Bounding box slices:\")\nprint(\"  x:\", x_slice)\nprint(\"  y:\", y_slice)\nprint(\"  z:\", z_slice)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:37.828282Z","iopub.execute_input":"2025-07-16T22:36:37.828527Z","iopub.status.idle":"2025-07-16T22:36:39.335975Z","shell.execute_reply.started":"2025-07-16T22:36:37.828512Z","shell.execute_reply":"2025-07-16T22:36:39.335065Z"}},"outputs":[{"name":"stdout","text":"Bounding box slices:\n  x: slice(219, 351, None)\n  y: slice(189, 316, None)\n  z: slice(156, 182, None)\n","output_type":"stream"}],"execution_count":147},{"cell_type":"markdown","source":"### the identified region is our ROI bounding box","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef slices_to_binary_mask(volume_shape, bbox_slices, dtype=np.uint8):\n    \"\"\"\n    Create a binary mask of given shape where voxels inside the provided\n    3D boundingâ€box slices are set to 1, and all others to 0.\n\n    Parameters\n    ----------\n    volume_shape : tuple of int\n        The full 3D volume dimensions, e.g. (X, Y, Z).\n    bbox_slices : tuple of slice\n        A 3â€tuple of slice objects (x_slice, y_slice, z_slice) defining\n        the region to mask.\n    dtype : dataâ€type, optional\n        The desired dataâ€type of the output mask (default: np.uint8).\n\n    Returns\n    -------\n    mask : np.ndarray\n        A binary mask array of shape `volume_shape`, with ones in the\n        region defined by `bbox_slices` and zeros elsewhere.\n    \"\"\"\n    if len(volume_shape) != len(bbox_slices):\n        raise ValueError(f\"volume_shape has {len(volume_shape)} dimensions, \"\n                         f\"but bbox_slices has {len(bbox_slices)} slices\")\n\n    # Initialize mask to zeros\n    mask = np.zeros(volume_shape, dtype=dtype)\n    # Set the boundingâ€box region to 1\n    mask[bbox_slices] = 1\n\n    return mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:39.336986Z","iopub.execute_input":"2025-07-16T22:36:39.337231Z","iopub.status.idle":"2025-07-16T22:36:39.342138Z","shell.execute_reply.started":"2025-07-16T22:36:39.337214Z","shell.execute_reply":"2025-07-16T22:36:39.341461Z"}},"outputs":[],"execution_count":148},{"cell_type":"code","source":"ROI_mask_path = \"ROI_binary_mask.nii.gz\"\nshape = ROI_segmentation_mask.get_fdata().shape\nprint(\"shape\", shape)\nROI_mask = slices_to_binary_mask(\n    volume_shape=shape,\n    bbox_slices=(x_slice,y_slice,z_slice),\n)\nnib.save(\n        nib.Nifti1Image(ROI_mask, affine=ROI_segmentation_mask.affine),\n        ROI_mask_path\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:39.343084Z","iopub.execute_input":"2025-07-16T22:36:39.343293Z","iopub.status.idle":"2025-07-16T22:36:40.220308Z","shell.execute_reply.started":"2025-07-16T22:36:39.343271Z","shell.execute_reply":"2025-07-16T22:36:40.219669Z"}},"outputs":[{"name":"stdout","text":"shape (512, 512, 283)\n","output_type":"stream"}],"execution_count":149},{"cell_type":"markdown","source":"### to **crop** correctly the volume around the *ROI*, we need to derive the **receptive field** of the sliding window inference, that depends on the *patch size*.","metadata":{}},{"cell_type":"code","source":"patch_size = np.array(predictor.configuration_manager.patch_size)\nprint(\"Patch size: \", patch_size)\n\n# Receptive field is twice the patch size-1\nRF = 2*(patch_size-1)\nprint(\"Receptive field: \", RF)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:40.221055Z","iopub.execute_input":"2025-07-16T22:36:40.221236Z","iopub.status.idle":"2025-07-16T22:36:40.226973Z","shell.execute_reply.started":"2025-07-16T22:36:40.221222Z","shell.execute_reply":"2025-07-16T22:36:40.225886Z"}},"outputs":[{"name":"stdout","text":"Patch size:  [ 72 160 160]\nReceptive field:  [142 318 318]\n","output_type":"stream"}],"execution_count":150},{"cell_type":"markdown","source":"### Consider the receptive field to compute the final slices for cropping\nRemember that model metadata are related to transposed volume (nnunetv2 takes (D, H, W) shape)","metadata":{}},{"cell_type":"code","source":"volume_path = ct_img_path\nvolume_shape = nib.load(volume_path).shape # (W, H, D) -> x, y, z\n# RF shape -> (D, H, W) -> z, y, x (model input shape)\nprint(\"Original volume shape:\", volume_shape)\n\nW, H, D = volume_shape\n\n# backward sorted receptive field axes\nRF_x, RF_y, RF_z = RF[2],RF[1],RF[0]\n\nx_slice_RF = slice(int(max(x_slice.start - RF_x/2, 0)), int(min(x_slice.stop + RF_x/2, W)))\ny_slice_RF = slice(int(max(y_slice.start - RF_y/2, 0)), int(min(y_slice.stop + RF_y/2, H)))\nz_slice_RF = slice(int(max(z_slice.start - RF_z/2, 0)), int(min(z_slice.stop + RF_z/2, D)))\n\nprint(\"new  x:\", x_slice_RF)\nprint(\"new  y:\", y_slice_RF)\nprint(\"new  z:\", z_slice_RF)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:40.228041Z","iopub.execute_input":"2025-07-16T22:36:40.228321Z","iopub.status.idle":"2025-07-16T22:36:40.249046Z","shell.execute_reply.started":"2025-07-16T22:36:40.228302Z","shell.execute_reply":"2025-07-16T22:36:40.248325Z"}},"outputs":[{"name":"stdout","text":"Original volume shape: (512, 512, 283)\nnew  x: slice(60, 510, None)\nnew  y: slice(30, 475, None)\nnew  z: slice(85, 253, None)\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\n\ndef crop_volume_and_affine(nii_path, bbox_slices, save_cropped_nii_path=None):\n    \"\"\"\n    Crop a 3D NIfTI volume using the given bounding-box slices and\n    recompute the affine so the cropped volume retains correct world coordinates.\n\n    Parameters\n    ----------\n    nii_path : str or Path\n        Path to the input NIfTI volume (.nii or .nii.gz).\n    bbox_slices : tuple of slice\n        A 3-tuple (x_slice, y_slice, z_slice) as returned by get_mask_bbox_slices().\n    save_cropped_nii_path : str or Path, optional\n        If provided, the cropped volume will be saved here as a new NIfTI.\n\n    Returns\n    -------\n    cropped_data : np.ndarray\n        The volume data cropped to the bounding box.\n    new_affine : np.ndarray\n        The updated 4Ã—4 affine transform for the cropped volume.\n    \"\"\"\n    # 1) Load the original image\n    img = nib.load(str(nii_path))\n    data = img.get_fdata()\n    affine = img.affine\n\n    # 2) Crop the data array\n    cropped_data = data[bbox_slices]\n\n    # 3) Extract the voxelâ€offsets for x, y, z from the slice starts\n    x_slice, y_slice, z_slice = bbox_slices\n    z0, y0, x0 = z_slice.start, y_slice.start, x_slice.start\n\n    # 4) Compute the new affine translation: shift the origin by the voxel offsets\n    # Note voxel coordinates are (i, j, k) = (x, y, z)\n    offset_vox = np.array([x0, y0, z0])\n    new_affine = affine.copy()\n    new_affine[:3, 3] += affine[:3, :3].dot(offset_vox)\n\n    # 5) Optionally save the cropped volume\n    if save_cropped_nii_path is not None:\n        cropped_img = nib.Nifti1Image(cropped_data, new_affine)\n        nib.save(cropped_img, str(save_cropped_nii_path))\n\n    return cropped_data, new_affine\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:40.249705Z","iopub.execute_input":"2025-07-16T22:36:40.249901Z","iopub.status.idle":"2025-07-16T22:36:40.265541Z","shell.execute_reply.started":"2025-07-16T22:36:40.249887Z","shell.execute_reply":"2025-07-16T22:36:40.265002Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"from pathlib import Path\n\nslices = (x_slice_RF, y_slice_RF, z_slice_RF)\n\nnii_path = ct_img_path\n\ncropped_volume, affine_cropped_volume = crop_volume_and_affine(\n    nii_path=nii_path,\n    bbox_slices=slices,\n    save_cropped_nii_path=Path(\"cropped_volume_with_RF.nii.gz\")\n)\n\nprint(\"Cropped data shape:\", cropped_volume.shape)\nprint(\"New affine:\\n\", affine_cropped_volume)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:40.266346Z","iopub.execute_input":"2025-07-16T22:36:40.266675Z","iopub.status.idle":"2025-07-16T22:36:43.344066Z","shell.execute_reply.started":"2025-07-16T22:36:40.266640Z","shell.execute_reply":"2025-07-16T22:36:43.343386Z"}},"outputs":[{"name":"stdout","text":"Cropped data shape: (450, 445, 168)\nNew affine:\n [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  2.29687500e+02]\n [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.50943756e+02]\n [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.31450000e+03]\n [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n","output_type":"stream"}],"execution_count":153},{"cell_type":"markdown","source":"### We need a way to check original mask overlapping in the new cropped volume","metadata":{}},{"cell_type":"code","source":"cropped_ROI_segmentation_mask, affine_ROI_segmentation_cropped_mask = crop_volume_and_affine(\n    nii_path=ROI_segmentation_mask_path,\n    bbox_slices=slices,\n    save_cropped_nii_path=Path(\"cropped_mask_with_RF.nii.gz\")\n)\n\nprint(\"Cropped mask shape:\", cropped_ROI_segmentation_mask.shape)\nprint(\"New mask affine:\\n\", affine_ROI_segmentation_cropped_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:43.344898Z","iopub.execute_input":"2025-07-16T22:36:43.345439Z","iopub.status.idle":"2025-07-16T22:36:44.441895Z","shell.execute_reply.started":"2025-07-16T22:36:43.345413Z","shell.execute_reply":"2025-07-16T22:36:44.441059Z"}},"outputs":[{"name":"stdout","text":"Cropped mask shape: (450, 445, 168)\nNew mask affine:\n [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  2.29687500e+02]\n [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.50943756e+02]\n [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.31450000e+03]\n [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n","output_type":"stream"}],"execution_count":154},{"cell_type":"markdown","source":"### We also need a mask to correctly ignoring out-of ROI context in our aggregation metrics","metadata":{}},{"cell_type":"code","source":"cropped_ROI_mask, affine_ROI_cropped_mask = crop_volume_and_affine(\n    nii_path=ROI_mask_path,\n    bbox_slices=slices,\n    save_cropped_nii_path=Path(\"cropped_mask_with_RF.nii.gz\")\n)\n\nprint(\"Cropped mask shape:\", cropped_ROI_mask.shape)\nprint(\"New mask affine:\\n\", affine_ROI_segmentation_cropped_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:44.442752Z","iopub.execute_input":"2025-07-16T22:36:44.443106Z","iopub.status.idle":"2025-07-16T22:36:45.770652Z","shell.execute_reply.started":"2025-07-16T22:36:44.443084Z","shell.execute_reply":"2025-07-16T22:36:45.769909Z"}},"outputs":[{"name":"stdout","text":"Cropped mask shape: (450, 445, 168)\nNew mask affine:\n [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  2.29687500e+02]\n [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.50943756e+02]\n [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.31450000e+03]\n [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n","output_type":"stream"}],"execution_count":155},{"cell_type":"markdown","source":"## We execute SHAP on this cropped image, and we only consider our ROI","metadata":{}},{"cell_type":"markdown","source":"### set device","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:45.771351Z","iopub.execute_input":"2025-07-16T22:36:45.771544Z","iopub.status.idle":"2025-07-16T22:36:45.775834Z","shell.execute_reply.started":"2025-07-16T22:36:45.771530Z","shell.execute_reply":"2025-07-16T22:36:45.775069Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":156},{"cell_type":"code","source":"# (a) load + cropped volume  (1, C, D, H, W) â€“ nnU-Net order\nnii_path_cropped = \"cropped_volume_with_RF.nii.gz\"\ndataset_json_path = Path(model_dir) / \"dataset.json\"\n\nvolume_np = nnunetv2_default_preprocessing(nii_path_cropped, predictor, dataset_json_path)\n\nvolume = torch.from_numpy(volume_np).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\n\nprint(\"Volume shape:\", volume.shape)                # (1, C, D, H, W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:45.776609Z","iopub.execute_input":"2025-07-16T22:36:45.776859Z","iopub.status.idle":"2025-07-16T22:36:47.783239Z","shell.execute_reply.started":"2025-07-16T22:36:45.776843Z","shell.execute_reply":"2025-07-16T22:36:47.782504Z"}},"outputs":[{"name":"stdout","text":"Volume shape: torch.Size([1, 1, 168, 445, 450])\n","output_type":"stream"}],"execution_count":157},{"cell_type":"code","source":"SUPERVOXEL_TYPE = \"FCC\"\nUSE_SAVED_MAP = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:47.784164Z","iopub.execute_input":"2025-07-16T22:36:47.784424Z","iopub.status.idle":"2025-07-16T22:36:47.788110Z","shell.execute_reply.started":"2025-07-16T22:36:47.784406Z","shell.execute_reply":"2025-07-16T22:36:47.787397Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"# (b) super-voxel / organ-id map  (W, H, D)\n# Load the image\nimg = nib.load(nii_path_cropped)\n\nif SUPERVOXEL_TYPE == \"FCC\":\n    supervoxel_map_path = 'FCC-supervoxel_map.nii.gz'\n    if USE_SAVED_MAP and os.path.exists(supervoxel_map_path):\n        supervoxel_map = nib.load(supervoxel_map_path).get_fdata()\n    else:\n        # Generate and save supervoxel map using the FCC\n        cube_side = 100.00 # [mm]\n        supervoxel_map = generate_supervoxel_map(img, S=cube_side) # (D, H, W)\n        \n\nelif SUPERVOXEL_TYPE == \"SLIC\":\n    supervoxel_map_path = 'SLIC_supervoxel_map.nii.gz'\n    if USE_SAVED_MAP and os.path.exists(supervoxel_map_path):\n        supervoxel_map = nib.load(supervoxel_map_path).get_fdata()\n    else:\n        # compute spacing for SLIC\n        data = img.get_fdata()\n        affine = img.affine\n        data = np.array(data, dtype=np.float32)\n        \n        # derive slic spacing from affine\n        spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))\n        spacing = tuple(spacing)  # convert to tuple for slic\n        \n        # Generate and save supervoxel map using SLIC\n        n_supervoxels = 380\n        supervoxel_map = apply_SLIC(data, spacing, n_supervoxels)\n        \n        # Save the result\n        supervoxel_map_img = nib.Nifti1Image(supervoxel_map.astype(np.int32), affine=affine)\n        nib.save(supervoxel_map_img, 'SLIC_supervoxel_map.nii.gz')\n        \nelse:\n    raise ValueError()\n\nprint(\"Mappa supervoxel shape:\", supervoxel_map.shape)\nn_supervoxels = len(np.unique(supervoxel_map))\nprint(\"Numero di supervoxels:\", n_supervoxels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:36:47.788837Z","iopub.execute_input":"2025-07-16T22:36:47.789043Z","iopub.status.idle":"2025-07-16T22:37:07.980084Z","shell.execute_reply.started":"2025-07-16T22:36:47.789028Z","shell.execute_reply":"2025-07-16T22:37:07.979364Z"}},"outputs":[{"name":"stdout","text":"Number of supervoxel centers: 384\nMappa supervoxel shape: (450, 445, 168)\nNumero di supervoxels: 384\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\n# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\nsv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\n\nsupervoxel_map = indexes.reshape(supervoxel_map.shape)\nprint(\"number of supervoxels: \", np.unique(supervoxel_map).size)\n\n# IMPORTANT ðŸ”¸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\nsupervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\n\nprint(\"Mask shape:\", supervoxel_map.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:07.980940Z","iopub.execute_input":"2025-07-16T22:37:07.981179Z","iopub.status.idle":"2025-07-16T22:37:10.546760Z","shell.execute_reply.started":"2025-07-16T22:37:07.981158Z","shell.execute_reply":"2025-07-16T22:37:10.545927Z"}},"outputs":[{"name":"stdout","text":"number of supervoxels:  384\nMask shape: torch.Size([168, 445, 450])\n","output_type":"stream"}],"execution_count":160},{"cell_type":"markdown","source":"### derive baseline cached dictionary\nusing planner, iterator, predictor (just temporary solution)","metadata":{}},{"cell_type":"code","source":"# this cause the death, try to isolate the problem\n\nimport math\nimport multiprocessing\nimport shutil\nfrom time import sleep\nfrom typing import Tuple\n\nimport SimpleITK\nimport numpy as np\nimport pandas as pd\nfrom batchgenerators.utilities.file_and_folder_operations import *\nfrom tqdm import tqdm\n\nimport nnunetv2\nfrom nnunetv2.paths import nnUNet_preprocessed, nnUNet_raw\nfrom nnunetv2.preprocessing.cropping.cropping import crop_to_nonzero\nfrom nnunetv2.preprocessing.resampling.default_resampling import compute_new_shape\nfrom nnunetv2.training.dataloading.nnunet_dataset import nnUNetDatasetBlosc2\nfrom nnunetv2.utilities.dataset_name_id_conversion import maybe_convert_to_dataset_name\nfrom nnunetv2.utilities.find_class_by_name import recursive_find_python_class\nfrom nnunetv2.utilities.plans_handling.plans_handler import PlansManager, ConfigurationManager\nfrom nnunetv2.utilities.utils import get_filenames_of_train_images_and_targets\n\ndef run_case_npy(preprocessor, data: np.ndarray, seg: Union[np.ndarray, None], properties: dict,\n                     plans_manager: PlansManager, configuration_manager: ConfigurationManager,\n                     dataset_json: Union[dict, str]):\n        # let's not mess up the inputs!\n        print(1)\n        data = data.astype(np.float32)  # this creates a copy\n        if seg is not None:\n            assert data.shape[1:] == seg.shape[1:], \"Shape mismatch between image and segmentation. Please fix your dataset and make use of the --verify_dataset_integrity flag to ensure everything is correct\"\n            seg = np.copy(seg)\n\n        has_seg = seg is not None\n\n        # apply transpose_forward, this also needs to be applied to the spacing!\n        print(2)\n        data = data.transpose([0, *[i + 1 for i in plans_manager.transpose_forward]])\n        print(3)\n        if seg is not None:\n            seg = seg.transpose([0, *[i + 1 for i in plans_manager.transpose_forward]])\n        print(4)\n        original_spacing = [properties['spacing'][i] for i in plans_manager.transpose_forward]\n\n        print(5)\n        # crop, remember to store size before cropping!\n        shape_before_cropping = data.shape[1:]\n        properties['shape_before_cropping'] = shape_before_cropping\n        # this command will generate a segmentation. This is important because of the nonzero mask which we may need\n        print(6)\n        data, seg, bbox = crop_to_nonzero(data, seg)\n        properties['bbox_used_for_cropping'] = bbox\n        # print(data.shape, seg.shape)\n        properties['shape_after_cropping_and_before_resampling'] = data.shape[1:]\n\n        # resample\n        target_spacing = configuration_manager.spacing  # this should already be transposed\n\n        if len(target_spacing) < len(data.shape[1:]):\n            # target spacing for 2d has 2 entries but the data and original_spacing have three because everything is 3d\n            # in 2d configuration we do not change the spacing between slices\n            target_spacing = [original_spacing[0]] + target_spacing\n\n        print(7)\n        new_shape = compute_new_shape(data.shape[1:], original_spacing, target_spacing)\n\n        # normalize\n        # normalization MUST happen before resampling or we get huge problems with resampled nonzero masks no\n        # longer fitting the images perfectly!\n        print(8)\n        data = preprocessor._normalize(data, seg, configuration_manager,\n                               plans_manager.foreground_intensity_properties_per_channel)\n\n        # print('current shape', data.shape[1:], 'current_spacing', original_spacing,\n        #       '\\ntarget shape', new_shape, 'target_spacing', target_spacing)\n        print(9)\n        old_shape = data.shape[1:]\n        data = configuration_manager.resampling_fn_data(data, new_shape, original_spacing, target_spacing)\n        print(10)\n        seg = configuration_manager.resampling_fn_seg(seg, new_shape, original_spacing, target_spacing)\n        if preprocessor.verbose:\n            print(f'old shape: {old_shape}, new_shape: {new_shape}, old_spacing: {original_spacing}, '\n                  f'new_spacing: {target_spacing}, fn_data: {configuration_manager.resampling_fn_data}')\n\n        # if we have a segmentation, sample foreground locations for oversampling and add those to properties\n        if has_seg:\n            # reinstantiating LabelManager for each case is not ideal. We could replace the dataset_json argument\n            # with a LabelManager Instance in this function because that's all its used for. Dunno what's better.\n            # LabelManager is pretty light computation-wise.\n            print(11)\n            label_manager = plans_manager.get_label_manager(dataset_json)\n            print(12)\n            collect_for_this = label_manager.foreground_regions if label_manager.has_regions \\\n                else label_manager.foreground_labels\n\n            # when using the ignore label we want to sample only from annotated regions. Therefore we also need to\n            # collect samples uniformly from all classes (incl background)\n            if label_manager.has_ignore_label:\n                collect_for_this.append([-1] + label_manager.all_labels)\n\n            # no need to filter background in regions because it is already filtered in handle_labels\n            # print(all_labels, regions)\n            print(13)\n            properties['class_locations'] = preprocessor._sample_foreground_locations(seg, collect_for_this,\n                                                                                   verbose=preprocessor.verbose)\n            print(14)\n            seg = preprocessor.modify_seg_fn(seg, plans_manager, dataset_json, configuration_manager)\n        print(15)\n        if np.max(seg) > 127:\n            print(16)\n            seg = seg.astype(np.int16)\n        else:\n            print(17)\n            seg = seg.astype(np.int8)\n        print(18)\n        return data, seg, properties","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-16T22:37:10.547649Z","iopub.execute_input":"2025-07-16T22:37:10.547949Z","iopub.status.idle":"2025-07-16T22:37:10.562496Z","shell.execute_reply.started":"2025-07-16T22:37:10.547923Z","shell.execute_reply":"2025-07-16T22:37:10.561744Z"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"def get_cached_output_dictionary(volume_file: Path,\n                                 predictor: CustomNNUNetPredictor,\n                                verbose: bool = False) -> dict:\n        \"\"\"\n        Return a dictionary indexed by the slices for the sliding window, of the output of the inference for each patch\n        of the original volume\n        \"\"\"\n     \n        rw = predictor.plans_manager.image_reader_writer_class()\n\n        # If nnU-Net returns a class instead of an instance, instantiate it\n        if callable(rw) and not hasattr(rw, \"read_images\"):\n            rw = rw()\n\n        orig_image, orig_props = rw.read_images(\n            [str(volume_file)]\n        )                     # (C, Z, Y, X)\n    \n        preprocessor = predictor.configuration_manager.preprocessor_class()\n        # the following cause the kernel death at first notebook run\n        data_pp, _, _ = preprocessor.run_case_npy(\n                orig_image,\n                seg=None,\n                properties=orig_props,\n                plans_manager=predictor.plans_manager,\n                configuration_manager=predictor.configuration_manager,\n                dataset_json=predictor.dataset_json\n            )\n     \n        # to torch, channel-first is already true\n        inp_tensor = torch.from_numpy(data_pp)\n\n        slicers = predictor._internal_get_sliding_window_slicers(inp_tensor.shape[1:])\n       \n        if verbose:\n            print(\"first 3 slicers of Iterator object: \", slicers[:3])\n\n        dictionary = predictor.get_output_dictionary_sliding_window(inp_tensor, slicers)\n\n        return dictionary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:10.563301Z","iopub.execute_input":"2025-07-16T22:37:10.563560Z","iopub.status.idle":"2025-07-16T22:37:10.587656Z","shell.execute_reply.started":"2025-07-16T22:37:10.563533Z","shell.execute_reply":"2025-07-16T22:37:10.586970Z"}},"outputs":[],"execution_count":162},{"cell_type":"code","source":"USE_STORED_DICTIONARY = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:10.588477Z","iopub.execute_input":"2025-07-16T22:37:10.589043Z","iopub.status.idle":"2025-07-16T22:37:10.609028Z","shell.execute_reply.started":"2025-07-16T22:37:10.589018Z","shell.execute_reply":"2025-07-16T22:37:10.608400Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"import pickle as pkl\n\nif USE_STORED_DICTIONARY and os.path.exists(\"cropped_baseline_output_dictionary_cache.pkl\"):\n    with open(\"cropped_baseline_output_dictionary_cache.pkl\", \"rb\") as f:\n        cropped_baseline_pred_cache = pkl.load(f)\nelse:\n    cropped_baseline_pred_cache = get_cached_output_dictionary(\n        volume_file = nii_path_cropped,\n        predictor = predictor,\n        verbose = True,\n    )\n    # Write to file\n    with open(\"cropped_baseline_output_dictionary_cache.pkl\", \"wb\") as f:\n        pkl.dump(cropped_baseline_pred_cache, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:10.609799Z","iopub.execute_input":"2025-07-16T22:37:10.609964Z","iopub.status.idle":"2025-07-16T22:37:12.247317Z","shell.execute_reply.started":"2025-07-16T22:37:10.609952Z","shell.execute_reply":"2025-07-16T22:37:12.246611Z"}},"outputs":[],"execution_count":164},{"cell_type":"code","source":"print(cropped_baseline_pred_cache[(None, None, None), (0, 72, None), (0, 160, None), (0, 160, None)].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.248204Z","iopub.execute_input":"2025-07-16T22:37:12.248760Z","iopub.status.idle":"2025-07-16T22:37:12.253551Z","shell.execute_reply.started":"2025-07-16T22:37:12.248722Z","shell.execute_reply":"2025-07-16T22:37:12.252767Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 72, 160, 160])\n","output_type":"stream"}],"execution_count":165},{"cell_type":"markdown","source":"### get our cropped ROI segmentatoin mask","metadata":{}},{"cell_type":"code","source":"# segmentation mask cropped to ROI, with background extended to rf\nROI_segmentation_mask = np.transpose(cropped_ROI_segmentation_mask, (2, 1, 0))\nROI_segmentation_mask = torch.from_numpy(ROI_segmentation_mask).to(device)\n\nprint(ROI_segmentation_mask.shape)\n\n# ROI bounding box with background extended to RF\nROI_mask = np.transpose(cropped_ROI_mask, (2, 1, 0))\nROI_mask = torch.from_numpy(ROI_mask).to(device)\n\nprint(ROI_mask.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.254245Z","iopub.execute_input":"2025-07-16T22:37:12.254425Z","iopub.status.idle":"2025-07-16T22:37:12.656354Z","shell.execute_reply.started":"2025-07-16T22:37:12.254403Z","shell.execute_reply":"2025-07-16T22:37:12.655652Z"}},"outputs":[{"name":"stdout","text":"torch.Size([168, 445, 450])\ntorch.Size([168, 445, 450])\n","output_type":"stream"}],"execution_count":166},{"cell_type":"markdown","source":"### Include masking in the forward function","metadata":{}},{"cell_type":"markdown","source":"### **Chrabaszcz aggregation** Â \n\nLet\n\n* $z_1^{(i)}(x)$ â€“ class-1 logit at voxel $x$ after perturbation *i*\n* $P_i(x)=\\mathbf 1\\!\\left[\\arg\\max_c z_c^{(i)}(x)=1\\right]$ â€“ binary mask of voxels currently predicted as lymph-node\n* no ROI, total volume considered\n\n$$\nS_{\\text{Chr}}^{(i)} \\;=\\;\n\\frac{1}{\\alpha}\\sum_{x} P_i(x)\\;z_1^{(i)}(x)\n$$\n\nwhere $\\alpha$ is the `scaling_factor`.\n\n* **Counts evidence only from voxels the model *currently* labels as class 1.**\n* *False positives (FP):* contribute **positively** (they are in $P_i$).\n* *False negatives (FN):* contribute **zero** (their logit is absent).\n\nSource: Chrabaszcz et al., *Aggregated Attributions for Explanatory Analysis of 3-D Segmentation Models*, 2024.\n","metadata":{}},{"cell_type":"code","source":"def chrabaszcz_aggregation(logits: torch.Tensor,\n                           scaling_factor: float = 1.0,\n                          ) -> torch.Tensor:\n    \"\"\"\n    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n    Explanatory Analysis of 3D Segmentation Models\"\n    \"\"\"\n    seg_mask = (torch.argmax(logits, dim=0) == 1)\n    aggregate = torch.sum(logits[1].double() * seg_mask)\n\n    return aggregate / scaling_factor  # normalize to avoid overflows in SHAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.657084Z","iopub.execute_input":"2025-07-16T22:37:12.657318Z","iopub.status.idle":"2025-07-16T22:37:12.661778Z","shell.execute_reply.started":"2025-07-16T22:37:12.657301Z","shell.execute_reply":"2025-07-16T22:37:12.661041Z"}},"outputs":[],"execution_count":167},{"cell_type":"markdown","source":"\n### **True positive aggregation** (Chrabaszcz aggregation + baseline-mask filtering)\n\nIntroduce the unperturbed prediction $P_0$.\nKeep only voxels that are **still** class 1 *and* were class 1 before:\n\n$$\nS_{\\text{Chr\\,keep}}^{(i)} \\;=\\;\n\\frac{1}{\\alpha}\\sum_{x} \\bigl[P_i(x)\\land P_0(x)\\bigr]\\;z_1^{(i)}(x)\n$$\n\n* **True positives preserved** (TP core) add positive evidence.\n* **FP created by the perturbation** are **ignored** (masked out).\n* **FN** lower the score indirectly because their logits disappear from the sum.\n\nConceptually this is the **positive part** of a signed logit-difference metric.","metadata":{}},{"cell_type":"code","source":"def true_positive_aggregation(logits: torch.Tensor,\n                          unperturbed_binary_mask: torch.Tensor,\n                           scaling_factor: float = 1.0,\n                          ) -> torch.Tensor:\n    \"\"\"\n    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n    Explanatory Analysis of 3D Segmentation Models\", with the  addition of filtering by the unperturbed segmentation.\n    We can use this to ignore \"false positive\" voxels -> only account for true positive contribution;\n    so this corresponds conceptually to the positive part of a logits difference metric\n    \"\"\"\n    seg_mask = (torch.argmax(logits, dim=0) == 1)          # (D,H,W)\n    seg_mask = seg_mask.bool() & unperturbed_binary_mask.bool()  # prefer boolean indexing for reletively sparse tensors\n    aggregate = torch.sum(logits[1].double()[seg_mask])\n\n    return aggregate / scaling_factor  # normalize to avoid overflows in SHAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.662734Z","iopub.execute_input":"2025-07-16T22:37:12.662996Z","iopub.status.idle":"2025-07-16T22:37:12.681038Z","shell.execute_reply.started":"2025-07-16T22:37:12.662981Z","shell.execute_reply":"2025-07-16T22:37:12.680494Z"}},"outputs":[],"execution_count":168},{"cell_type":"markdown","source":"---\n\n### **False-positive aggregation**\n\nDirectly sum class-1 evidence from **new** positives inside ROI:\n\n$$\nS_{\\text{FP}}^{(i)} \\;=\\;\n\\frac{1}{\\alpha}\\sum_{x}\n\\bigl[P_i(x)\\land\\neg P_0(x)\\land R(x)\\bigr]\\;z_1^{(i)}(x)\n$$\n\n* Measures **only** the spurious lymph-node evidence a perturbation introduces.\n* Higher value â‡’ stronger tendency to hallucinate extra nodes.","metadata":{}},{"cell_type":"code","source":"def false_positive_aggregation(logits: torch.Tensor,\n                              unperturbed_binary_mask: torch.Tensor,\n                              ROI_mask: torch.Tensor,\n                              scaling_factor: float = 1.0,\n                              ) -> torch.Tensor:\n    \"\"\"\n    Negative part of signed logit-difference objective, returned with positive sign;\n    Only accounts for false positive voxels in segmentation (spurious lymph nodes)\n    \"\"\"\n    # current segmentation (prevailing class)\n    seg_mask = (torch.argmax(logits, dim=0) == 1)     # (D,H,W) âˆˆ {0,1}\n\n    fp_mask  = seg_mask * ROI_mask * torch.logical_not(unperturbed_binary_mask.bool()).float()   # prefer float multiplication for dense tensors\n    aggregate = torch.sum(logits[1].double() * fp_mask) \n\n    return aggregate / scaling_factor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.681777Z","iopub.execute_input":"2025-07-16T22:37:12.682185Z","iopub.status.idle":"2025-07-16T22:37:12.703175Z","shell.execute_reply.started":"2025-07-16T22:37:12.682162Z","shell.execute_reply":"2025-07-16T22:37:12.702660Z"}},"outputs":[],"execution_count":169},{"cell_type":"markdown","source":"---\n\n### **Dice aggregation (prediction-vs-baseline, ROI-restricted)**\n\nLet $R(x)$ be the ROI mask.\n\n$$\nP_i' = P_i \\odot R, \\qquad\nP_0' = P_0 \\odot R\n$$\n\n$$\nS_{\\text{Dice}}^{(i)} \\;=\\;\n\\frac{1}{\\alpha}\\;\n\\frac{2\\,\\langle P_i',\\,P_0'\\rangle}{\\lVert P_i'\\rVert_1 + \\lVert P_0'\\rVert_1 + \\varepsilon}\n$$\n\n* Drops when either **FP** ($P_i'=1,\\,P_0'=0$) or **FN** ($P_i'=0,\\,P_0'=1$) appear â†’ penalises both error types symmetrically.\n\nBased on the â€œself-consistency Diceâ€ used in MiSuRe (Hasany et al., 2024).\n\n\n\n","metadata":{}},{"cell_type":"code","source":"def dice_aggregation(logits: torch.Tensor,\n                    unperturbed_binary_mask: torch.Tensor,\n                    ROI_mask: torch.Tensor,\n                    scaling_factor: float = 1.0,\n                    ) -> torch.Tensor:\n    \"\"\"\n    Use Dice score, the same aggregation measure from \"Hasany et al. - 2024 - MiSuRe is all you need to explain your image segmentation\"\n    Dice score provides a single aggregation metric that accounts for both false negatives and false positives penalization.\n    Specificly, we instead score each perturbation supervoxels by that Dice => supervoxels that contribute the most in reproducing\n    the baseline segmentation, will get an higher score\n    \"\"\"\n    # 1. Boolean masks restricted to ROI\n    pred = (logits.argmax(dim=0) == 1).float() * ROI_mask.float()\n    base = unperturbed_binary_mask.float()      * ROI_mask.float()\n\n    # 2. Intersection and denominator\n    inter = (pred * base).sum()\n    denom = pred.sum() + base.sum() + eps       # |P| + |B|\n\n    # 3. Dice coefficient\n    dice = (2.0 * inter) / denom\n\n    return dice / scaling_factor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.703815Z","iopub.execute_input":"2025-07-16T22:37:12.704006Z","iopub.status.idle":"2025-07-16T22:37:12.721604Z","shell.execute_reply.started":"2025-07-16T22:37:12.703994Z","shell.execute_reply":"2025-07-16T22:37:12.721123Z"}},"outputs":[],"execution_count":170},{"cell_type":"markdown","source":"---\n\n### **Signed logit-difference (masked)**\n\nDefine a signed weight\n\n$$\nw(x)=\n\\begin{cases}\n+1 & \\text{if } P_0(x)=1\\\\\n-1 & \\text{otherwise}\n\\end{cases},\n\\qquad w(x)\\leftarrow w(x)\\,R(x)\n$$\n\n$$\nS_{\\text{LD}}^{(i)} \\;=\\;\n\\frac{1}{\\alpha}\\sum_{x} P_i(x)\\;w(x)\\;z_1^{(i)}(x)\n$$\n\n* **Positive attribution:** voxels that *keep* the baseline TP (support segmentation).\n* **Negative attribution:** voxels that become class 1 **only** after perturbation (generate FP inside ROI).\n* FN reduce the positive term (logits disappear) but do **not** add negative mass.","metadata":{}},{"cell_type":"code","source":"\n\ndef logit_difference_aggregation(\n        logits: torch.Tensor,\n        unperturbed_binary_mask: torch.Tensor,\n        ROI_mask: torch.Tensor,\n        scaling_factor: float = 1.0,\n) -> torch.Tensor:\n    \"\"\"\n    Signed logit-difference objective *masked by the prevailing class*.\n    \"\"\"\n    # current segmentation (prevailing class)\n    seg_mask = (torch.argmax(logits, dim=0) == 1)     # (D,H,W) âˆˆ {0,1}\n\n    # +1 inside baseline positives, âˆ’1 elsewhere...\n    signed_weight = torch.where(unperturbed_binary_mask.bool(),\n                                torch.tensor(1.0, device=logits.device),\n                                torch.tensor(-1.0, device=logits.device))\n\n    # ... but we only care of false positives inside the ROI (we don't even have the segmentation mask outside the ROI)\n    signed_weight = signed_weight * ROI_mask\n\n    # aggregate signed class-1 evidence, restricted to voxels\n    # that are *currently* predicted as class-1 (seg_mask)\n    aggregate = torch.sum(logits[1] * seg_mask * signed_weight)\n    return aggregate / scaling_factor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.722244Z","iopub.execute_input":"2025-07-16T22:37:12.722426Z","iopub.status.idle":"2025-07-16T22:37:12.744937Z","shell.execute_reply.started":"2025-07-16T22:37:12.722413Z","shell.execute_reply":"2025-07-16T22:37:12.744344Z"}},"outputs":[],"execution_count":171},{"cell_type":"code","source":"# ------------------------------------------------------------\n# â·  Forward wrapper that nnU-Net expects\n# ------------------------------------------------------------\n\n@torch.inference_mode()\ndef forward_segmentation_output_to_explain(\n        input_image:         torch.Tensor,\n        perturbation_mask:   torch.BoolTensor | None,\n        ROI_segmentation_mask:      torch.Tensor,   # remember that must be cropped to the same size of the other tensors\n        ROI_bounding_box_mask:      torch.Tensor,\n        baseline_prediction_dict: dict\n) -> torch.Tensor:           # returns a scalar per sample\n    \"\"\"\n    Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n    by the network â€“ adapt to your real metric as needed.\n    \"\"\"\n    logits = predictor.predict_sliding_window_return_logits_with_caching(\n        input_image, perturbation_mask, baseline_prediction_dict,\n    )                              # (C, D, H, W)\n    # we now mask both by the segmentation prevalent class, and by ROI\n    D,H,W = logits.shape[1:]\n    \"\"\"aggregate = logit_difference_aggregation(\n        logits = logits,\n        unperturbed_binary_mask = ROI_segmentation_mask,\n        ROI_mask = ROI_bounding_box_mask,\n        scaling_factor = (D*H*W)\n    )\"\"\"\\\n    aggregate = true_positive_aggregation(\n                    logits, \n                    ROI_segmentation_mask,\n                    scaling_factor=(D*W*H),\n    )\n\n    return aggregate\n\n# c) wrap your cachedâ€forward method:\nexplainer = KernelShapWithMask(\n    forward_func=lambda vol, _perturbation_mask: forward_segmentation_output_to_explain(\n        input_image=vol,\n        perturbation_mask=_perturbation_mask,\n        ROI_segmentation_mask=ROI_segmentation_mask,\n        ROI_bounding_box_mask=ROI_mask,\n        baseline_prediction_dict=cropped_baseline_pred_cache),\n    surrogate_model = \"lasso\",\n    alpha_surrogate = 0.003,\n    max_iter_surrogate = 10000\n)\n\n# d) compute SHAP\nattr = explainer.attribute(\n    inputs=volume,       # (1,C,D,H,W)\n    baselines=0.0, \n    feature_mask=supervoxel_map,\n    n_samples=2000,    \n    return_input_shape=True,\n    monitor_log_path=None,\n    monitor_convergence_step=10,\n    monitor_local_accuracy_step=20,\n    show_progress=True,\n)\nprint(\"Attributions:\", attr.shape)  # â†’ (1,C,D,H,W)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:37:12.745545Z","iopub.execute_input":"2025-07-16T22:37:12.745738Z","iopub.status.idle":"2025-07-17T04:18:53.674728Z","shell.execute_reply.started":"2025-07-16T22:37:12.745722Z","shell.execute_reply":"2025-07-17T04:18:53.674034Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Kernel Shap With Mask attribution:   0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9e79f423ad4512ac4791bac813fa18"}},"metadata":{}},{"name":"stdout","text":"Attributions: torch.Size([1, 1, 168, 445, 450])\n","output_type":"stream"}],"execution_count":172},{"cell_type":"code","source":"with open('dataset-tp.pkl', 'wb') as file:\n    pickle.dump(explainer.dataset, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T04:18:53.675571Z","iopub.execute_input":"2025-07-17T04:18:53.675859Z","iopub.status.idle":"2025-07-17T04:18:54.888367Z","shell.execute_reply.started":"2025-07-17T04:18:53.675839Z","shell.execute_reply":"2025-07-17T04:18:54.887475Z"}},"outputs":[],"execution_count":173},{"cell_type":"code","source":"attr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\nattr_img = nib.Nifti1Image(attr_postprocessed, affine_cropped_volume)\nnib.save(attr_img, 'attribution_map-TP-Lasso.nii.gz')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T04:18:54.889275Z","iopub.execute_input":"2025-07-17T04:18:54.889522Z","iopub.status.idle":"2025-07-17T04:18:55.513981Z","shell.execute_reply.started":"2025-07-17T04:18:54.889505Z","shell.execute_reply":"2025-07-17T04:18:55.512941Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"with open('dataset-tp.pkl', 'rb') as f:\n    ds = pickle.load(f)\nprint(ds)\nn_samples = 2000\nprint(n_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:48:39.678850Z","iopub.execute_input":"2025-07-17T06:48:39.679470Z","iopub.status.idle":"2025-07-17T06:48:39.690429Z","shell.execute_reply.started":"2025-07-17T06:48:39.679443Z","shell.execute_reply":"2025-07-17T06:48:39.689510Z"}},"outputs":[{"name":"stdout","text":"<torch.utils.data.dataset.TensorDataset object at 0x7d97b7aa1a50>\n2000\n","output_type":"stream"}],"execution_count":180},{"cell_type":"code","source":"data_loader = DataLoader(ds, batch_size=n)\nsurrogate.fit(data_loader)\n\n# Get coefficients using representation() method\nrep = surrogate.representation()\nbeta = torch.as_tensor(rep, dtype=torch.float32).flatten()\nphis = beta      # coefficients\n\nphis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:52.979371Z","iopub.execute_input":"2025-07-17T07:44:52.979708Z","iopub.status.idle":"2025-07-17T07:44:54.256392Z","shell.execute_reply.started":"2025-07-17T07:44:52.979684Z","shell.execute_reply":"2025-07-17T07:44:54.255683Z"}},"outputs":[{"execution_count":260,"output_type":"execute_result","data":{"text/plain":"tensor([0.0000e+00, 0.0000e+00, 3.6019e-04, 6.9423e-04, 6.5505e-04, 1.8952e-04,\n        3.3242e-04, 9.3354e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0150e-04,\n        0.0000e+00, 0.0000e+00, 8.4546e-05, 0.0000e+00, 3.3218e-05, 1.0403e-05,\n        0.0000e+00, 4.0247e-05, 4.9692e-05, 0.0000e+00, 1.0656e-06, 0.0000e+00,\n        2.1299e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7880e-05, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 1.4176e-05, 5.2130e-05, 7.7386e-06, 2.8610e-09,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2438e-05, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 2.6299e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 2.4004e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        2.0359e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2611e-04, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3052e-06, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.2319e-06, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 9.2820e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5437e-05, 0.0000e+00, 1.1829e-06,\n        0.0000e+00, 0.0000e+00, 2.0817e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7727e-05, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 1.2393e-04, 5.3207e-04, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])"},"metadata":{}}],"execution_count":260},{"cell_type":"code","source":"\"\"\"from captum._utils.models.linear_model import SkLearnLinearRegression as LinReg\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nsurrogate = LinReg()\ndata_loader = DataLoader(ds, batch_size=n_samples)\nsurrogate.fit(data_loader)\n\nprint(surrogate.representation())\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:54.257533Z","iopub.execute_input":"2025-07-17T07:44:54.257803Z","iopub.status.idle":"2025-07-17T07:44:54.263295Z","shell.execute_reply.started":"2025-07-17T07:44:54.257782Z","shell.execute_reply":"2025-07-17T07:44:54.262565Z"}},"outputs":[{"execution_count":261,"output_type":"execute_result","data":{"text/plain":"'from captum._utils.models.linear_model import SkLearnLinearRegression as LinReg\\nfrom torch.utils.data import DataLoader, TensorDataset\\n\\n\\nsurrogate = LinReg()\\ndata_loader = DataLoader(ds, batch_size=n_samples)\\nsurrogate.fit(data_loader)\\n\\nprint(surrogate.representation())'"},"metadata":{}}],"execution_count":261},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, Subset\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Extract the tensors\nX, y, w = (t.cpu() for t in ds.tensors)  # interpretable inputs, outputs, weights\n\nprint(f\"Dataset shape: X={X.shape}, y={y.shape}, w={w.shape}\")\nprint(f\"Output range: [{y.min():.6f}, {y.max():.6f}]\")\nprint(f\"Weight range: [{w.min():.6f}, {w.max():.6f}]\")\nprint(f\"First sample: X[0].sum()={X[0].sum()}, y[0]=0.007214, w[0]=1.00e+06\")\n\n# Analyze convergence at different checkpoints\ncheckpoints = [20, 50, 100, 200, 400, 600, 1000, 1500, 2000]\nconv_distances = []\ndeltas = []\ncondition_numbers = []\n\nfrom captum._utils.models.linear_model import SkLearnLinearRegression as LinReg\nfrom captum._utils.models.linear_model import SkLearnLasso\n\nbeta_prev = None\nfor n in checkpoints:\n    # Fit model with first n samples\n    surrogate = SkLearnLasso(alpha=0.0003, max_iter=10000)\n    #surrogate = LinReg()\n    subset = Subset(ds, range(n))\n    data_loader = DataLoader(subset, batch_size=n)\n    surrogate.fit(data_loader)\n    \n    # Get coefficients using representation() method\n    rep = surrogate.representation()\n    beta = torch.as_tensor(rep, dtype=torch.float32).flatten()\n    #phi0 = beta[0].item()\n    phis = beta#beta[1:] representation returns only real weights, no bias\n    \n    # Calculate convergence distance\n    if beta_prev is not None:\n        conv_dist = torch.norm(beta - beta_prev, p=1).item()\n        conv_distances.append(conv_dist)\n    \n    # Calculate delta_shap (local accuracy)\n    f_x = y[0].item()  # unperturbed output\n    delta = abs(f_x - (phi0 + phis.sum().item()))\n    deltas.append(delta)\n    \n    # Calculate condition number of weighted design matrix\n    X_n = X[:n].numpy()\n    w_n = w[:n].numpy()\n    W_sqrt = np.diag(np.sqrt(w_n.flatten()))\n    # Add intercept column\n    X_with_intercept = np.column_stack([np.ones(n), X_n])\n    X_weighted_with_intercept = W_sqrt @ X_with_intercept\n    try:\n        cond = np.linalg.cond(X_weighted_with_intercept)\n        condition_numbers.append(cond)\n    except:\n        condition_numbers.append(np.inf)\n    \n    print(f\"\\nn={n:4d}: delta={delta:.2e}, sum(phis)={phis.sum().item():.6f}\")\n    print(f\"        Num non-zero coefs: {(phis.abs() > 1e-10).sum().item()}\")\n    if len(condition_numbers) > 0:\n        print(f\"        Condition number: {condition_numbers[-1]:.2e}\")\n    \n    beta_prev = beta.detach().clone()\n\n# Plot analysis\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Delta over iterations\naxes[0,0].plot(checkpoints, deltas, 'r.-')\naxes[0,0].set_xlabel('Iterations')\naxes[0,0].set_ylabel('Delta SHAP')\naxes[0,0].set_yscale('log')\naxes[0,0].set_title('Local Accuracy Error')\n\n# Convergence distance\nif conv_distances:\n    axes[0,1].plot(checkpoints[1:], conv_distances, 'b.-')\n    axes[0,1].set_xlabel('Iterations')\n    axes[0,1].set_ylabel('Conv Dist L1')\n    axes[0,1].set_title('Convergence Distance')\n\n# Condition numbers\naxes[1,0].plot(checkpoints, condition_numbers, 'g.-')\naxes[1,0].set_xlabel('Iterations')\naxes[1,0].set_ylabel('Condition Number')\naxes[1,0].set_yscale('log')\naxes[1,0].set_title('Matrix Conditioning')\n\n# Investigate around iteration 400\nprint(\"\\n=== Investigating around iteration 400 ===\")\nwindow = list(range(380, 420, 2))  # every 2 iterations to save time\ndeltas_window = []\nfor n in window:\n    surrogate = SkLearnLasso(alpha=0.0003, max_iter=10000)\n    #surrogate = LinReg()\n    subset = Subset(ds, range(n))\n    data_loader = DataLoader(subset, batch_size=n)\n    surrogate.fit(data_loader)\n    \n    rep = surrogate.representation()\n    beta = torch.as_tensor(rep, dtype=torch.float32).flatten()\n    #phi0 = beta[0].item()\n    phis = beta#beta[1:] representation returns only real weights, no bias\n    \n    delta = abs(y[0].item() - (phi0 + phis.sum().item()))\n    deltas_window.append(delta)\n\naxes[1,1].plot(window, deltas_window, 'k.-')\naxes[1,1].set_xlabel('Iterations')\naxes[1,1].set_ylabel('Delta SHAP')\naxes[1,1].set_title('Zoom around iteration 400')\n\nplt.tight_layout()\nplt.show()\n\n# Check specific properties\nprint(\"\\n=== Additional Analysis ===\")\nprint(f\"Number of unique samples: {len(torch.unique(X, dim=0))}\")\nprint(f\"Number of all-zero samples: {(X.sum(dim=1) == 0).sum()}\")\nprint(f\"Number of all-one samples: {(X.sum(dim=1) == X.shape[1]).sum()}\")\n\n# Check sample diversity\nprint(f\"\\n=== Sample Diversity ===\")\nfor k in [1, 5, 10, 50, 100, 383]:\n    mask_k = (X.sum(dim=1) == k)\n    print(f\"Samples with exactly {k} features active: {mask_k.sum()}\")\n\n# Look for the problematic sample around 400\nprint(f\"\\n=== Samples around position 400 ===\")\nfor i in range(395, 405):\n    print(f\"Sample {i}: sum={X[i].sum():.0f}, y={y[i]:.6f}, w={w[i]:.0f}\")\n\n# Check if there are any extreme outliers in outputs\nprint(f\"\\n=== Output distribution ===\")\nsorted_outputs = torch.sort(y)[0]\nprint(f\"Min 5 outputs: {sorted_outputs[:5]}\")\nprint(f\"Max 5 outputs: {sorted_outputs[-5:]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:54.263961Z","iopub.execute_input":"2025-07-17T07:44:54.264182Z","iopub.status.idle":"2025-07-17T07:44:58.994647Z","shell.execute_reply.started":"2025-07-17T07:44:54.264164Z","shell.execute_reply":"2025-07-17T07:44:58.993865Z"}},"outputs":[{"name":"stdout","text":"Dataset shape: X=torch.Size([2000, 384]), y=torch.Size([2000]), w=torch.Size([2000])\nOutput range: [0.000000, 0.007278]\nWeight range: [1.000000, 1000000.000000]\nFirst sample: X[0].sum()=384.0, y[0]=0.007214, w[0]=1.00e+06\n\nn=  20: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 6\n        Condition number: 2.43e+17\n\nn=  50: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 8\n        Condition number: 4.68e+17\n\nn= 100: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 8\n        Condition number: 5.01e+17\n\nn= 200: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 5\n        Condition number: 1.09e+18\n\nn= 400: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 7\n        Condition number: 1.14e+06\n\nn= 600: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 20\n        Condition number: 3.24e+04\n\nn=1000: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 22\n        Condition number: 9.48e+03\n\nn=1500: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 24\n        Condition number: 5.41e+03\n\nn=2000: delta=1.20e-03, sum(phis)=0.006014\n        Num non-zero coefs: 33\n        Condition number: 3.87e+03\n\n=== Investigating around iteration 400 ===\n\n=== Additional Analysis ===\nNumber of unique samples: 1945\nNumber of all-zero samples: 1\nNumber of all-one samples: 1\n\n=== Sample Diversity ===\nSamples with exactly 1 features active: 155\nSamples with exactly 5 features active: 37\nSamples with exactly 10 features active: 17\nSamples with exactly 50 features active: 6\nSamples with exactly 100 features active: 0\nSamples with exactly 383 features active: 149\n\n=== Samples around position 400 ===\nSample 395: sum=203, y=0.000786, w=1\nSample 396: sum=310, y=0.003195, w=1\nSample 397: sum=1, y=0.000000, w=1\nSample 398: sum=383, y=0.007214, w=1\nSample 399: sum=76, y=0.000000, w=1\nSample 400: sum=6, y=0.000000, w=1\nSample 401: sum=15, y=0.000000, w=1\nSample 402: sum=307, y=0.005777, w=1\nSample 403: sum=360, y=0.004645, w=1\nSample 404: sum=373, y=0.003181, w=1\n\n=== Output distribution ===\nMin 5 outputs: tensor([0., 0., 0., 0., 0.])\nMax 5 outputs: tensor([0.0072, 0.0073, 0.0073, 0.0073, 0.0073])\n","output_type":"stream"}],"execution_count":262},{"cell_type":"code","source":"phis.sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:58.995349Z","iopub.execute_input":"2025-07-17T07:44:58.995532Z","iopub.status.idle":"2025-07-17T07:44:59.001389Z","shell.execute_reply.started":"2025-07-17T07:44:58.995518Z","shell.execute_reply":"2025-07-17T07:44:59.000765Z"}},"outputs":[{"execution_count":263,"output_type":"execute_result","data":{"text/plain":"tensor(0.0060)"},"metadata":{}}],"execution_count":263},{"cell_type":"code","source":"def _convert_output_shape1(\n    formatted_inp: Tuple[Tensor, ...],\n    feature_mask: Tuple[Tensor, ...],\n    coefs: Tensor,\n    num_interp_features: int,\n    is_inputs_tuple: bool,\n    leading_dim_one: bool = False,\n) -> Union[Tensor, Tuple[Tensor, ...]]:\n    attr = [\n        torch.zeros_like(single_inp, dtype=torch.float)\n        for single_inp in formatted_inp\n    ]\n    for tensor_ind in range(len(formatted_inp)):\n        for single_feature in range(num_interp_features):\n            attr[tensor_ind] += (\n                coefs[single_feature].item()\n                * (feature_mask[tensor_ind] == single_feature).float()\n            )\n          \n\n    if leading_dim_one:\n        for i in range(len(attr)):\n            attr[i] = attr[i][0:1]\n    return _format_output(is_inputs_tuple, tuple(attr))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:59.003570Z","iopub.execute_input":"2025-07-17T07:44:59.003882Z","iopub.status.idle":"2025-07-17T07:44:59.023330Z","shell.execute_reply.started":"2025-07-17T07:44:59.003864Z","shell.execute_reply":"2025-07-17T07:44:59.022440Z"}},"outputs":[],"execution_count":264},{"cell_type":"code","source":"torch.unique(supervoxel_map).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:59.024168Z","iopub.execute_input":"2025-07-17T07:44:59.024555Z","iopub.status.idle":"2025-07-17T07:44:59.080962Z","shell.execute_reply.started":"2025-07-17T07:44:59.024526Z","shell.execute_reply":"2025-07-17T07:44:59.080376Z"}},"outputs":[{"execution_count":265,"output_type":"execute_result","data":{"text/plain":"torch.Size([384])"},"metadata":{}}],"execution_count":265},{"cell_type":"code","source":"from captum.attr._utils.common import _format_input_baseline\n\nformatted_inputs, baselines = _format_input_baseline(volume, 0)\nfeature_mask, num_interp_features = construct_feature_mask(\n            supervoxel_map, formatted_inputs\n)\nformatted_supervoxel_map = tuple([supervoxel_map])\nnum_interp_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:59.081527Z","iopub.execute_input":"2025-07-17T07:44:59.081715Z","iopub.status.idle":"2025-07-17T07:44:59.088278Z","shell.execute_reply.started":"2025-07-17T07:44:59.081701Z","shell.execute_reply":"2025-07-17T07:44:59.087769Z"}},"outputs":[{"execution_count":266,"output_type":"execute_result","data":{"text/plain":"384"},"metadata":{}}],"execution_count":266},{"cell_type":"code","source":"38055858*384","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:59.089044Z","iopub.execute_input":"2025-07-17T07:44:59.089270Z","iopub.status.idle":"2025-07-17T07:44:59.108985Z","shell.execute_reply.started":"2025-07-17T07:44:59.089250Z","shell.execute_reply":"2025-07-17T07:44:59.108336Z"}},"outputs":[{"execution_count":267,"output_type":"execute_result","data":{"text/plain":"14613449472"},"metadata":{}}],"execution_count":267},{"cell_type":"code","source":"512*512*300","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:44:59.109566Z","iopub.execute_input":"2025-07-17T07:44:59.110179Z","iopub.status.idle":"2025-07-17T07:44:59.128971Z","shell.execute_reply.started":"2025-07-17T07:44:59.110162Z","shell.execute_reply":"2025-07-17T07:44:59.128249Z"}},"outputs":[{"execution_count":268,"output_type":"execute_result","data":{"text/plain":"78643200"},"metadata":{}}],"execution_count":268},{"cell_type":"code","source":"attribution_map = _convert_output_shape1(\n                formatted_inputs,\n                formatted_supervoxel_map,\n                phis,\n                num_interp_features,\n                True,\n    \n            False,\n            )[0]\nattribution_map.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:45:57.402876Z","iopub.execute_input":"2025-07-17T07:45:57.403417Z","iopub.status.idle":"2025-07-17T07:45:58.025131Z","shell.execute_reply.started":"2025-07-17T07:45:57.403394Z","shell.execute_reply":"2025-07-17T07:45:58.024451Z"}},"outputs":[{"execution_count":271,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1, 168, 445, 450])"},"metadata":{}}],"execution_count":271},{"cell_type":"code","source":"attr_postprocessed = attribution_map[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\nattr_img = nib.Nifti1Image(attr_postprocessed, affine_cropped_volume)\nnib.save(attr_img, 'attribution_map-TP-Lasso-prova.nii.gz')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T07:46:10.499693Z","iopub.execute_input":"2025-07-17T07:46:10.500444Z","iopub.status.idle":"2025-07-17T07:46:11.088920Z","shell.execute_reply.started":"2025-07-17T07:46:10.500418Z","shell.execute_reply":"2025-07-17T07:46:11.087974Z"}},"outputs":[],"execution_count":273}]}