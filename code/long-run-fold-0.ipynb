{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6a9f52",
   "metadata": {},
   "source": [
    "# 1. Import Packages for the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bae1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic packages for later use\n",
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0dfc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c406cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IN_COLAB = not IN_KAGGLE and os.path.exists('/content')\n",
    "IN_DEIB = not IN_KAGGLE and not IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870339bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IN_DEIB:\n",
    "    !pip install nnunetv2\n",
    "    !pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778345b",
   "metadata": {},
   "source": [
    "# 2. Mount the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73794372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir: /workspace/output\n"
     ]
    }
   ],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Google Colab\n",
    "    # for colab users only - mounting the drive\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount = True)\n",
    "\n",
    "    drive_dir = \"/content/drive/My Drive\"\n",
    "    mount_dir = join(drive_dir, \"tesi\", \"automi\")\n",
    "    base_dir = os.getcwd()\n",
    "elif IN_KAGGLE:\n",
    "    # Kaggle\n",
    "    mount_dir = \"/kaggle/input/\"\n",
    "    base_dir = os.getcwd()\n",
    "    print(base_dir)\n",
    "    !ls '/kaggle/input'\n",
    "    !cd \"/kaggle/input/automi-seg\" ; ls\n",
    "else:\n",
    "    mount_dir = \"/workspace/data\"\n",
    "    base_dir = \"/workspace/output\"\n",
    "    os.chdir(base_dir)\n",
    "    print(\"base_dir:\", base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0894ea9",
   "metadata": {},
   "source": [
    "# 3. Setting up nnU-Nets folder structure and environment variables\n",
    "nnUnet expects a certain folder structure and environment variables.\n",
    "\n",
    "Roughly they tell nnUnet:\n",
    "1. Where to look for stuff\n",
    "2. Where to put stuff\n",
    "\n",
    "For more information about this please check: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da019dc2",
   "metadata": {},
   "source": [
    "## 3.1 Set environment Variables and creating folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604985dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: /workspace/data/nnunet_raw\n",
      "nnUNet_results: /workspace/data/results\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# ðŸ“¦ SETUP nnUNet ENVIRONMENT\n",
    "# ===========================\n",
    "\n",
    "# Definisci i path da settare\n",
    "path_dict = {\n",
    "    \"nnUNet_raw\": join(mount_dir, \"nnunet_raw\"),\n",
    "    \"nnUNet_preprocessed\": join(mount_dir, \"preprocessed_files\"),#\"nnUNet_preprocessed\"),\n",
    "    \"nnUNet_results\": join(mount_dir, \"results\"),#\"nnUNet_results\"),\n",
    "    # \"RAW_DATA_PATH\": join(mount_dir, \"RawData\"),  # Facoltativo, se ti serve salvare zips\n",
    "}\n",
    "\n",
    "# Scrivi i path nelle variabili di ambiente, che vengono lette dal modulo paths di nnunetv2\n",
    "for env_var, path in path_dict.items():\n",
    "    os.environ[env_var] = path\n",
    "\n",
    "from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    if nnUNet_raw == None:\n",
    "        nnUNet_raw = \"/kaggle/input/nnunet_raw\"\n",
    "    if nnUNet_results == None:\n",
    "        nnUNet_results = \"/kaggle/input/results\"\n",
    "    # Kaggle has some very unconsistent behaviors in dataset mounting...\n",
    "    #nnUNet_raw = \"/kaggle/input/automi-seg/nnunet_raw\"\n",
    "    #nnUNet_results = \"/kaggle/input/automi-seg/results\"\n",
    "    \n",
    "print(\"nnUNet_raw:\", nnUNet_raw)\n",
    "print(\"nnUNet_results:\", nnUNet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a822df",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0711042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all volumes of fold 0 test set except 00039, already examined\n",
    "volume_codes = [\"00004\", \"00005\", \"00024\", \"00027\", \"00029\", \"00034\", \"00044\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab74e90",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or no access: '/workspace/data/nnunet_raw/total_segmentator_structures/AUTOMI_00004_0000/mask_mask_add_input_20_total_segmentator.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/nibabel/loadsave.py:101\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     stat_result \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/data/nnunet_raw/total_segmentator_structures/AUTOMI_00004_0000/mask_mask_add_input_20_total_segmentator.nii.gz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     organ_mask_paths[volume_code] \u001b[38;5;241m=\u001b[39m join(nnUNet_raw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_segmentator_structures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUTOMI_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvolume_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_0000\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_mask_add_input_20_total_segmentator.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m ct_img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(ct_img_paths[volume_code])\n\u001b[0;32m---> 12\u001b[0m organ_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43morgan_mask_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvolume_code\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVolume \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvolume_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCT shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ct_img\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/nibabel/loadsave.py:103\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     stat_result \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(filename)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or no access: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stat_result\u001b[38;5;241m.\u001b[39mst_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImageFileError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or no access: '/workspace/data/nnunet_raw/total_segmentator_structures/AUTOMI_00004_0000/mask_mask_add_input_20_total_segmentator.nii.gz'"
     ]
    }
   ],
   "source": [
    "ct_img_paths = {}\n",
    "organ_mask_paths = {}\n",
    "\n",
    "for volume_code in volume_codes:\n",
    "    if IN_KAGGLE:\n",
    "        ct_img_paths[volume_code] = join(nnUNet_raw, \"imagesTr\", f\"AUTOMI_{volume_code}_0000.nii\")\n",
    "        organ_mask_paths[volume_code] = join(nnUNet_raw, \"total_segmentator_structures\", f\"AUTOMI_{volume_code}_0000\", \"mask_mask_add_input_20_total_segmentator.nii\")\n",
    "    else:\n",
    "        ct_img_paths[volume_code] = join(nnUNet_raw, \"imagesTr\", f\"AUTOMI_{volume_code}_0000.nii.gz\")\n",
    "        organ_mask_paths[volume_code] = join(nnUNet_raw, \"total_segmentator_structures\", f\"AUTOMI_{volume_code}_0000\", \"mask_mask_add_input_20_total_segmentator.nii.gz\")\n",
    "    ct_img = nib.load(ct_img_paths[volume_code])\n",
    "    organ_mask = nib.load(organ_mask_paths[volume_code])\n",
    "    print(f\"Volume {volume_code}:\")\n",
    "    print(\"CT shape:\", ct_img.shape)\n",
    "    print(\"Organ shape:\", organ_mask.shape)\n",
    "    print(\"Spacing:\", ct_img.header.get_zooms())\n",
    "    print(\"Organ spacing:\", organ_mask.header.get_zooms())\n",
    "    assert np.all(ct_img.affine == organ_mask.affine), \"CT and organ mask affine matrices do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b07b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affine: [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  3.00000000e+02]\n",
      " [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.85300003e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.43419995e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "ct_img = nib.load(ct_img_paths[volume_codes[1]])\n",
    "print(\"affine:\", ct_img.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99181eff",
   "metadata": {},
   "source": [
    "## Re-align CT scan with its own organ segmentation mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0bf25",
   "metadata": {},
   "source": [
    "### apparently, this was solved in the remaining volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97bde492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import SimpleITK as sitk\\n\\n# Load CT and misaligned organ mask\\nct = sitk.ReadImage(ct_img_path, sitk.sitkFloat32)\\norgan_mask = sitk.ReadImage(organ_mask_path, sitk.sitkUInt8)\\n\\n# Resample organ mask to match CT space\\nresampler = sitk.ResampleImageFilter()\\nresampler.SetReferenceImage(ct)\\nresampler.SetInterpolator(sitk.sitkNearestNeighbor)\\norgan_resampled = resampler.Execute(organ_mask)\\n\\n# Save aligned output\\nsitk.WriteImage(organ_resampled, \"organ_mask_resampled_to_ct.nii.gz\")'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import SimpleITK as sitk\n",
    "\n",
    "# Load CT and misaligned organ mask\n",
    "ct = sitk.ReadImage(ct_img_path, sitk.sitkFloat32)\n",
    "organ_mask = sitk.ReadImage(organ_mask_path, sitk.sitkUInt8)\n",
    "\n",
    "# Resample organ mask to match CT space\n",
    "resampler = sitk.ResampleImageFilter()\n",
    "resampler.SetReferenceImage(ct)\n",
    "resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "organ_resampled = resampler.Execute(organ_mask)\n",
    "\n",
    "# Save aligned output\n",
    "sitk.WriteImage(organ_resampled, \"organ_mask_resampled_to_ct.nii.gz\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87fc749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#organ_mask_path = join(nnUNet_raw, \"organ_mask_resampled_to_ct.nii.gz\")\\norgan_mask_path = \"organ_mask_resampled_to_ct.nii.gz\"\\nct_img = nib.load(ct_img_path)\\norgan_mask = nib.load(organ_mask_path)\\nprint(\"CT shape:\", ct_img.shape)\\nprint(\"Organ shape:\", organ_mask.shape)\\nprint(\"Spacing:\", ct_img.header.get_zooms())\\nprint(\"Organ spacing:\", organ_mask.header.get_zooms())'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#organ_mask_path = join(nnUNet_raw, \"organ_mask_resampled_to_ct.nii.gz\")\n",
    "organ_mask_path = \"organ_mask_resampled_to_ct.nii.gz\"\n",
    "ct_img = nib.load(ct_img_path)\n",
    "organ_mask = nib.load(organ_mask_path)\n",
    "print(\"CT shape:\", ct_img.shape)\n",
    "print(\"Organ shape:\", organ_mask.shape)\n",
    "print(\"Spacing:\", ct_img.header.get_zooms())\n",
    "print(\"Organ spacing:\", organ_mask.header.get_zooms())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2220c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model directory; note that this is readonly in Kaggle environment\n",
    "if IN_KAGGLE:\n",
    "    model_dir = join(nnUNet_results, 'Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres')\n",
    "else:\n",
    "    model_dir = join(nnUNet_results, 'nnUNetTrainer__nnUNetPlans__3d_fullres')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5438ea",
   "metadata": {},
   "source": [
    "## Utility to export logits to a visualizable segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee84330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from nnunetv2.configuration import default_num_processes\n",
    "from nnunetv2.inference.export_prediction import export_prediction_from_logits\n",
    "\n",
    "def export_logits_to_nifty_segmentation(\n",
    "    predictor,\n",
    "    volume_file: Path,\n",
    "    model_dir: str,\n",
    "    logits: Union[str, np.ndarray, torch.Tensor],\n",
    "    npz_dir: str | None,\n",
    "    output_dir: str = \"\",\n",
    "    fold: int = 0,\n",
    "    save_probs: bool = False,\n",
    "    from_file: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a saved .npz logits file into a native-space NIfTI segmentation using nnU-Net's helper.\n",
    "\n",
    "    Args:\n",
    "        predictor: An instantiated nnU-Net predictor object with loaded plans/configs.\n",
    "        volume_file: An Path object pointing to the raw image file.\n",
    "        model_dir (str): Path to the nnU-Net mode1Introductionl directory containing dataset.json.\n",
    "        logits (str or tensor): Base name of the .npz logits file (no extension), when from_file=True\n",
    "        npz_dir (str): Directory where the .npz file is stored.\n",
    "        output_dir (str): Directory where the .nii.gz segmentation will be saved.\n",
    "        fold (int): The fold number used for prediction (default is 0).\n",
    "        save_probs (bool): Whether to save softmax probabilities as a .npz file.\n",
    "        from_file (bool). Whether to convert from a file instead of from the logits (default true)\n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        npz_logits = Path(npz_dir) / f\"{logits}.npz\"\n",
    "        output_nii = Path(output_dir) / f\"{logits}_seg.nii.gz\"\n",
    "        logits = np.load(npz_logits)[\"logits\"]\n",
    "    else:\n",
    "        output_nii = Path(output_dir) / \"exported_seg.nii.gz\"\n",
    "\n",
    "    plans_manager = predictor.plans_manager\n",
    "    configuration_manager = predictor.configuration_manager\n",
    "    dataset_json = Path(model_dir) / \"dataset.json\"\n",
    "\n",
    "    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n",
    "    rw = plans_manager.image_reader_writer_class()\n",
    "    if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "        rw = rw()\n",
    "    img_np, img_props = rw.read_images([str(volume_file)])\n",
    "\n",
    "    _, _, data_props = preprocessor.run_case_npy(\n",
    "        img_np, seg=None, properties=img_props,\n",
    "        plans_manager=plans_manager,\n",
    "        configuration_manager=configuration_manager,\n",
    "        dataset_json=dataset_json\n",
    "    )\n",
    "\n",
    "\n",
    "    export_prediction_from_logits(\n",
    "        predicted_array_or_file=logits,\n",
    "        properties_dict=data_props,\n",
    "        configuration_manager=configuration_manager,\n",
    "        plans_manager=plans_manager,\n",
    "        dataset_json_dict_or_file=str(dataset_json),\n",
    "        output_file_truncated=os.path.splitext(str(output_nii))[0],\n",
    "        save_probabilities=save_probs,\n",
    "        num_threads_torch=default_num_processes\n",
    "    )\n",
    "\n",
    "    print(f\"âœ…  NIfTI segmentation written â†’ {output_nii}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7e5ae55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'export_logits_to_nifty_segmentation(\\n    predictor=predictor,\\n    plan=plan,\\n    model_dir=Path(model_dir),\\n    logits_filename=\"pred_00007\",\\n    npz_dir=\"SHAP/shap_run\",\\n    output_dir=\"SHAP/shap_run\",\\n    fold=0,\\n    save_probs=False\\n)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"export_logits_to_nifty_segmentation(\n",
    "    predictor=predictor,\n",
    "    plan=plan,\n",
    "    model_dir=Path(model_dir),\n",
    "    logits_filename=\"pred_00007\",\n",
    "    npz_dir=\"SHAP/shap_run\",\n",
    "    output_dir=\"SHAP/shap_run\",\n",
    "    fold=0,\n",
    "    save_probs=False\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ab0ae",
   "metadata": {},
   "source": [
    "## We define a sliding window caching for faster multi-inference scenario, like SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b1194",
   "metadata": {},
   "source": [
    "### Try to override the sliding_window_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9dcd232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from acvl_utils.cropping_and_padding.padding import pad_nd_image\n",
    "from nnunetv2.utilities.helpers import empty_cache, dummy_context\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.inference.sliding_window_prediction import compute_gaussian, compute_steps_for_sliding_window\n",
    "\n",
    "class CustomNNUNetPredictor(nnUNetPredictor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_sliding_window_return_logits_with_caching(self, input_image: torch.Tensor,\n",
    "                                                          perturbation_mask: torch.BoolTensor | None,\n",
    "                                                          baseline_prediction_dict: dict) \\\n",
    "            -> Union[np.ndarray, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Method predict_sliding_window_return_logits taken from official nnunetv2 documentation:\n",
    "        https://github.com/MIC-DKFZ/nnUNet/blob/58a3b121a6d1846a978306f6c79a7c005b7d669b/nnunetv2/inference/predict_from_raw_data.py\n",
    "        We add a perturbation_mask parameter to check each patch for the actual presence of a perturbation\n",
    "        \"\"\"\n",
    "        # fallback to original method if perturbation_mask is None\n",
    "        if perturbation_mask is None:\n",
    "            return self.predict_sliding_window_return_logits(input_image)\n",
    "                \n",
    "        assert isinstance(input_image, torch.Tensor)\n",
    "        self.network = self.network.to(self.device)\n",
    "        self.network.eval()\n",
    "\n",
    "        empty_cache(self.device)\n",
    "\n",
    "        # DEBUG --------------\n",
    "        \"\"\"voxels  = np.prod(input_image.shape[1:])          # (X*Y*Z)\n",
    "        bytes_per_voxel = 2                        # fp16\n",
    "        needed  = voxels * self.label_manager.num_segmentation_heads * bytes_per_voxel\n",
    "        print(f\"â‰ˆ{needed/1e9:.1f} GB per predicted_logits\")\"\"\"\n",
    "\n",
    "        # Autocast can be annoying\n",
    "        # If the device_type is 'cpu' then it's slow as heck on some CPUs (no auto bfloat16 support detection)\n",
    "        # and needs to be disabled.\n",
    "        # If the device_type is 'mps' then it will complain that mps is not implemented, even if enabled=False\n",
    "        # is set. Whyyyyyyy. (this is why we don't make use of enabled=False)\n",
    "        # So autocast will only be active if we have a cuda device.\n",
    "        with torch.autocast(self.device.type, enabled=True) if self.device.type == 'cuda' else dummy_context():\n",
    "            assert input_image.ndim == 4, 'input_image must be a 4D np.ndarray or torch.Tensor (c, x, y, z)'\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'Input shape: {input_image.shape}')\n",
    "                print(\"step_size:\", self.tile_step_size)\n",
    "                print(\"mirror_axes:\", self.allowed_mirroring_axes if self.use_mirroring else None)\n",
    "                print(f'Perturbation mask shape: {perturbation_mask.shape}')\n",
    "\n",
    "\n",
    "            # if input_image is smaller than tile_size we need to pad it to tile_size.\n",
    "            data, slicer_revert_padding = pad_nd_image(input_image, self.configuration_manager.patch_size,\n",
    "                                                       'constant', {'value': 0}, True,\n",
    "                                                       None)\n",
    "\n",
    "            # slicers can be applied to both perturbed volume and \n",
    "            slicers = self._internal_get_sliding_window_slicers(data.shape[1:])\n",
    "\n",
    "            if self.perform_everything_on_device and self.device != 'cpu':\n",
    "                # behavior changed\n",
    "                try:\n",
    "                    predicted_logits = self._internal_predict_sliding_window_return_logits(\n",
    "                        data, slicers, True, perturbation_mask, baseline_prediction_dict, caching=True\n",
    "                    )\n",
    "                except RuntimeError as e:\n",
    "                    if \"CUDA out of memory\" in str(e):\n",
    "                        print(\"âš ï¸  CUDA OOM, cambiare batch size o patch size!\")\n",
    "                        raise\n",
    "                    else:\n",
    "                        # Mostra l'errore reale e aborta: niente CPU fallback\n",
    "                        raise\n",
    "            else:\n",
    "                predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n",
    "                                                                                       self.perform_everything_on_device)\n",
    "\n",
    "            empty_cache(self.device)\n",
    "            # revert padding\n",
    "            predicted_logits = predicted_logits[(slice(None), *slicer_revert_padding[1:])]\n",
    "        return predicted_logits\n",
    "                \n",
    "\n",
    "    def _slice_key(self, slicer_tuple):\n",
    "        # make slicer object hashable to use it for cache lookup\n",
    "        return tuple((s.start, s.stop, s.step) for s in slicer_tuple)\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _internal_predict_sliding_window_return_logits(self,\n",
    "                                                       data: torch.Tensor,\n",
    "                                                       slicers,\n",
    "                                                       do_on_device: bool = True,\n",
    "                                                       perturbation_mask: torch.BoolTensor | None = None,\n",
    "                                                       baseline_prediction_dict: dict | None = None,\n",
    "                                                       caching: bool = False,\n",
    "                                                       ):\n",
    "        \"\"\"\n",
    "        Modified to manage the caching of patches\n",
    "        \"\"\"\n",
    "        predicted_logits = n_predictions = prediction = gaussian = workon = None\n",
    "        results_device = self.device if do_on_device else torch.device('cpu')\n",
    "        if next(self.network.parameters()).device != results_device:\n",
    "            self.network = self.network.to(results_device)\n",
    "\n",
    "        def producer(d, slh, q):\n",
    "            for s in slh:\n",
    "                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(results_device), s))\n",
    "            q.put('end')\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data to device\n",
    "            if self.verbose:\n",
    "                print(f'move image to device {results_device}')\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            # preallocate arrays\n",
    "            if self.verbose:\n",
    "                print(f'preallocating results arrays on device {results_device}')\n",
    "            predicted_logits = torch.zeros((self.label_manager.num_segmentation_heads, *data.shape[1:]),\n",
    "                                           dtype=torch.half,\n",
    "                                           device=results_device)\n",
    "            n_predictions = torch.zeros(data.shape[1:], dtype=torch.half, device=results_device)\n",
    "\n",
    "            if self.use_gaussian:\n",
    "                gaussian = compute_gaussian(tuple(self.configuration_manager.patch_size), sigma_scale=1. / 8,\n",
    "                                            value_scaling_factor=10,\n",
    "                                            device=results_device)\n",
    "            else:\n",
    "                gaussian = 1\n",
    "\n",
    "        \n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                cache_hits = 0\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    try:\n",
    "                        if caching and not self.check_overlapping(sl, perturbation_mask):\n",
    "                            prediction = baseline_prediction_dict[self._slice_key(sl)].to(results_device)\n",
    "                            cache_hits += 1\n",
    "                        else:\n",
    "                            prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(\"Errore nella predizione del patch\") from e\n",
    "\n",
    "                    # 2) sanity-check device\n",
    "                    assert prediction.device == predicted_logits.device\n",
    "\n",
    "                    if self.use_gaussian:\n",
    "                        prediction *= gaussian\n",
    "                    predicted_logits[sl] += prediction\n",
    "                    n_predictions[sl[1:]] += gaussian\n",
    "\n",
    "                    # free up gpu memory\n",
    "                    del prediction, workon\n",
    "                    \n",
    "                    queue.task_done()\n",
    "                    pbar.set_postfix(\n",
    "                        cache=f\"{cache_hits}\",\n",
    "                        mem=f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\"\n",
    "                    )\n",
    "                    pbar.update(1)\n",
    "            queue.join()\n",
    "            if self.verbose and not self.allow_tqdm:\n",
    "                print(f\"Cache hits: {cache_hits}\\\\{len(slicers)}\")\n",
    "            \n",
    "\n",
    "            # predicted_logits /= n_predictions\n",
    "            torch.div(predicted_logits, n_predictions, out=predicted_logits)\n",
    "            # check for infs\n",
    "            if torch.any(torch.isinf(predicted_logits)):\n",
    "                raise RuntimeError('Encountered inf in predicted array. Aborting... If this problem persists, '\n",
    "                                   'reduce value_scaling_factor in compute_gaussian or increase the dtype of '\n",
    "                                   'predicted_logits to fp32')\n",
    "        except Exception as e:\n",
    "            del predicted_logits, n_predictions, prediction, gaussian, workon\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return predicted_logits\n",
    "  \n",
    "\n",
    "\n",
    "    def get_output_dictionary_sliding_window(self, data: torch.Tensor, slicers,\n",
    "                                            do_on_device: bool = True,\n",
    "                                            ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        # create a dictionary that associates the output of the inference, to each slicer of the sliding window module\n",
    "        # this way we can set ready for cache the output for the untouched patches.\n",
    "        \"\"\"\n",
    "        \n",
    "        dictionary = dict()\n",
    "        prediction = workon = None\n",
    "        results_device = self.device if do_on_device else torch.device('cpu')\n",
    "        if next(self.network.parameters()).device != results_device:\n",
    "            self.network = self.network.to(results_device)\n",
    "\n",
    "        def producer(d, slh, q):\n",
    "            for s in slh:\n",
    "                #tqdm.write(f\"put patch {s} on queue\")    # dentro producer\n",
    "                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(self.device), s))\n",
    "            q.put('end')\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data and network to device\n",
    "            if self.verbose:\n",
    "                print(f'move image and model to device {results_device}')\n",
    "\n",
    "            self.network = self.network.to(results_device)\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "\n",
    "                    pred_cpu = pred_gpu.cpu()\n",
    "                    # save prediction in the dictionary\n",
    "                    dictionary[self._slice_key(sl)] = pred_cpu\n",
    "                    # immediately free gpu memory\n",
    "                    del pred_gpu\n",
    "                    \n",
    "                    queue.task_done()\n",
    "                    pbar.update()\n",
    "            queue.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            del workon#, prediction\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return dictionary\n",
    "\n",
    "\n",
    "\n",
    "    def check_overlapping(self, slicer, perturbation_mask: torch.BoolTensor) -> bool:\n",
    "        \"\"\"\n",
    "        Restituisce True se la patch definita da `slicer`\n",
    "        contiene almeno un voxel perturbato.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        slicer : tuple\n",
    "            Quello prodotto da `_internal_get_sliding_window_slicers`,\n",
    "            cioÃ¨ (slice(None), slice(x0,x1), slice(y0,y1), slice(z0,z1)).\n",
    "        perturbation_mask : torch.BoolTensor\n",
    "            Maschera (C, X, Y, Z) con True nei voxel da perturbare\n",
    "            (di solito C==1 o replicata sui canali).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True â†” almeno un voxel True nella patch.out\n",
    "        \"\"\"\n",
    "        # NB: il primo elemento del tuple Ã¨ sempre slice(None) (canali).\n",
    "        #     Lo manteniamo: non ha overhead e semplifica.\n",
    "        return perturbation_mask[slicer].any().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdd34d",
   "metadata": {},
   "source": [
    "## Try Captum's kernel SHAP on the organ mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166aee37",
   "metadata": {},
   "source": [
    "### first derive a customized class from Captum library, to use sliding window caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faaaf6b",
   "metadata": {},
   "source": [
    "## Try to customize KernelShap as a \"sibling\", so let's inherit the parent, LimeBase\n",
    "that's because we need to override (to-and-from)/interpret_rep_transform methods used to map the (1,M) binary mask vector with the perturbed volume AND the perturbation mask we need for caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59abd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# pyre-strict\n",
    "import inspect\n",
    "import math\n",
    "import typing\n",
    "import warnings\n",
    "from collections.abc import Iterator\n",
    "from typing import Any, Callable, cast, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from captum._utils.common import (\n",
    "    _expand_additional_forward_args,\n",
    "    _expand_target,\n",
    "    _flatten_tensor_or_tuple,\n",
    "    _format_output,\n",
    "    _format_tensor_into_tuples,\n",
    "    _get_max_feature_index,\n",
    "    _is_tuple,\n",
    "    _reduce_list,\n",
    "    _run_forward,\n",
    ")\n",
    "from captum._utils.models.linear_model import SkLearnLasso\n",
    "from captum._utils.models.model import Model\n",
    "from captum._utils.progress import progress\n",
    "from captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\n",
    "from captum.attr._utils.attribution import PerturbationAttribution\n",
    "from captum.attr._utils.batching import _batch_example_iterator\n",
    "from captum.attr._utils.common import (\n",
    "    _construct_default_feature_mask,\n",
    "    _format_input_baseline,\n",
    ")\n",
    "from captum.log import log_usage\n",
    "from torch import Tensor, BoolTensor\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class LimeBaseWithCustomArgumentToForwardFunc(PerturbationAttribution):\n",
    "    r\"\"\"\n",
    "    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        forward_func: Callable[..., Tensor],\n",
    "        interpretable_model: Model,\n",
    "        similarity_func: Callable[\n",
    "            ...,\n",
    "            Union[float, Tensor],\n",
    "        ],\n",
    "        perturb_func: Callable[..., object],\n",
    "        perturb_interpretable_space: bool,\n",
    "        from_interp_rep_transform: Optional[\n",
    "            Callable[..., Union[Tensor, Tuple[Tensor, ...]]]\n",
    "        ],\n",
    "        to_interp_rep_transform: Optional[Callable[..., Tensor]],\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "\n",
    "            forward_func (Callable): The forward function of the model or any\n",
    "                    modification of it. If a batch is provided as input for\n",
    "                    attribution, it is expected that forward_func returns a scalar\n",
    "                    representing the entire batch.\n",
    "            interpretable_model (Model): Model object to train interpretable model.\n",
    "                    A Model object provides a `fit` method to train the model,\n",
    "                    given a dataloader, with batches containing three tensors:\n",
    "\n",
    "                    - interpretable_inputs: Tensor\n",
    "                      [2D num_samples x num_interp_features],\n",
    "                    - expected_outputs: Tensor [1D num_samples],\n",
    "                    - weights: Tensor [1D num_samples]\n",
    "\n",
    "                    The model object must also provide a `representation` method to\n",
    "                    access the appropriate coefficients or representation of the\n",
    "                    interpretable model after fitting.\n",
    "                    Some predefined interpretable linear models are provided in\n",
    "                    captum._utils.models.linear_model including wrappers around\n",
    "                    SkLearn linear models as well as SGD-based PyTorch linear\n",
    "                    models.\n",
    "\n",
    "                    Note that calling fit multiple times should retrain the\n",
    "                    interpretable model, each attribution call reuses\n",
    "                    the same given interpretable model object.\n",
    "            similarity_func (Callable): Function which takes a single sample\n",
    "                    along with its corresponding interpretable representation\n",
    "                    and returns the weight of the interpretable sample for\n",
    "                    training interpretable model. Weight is generally\n",
    "                    determined based on similarity to the original input.\n",
    "                    The original paper refers to this as a similarity kernel.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> similarity_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_interpretable_input:\n",
    "                    >>>        Tensor [2D 1 x num_interp_features],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> float or Tensor containing float scalar\n",
    "\n",
    "                    perturbed_input and original_input will be the same type and\n",
    "                    contain tensors of the same shape (regardless of whether or not\n",
    "                    the sampling function returns inputs in the interpretable\n",
    "                    space). original_input is the same as the input provided\n",
    "                    when calling attribute.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "            perturb_func (Callable): Function which returns a single\n",
    "                    sampled input, generally a perturbation of the original\n",
    "                    input, which is used to train the interpretable surrogate\n",
    "                    model. Function can return samples in either\n",
    "                    the original input space (matching type and tensor shapes\n",
    "                    of original input) or in the interpretable input space,\n",
    "                    which is a vector containing the intepretable features.\n",
    "                    Alternatively, this function can return a generator\n",
    "                    yielding samples to train the interpretable surrogate\n",
    "                    model, and n_samples perturbations will be sampled\n",
    "                    from this generator.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> perturb_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor, tuple[Tensor, ...], or\n",
    "                    >>>    generator yielding tensor or tuple[Tensor, ...]\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "\n",
    "                    Returned sampled input should match the input type (Tensor\n",
    "                    or Tuple of Tensor and corresponding shapes) if\n",
    "                    perturb_interpretable_space = False. If\n",
    "                    perturb_interpretable_space = True, the return type should\n",
    "                    be a single tensor of shape 1 x num_interp_features,\n",
    "                    corresponding to the representation of the\n",
    "                    sample to train the interpretable model.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "            perturb_interpretable_space (bool): Indicates whether\n",
    "                    perturb_func returns a sample in the interpretable space\n",
    "                    (tensor of shape 1 x num_interp_features) or a sample\n",
    "                    in the original space, matching the format of the original\n",
    "                    input. Once sampled, inputs can be converted to / from\n",
    "                    the interpretable representation with either\n",
    "                    to_interp_rep_transform or from_interp_rep_transform.\n",
    "            from_interp_rep_transform (Callable): Function which takes a\n",
    "                    single sampled interpretable representation (tensor\n",
    "                    of shape 1 x num_interp_features) and returns\n",
    "                    the corresponding representation in the input space\n",
    "                    (matching shapes of original input to attribute).\n",
    "\n",
    "                    This argument is necessary if perturb_interpretable_space\n",
    "                    is True, otherwise None can be provided for this argument.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> from_interp_rep_transform(\n",
    "                    >>>    curr_sample: Tensor [2D 1 x num_interp_features]\n",
    "                    >>>    original_input: Tensor or Tuple of Tensors,\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor or tuple[Tensor, ...]\n",
    "\n",
    "                    Returned sampled input should match the type of original_input\n",
    "                    and corresponding tensor shapes.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "\n",
    "            to_interp_rep_transform (Callable): Function which takes a\n",
    "                    sample in the original input space and converts to\n",
    "                    its interpretable representation (tensor\n",
    "                    of shape 1 x num_interp_features).\n",
    "\n",
    "                    This argument is necessary if perturb_interpretable_space\n",
    "                    is False, otherwise None can be provided for this argument.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> to_interp_rep_transform(\n",
    "                    >>>    curr_sample: Tensor or Tuple of Tensors,\n",
    "                    >>>    original_input: Tensor or Tuple of Tensors,\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor [2D 1 x num_interp_features]\n",
    "\n",
    "                    curr_sample will match the type of original_input\n",
    "                    and corresponding tensor shapes.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "        \"\"\"\n",
    "        PerturbationAttribution.__init__(self, forward_func)\n",
    "        self.interpretable_model = interpretable_model\n",
    "        self.similarity_func = similarity_func\n",
    "        self.perturb_func = perturb_func\n",
    "        self.perturb_interpretable_space = perturb_interpretable_space\n",
    "        self.from_interp_rep_transform = from_interp_rep_transform\n",
    "        self.to_interp_rep_transform = to_interp_rep_transform\n",
    "\n",
    "        if self.perturb_interpretable_space:\n",
    "            assert (\n",
    "                self.from_interp_rep_transform is not None\n",
    "            ), \"Must provide transform from interpretable space to original input space\"\n",
    "            \" when sampling from interpretable space.\"\n",
    "        else:\n",
    "            assert (\n",
    "                self.to_interp_rep_transform is not None\n",
    "            ), \"Must provide transform from original input space to interpretable space\"\n",
    "\n",
    "    @log_usage(part_of_slo=True)\n",
    "    @torch.no_grad()\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[Tuple[object, ...]] = None,\n",
    "        n_samples: int = 50,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        show_progress: bool = False,\n",
    "        # --- MONITOR CONVERGENCE QUALITY\n",
    "        monitor_log_path: str | None = None,\n",
    "        monitor_convergence_step: int | None = 20,\n",
    "        monitor_local_accuracy_step: int | None = 50,\n",
    "        **kwargs: object,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to the inputs of the model using the approach described above.\n",
    "        It trains an interpretable model and returns a representation of the\n",
    "        interpretable model.\n",
    "\n",
    "        It is recommended to only provide a single example as input (tensors\n",
    "        with first dimension or batch size = 1). This is because LIME is generally\n",
    "        used for sample-based interpretability, training a separate interpretable\n",
    "        model to explain a model's prediction on each individual example.\n",
    "\n",
    "        A batch of inputs can be provided as inputs only if forward_func\n",
    "        returns a single value per batch (e.g. loss).\n",
    "        The interpretable feature representation should still have shape\n",
    "        1 x num_interp_features, corresponding to the interpretable\n",
    "        representation for the full batch, and perturbations_per_eval\n",
    "        must be set to 1.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n",
    "                        is computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            target (int, tuple, Tensor, or list, optional): Output indices for\n",
    "                        which surrogate model is trained\n",
    "                        (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs w            except --------ith > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (Any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. For all other types,\n",
    "                        the given argument is used for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            n_samples (int, optional): The number of samples of the original\n",
    "                        model used to train the surrogate interpretable model.\n",
    "                        Default: `50` if `n_samples` is not provided.\n",
    "            perturbations_per_eval (int, optional): Allows multiple samples\n",
    "                        to be processed simultaneously in one call to forward_fn.\n",
    "                        Each forward pass will contain a maximum of\n",
    "                        perturbations_per_eval * #examples samples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain at most\n",
    "                        (perturbations_per_eval * #examples) / num_devices\n",
    "                        samples.\n",
    "                        If the forward function returns a single scalar per batch,\n",
    "                        perturbations_per_eval must be set to 1.\n",
    "                        Default: 1\n",
    "            show_progress (bool, optional): Displays the progress of computation.\n",
    "                        It will try to use tqdm if available for advanced features\n",
    "                        (e.g. time estimation). Otherwise, it will fallback to\n",
    "                        a simple output of progress.\n",
    "                        Default: False\n",
    "            monitor_log_path (str, optional): Path to the log file for monitoring convergence.\n",
    "                        if None, no monitoring is performed.\n",
    "                        Default: None\n",
    "            monitor_convergence_step (int, optional): Number of iterations over which\n",
    "                        the difference among two attribution is computerd.\n",
    "                        Default: 20\n",
    "            monitor_local_accuracy_step (int, optional): Number of iterations over which\n",
    "                        the local accuracy of an attribution is computerd.\n",
    "                        Default: 50\n",
    "            **kwargs (Any, optional): Any additional arguments necessary for\n",
    "                        sampling and transformation functions (provided to\n",
    "                        constructor).\n",
    "                        Default: None\n",
    "\n",
    "        Returns:\n",
    "            **interpretable model representation**:\n",
    "            - **interpretable model representation** (*Any*):\n",
    "                    A representation of the interpretable model trained. The return\n",
    "                    type matches the return type of train_interpretable_model_func.\n",
    "                    For example, this could contain coefficients of a\n",
    "                    linear surrogate model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> # SimpleClassifier takes a single input tensor of\n",
    "            >>> # float features with size N x 5,\n",
    "            >>> # and returns an Nx3 tensor of class probabilities.\n",
    "            >>> net = SimpleClassifier()\n",
    "            >>>\n",
    "            >>> # We will train an interpretable model with the same\n",
    "            >>> # features by simply sampling with added Gaussian noise\n",
    "            >>> # to the inputs and training a model to predict the\n",
    "            >>> # score of the target class.\n",
    "            >>>\n",
    "            >>> # For interpretable model training, we will use sklearn\n",
    "            >>> # linear model in this example. We have provided wrappers\n",
    "            >>> # around sklearn linear models to fit the Model interface.\n",
    "            >>> # Any arguments provided to the sklearn constructor can also\n",
    "            >>> # be provided to the wrapper, e.g.:\n",
    "            >>> # SkLearnLinearModel(\"linear_model.Ridge\", alpha=2.0)\n",
    "            >>> from captum._utils.models.linear_model import SkLearnLinearModel\n",
    "            >>>\n",
    "            >>>\n",
    "            >>> # Define similarity kernel (exponential kernel based on L2 norm)\n",
    "            >>> def similarity_kernel(\n",
    "            >>>     original_input: Tensor,\n",
    "            >>>     perturbed_input: Tensor,\n",
    "            >>>     perturbed_interpretable_input: Tensor,\n",
    "            >>>     **kwargs)->Tensor:\n",
    "            >>>         # kernel_width will be provided to attribute as a kwarg\n",
    "            >>>         kernel_width = kwargs[\"kernel_width\"]\n",
    "            >>>         l2_dist = torch.norm(original_input - perturbed_input)\n",
    "            >>>         return torch.exp(- (l2_dist**2) / (kernel_width**2))\n",
    "            >>>\n",
    "            >>>\n",
    "            >>> # Define sampling function\n",
    "            >>> # This function samples in original input space\n",
    "            >>> def perturb_func(\n",
    "            >>>     original_input: Tensor,\n",
    "            >>>     **kwargs)->Tensor:\n",
    "            >>>         return original_input + torch.randn_like(original_input)\n",
    "            >>>\n",
    "            >>> # For this example, we are setting the interpretable input to\n",
    "            >>> # match the model input, so the to_interp_rep_transform\n",
    "            >>> # function simply returns the input. In most cases, the interpretable\n",
    "            >>> # input will be different and may have a smaller feature set, so\n",
    "            >>> # an appropriate transformation function should be provided.\n",
    "            >>>\n",
    "            >>> def to_interp_transform(curr_sample, original_inp,\n",
    "            >>>                                      **kwargs):\n",
    "            >>>     return curr_sample\\\n",
    "            >>>\n",
    "            >>> # Generating random input with size 1 x 5\n",
    "            >>> input = torch.randn(1, 5)\n",
    "            >>> # Defining LimeBase interpreter\n",
    "            >>> lime_attr = LimeBase(net,\n",
    "                                     SkLearnLinearModel(\"linear_model.Ridge\"),\n",
    "                                     similarity_func=similarity_kernel,\n",
    "                                     perturb_func=perturb_func,\n",
    "                                     perturb_interpretable_space=False,\n",
    "                                     from_interp_rep_transform=None,\n",
    "                                     to_interp_rep_transform=to_interp_transform)\n",
    "            >>> # Computes interpretable model, returning coefficients of linear\n",
    "            >>> # model.\n",
    "            >>> attr_coefs = lime_attr.attribute(input, target=1, kernel_width=1.1)\n",
    "        \"\"\"\n",
    "        inp_tensor = cast(Tensor, inputs) if isinstance(inputs, Tensor) else inputs[0]\n",
    "        device = inp_tensor.device\n",
    "        \n",
    "        # --------------------------------------------------------------------- #\n",
    "        # 1.  Lists that grow while we sample                                   #\n",
    "        # --------------------------------------------------------------------- #\n",
    "        interpretable_inps, similarities, outputs = [], [], []\n",
    "        \n",
    "        curr_model_inputs = []\n",
    "        expanded_additional_args = None\n",
    "        expanded_target = None\n",
    "        gen_perturb_func = self._get_perturb_generator_func(inputs, **kwargs)\n",
    "\n",
    "        # --------------------------------------------------------------------- #\n",
    "        # 2.  Monitoring initialisation                                         #\n",
    "        # --------------------------------------------------------------------- #\n",
    "        MONITOR = monitor_log_path is not None\n",
    "        if MONITOR:\n",
    "            beta_prev = None\n",
    "            k_monitor_conv   = monitor_convergence_step\n",
    "            k_monitor_delta  = monitor_local_accuracy_step\n",
    "            logf = open(monitor_log_path, \"a\")\n",
    "        \n",
    "        if show_progress:\n",
    "            attr_progress = progress(\n",
    "                total=math.ceil(n_samples / perturbations_per_eval),\n",
    "                desc=f\"{self.get_name()} attribution\",\n",
    "            )\n",
    "            attr_progress.update(0)\n",
    "        \n",
    "        feature_mask = kwargs[\"feature_mask\"]\n",
    "        batch_count = 0\n",
    "\n",
    "        # --------------------------------------------------------------------- #\n",
    "        # 3.  Main sampling loop                                                #\n",
    "        # --------------------------------------------------------------------- #\n",
    "        for _ in range(n_samples):\n",
    "            try:\n",
    "                interpretable_inp, curr_model_input = gen_perturb_func()\n",
    "                perturbation_mask = self._get_perturbation_mask(\n",
    "                    interpretable_inp, curr_model_input, feature_mask\n",
    "                )\n",
    "            except StopIteration:\n",
    "                warnings.warn(\n",
    "                    \"Generator completed prior to given n_samples iterations!\",\n",
    "                    stacklevel=1,\n",
    "                )\n",
    "                break\n",
    "            except Exception:\n",
    "                print(\"error in the perturbation mask generation\")\n",
    "                raise\n",
    "        \n",
    "            # ------------ Build forward args with mask ------------------------ #\n",
    "            if additional_forward_args is None:\n",
    "                additional_forward_args_with_mask = (perturbation_mask,)\n",
    "            elif isinstance(additional_forward_args, tuple):\n",
    "                additional_forward_args_with_mask = additional_forward_args + (perturbation_mask,)\n",
    "            else:\n",
    "                additional_forward_args_with_mask = (additional_forward_args, perturbation_mask)\n",
    "\n",
    "            # ------------ Book-keeping per sample ----------------------------- #\n",
    "            batch_count += 1\n",
    "            interpretable_inps.append(interpretable_inp)\n",
    "            curr_model_inputs.append(curr_model_input)\n",
    "        \n",
    "            curr_sim = self.similarity_func(inputs, curr_model_input, interpretable_inp, **kwargs)\n",
    "            similarities.append(\n",
    "                curr_sim.flatten()\n",
    "                if isinstance(curr_sim, Tensor)\n",
    "                else torch.tensor([curr_sim], device=device)\n",
    "            )\n",
    "\n",
    "            # ------------ When we have one evaluation batch ready ------------- #\n",
    "            if len(curr_model_inputs) == perturbations_per_eval:\n",
    "                expanded_additional_args = _expand_additional_forward_args(\n",
    "                    additional_forward_args_with_mask, len(curr_model_inputs)\n",
    "                )\n",
    "                if expanded_target is None:\n",
    "                    expanded_target = _expand_target(target, len(curr_model_inputs))\n",
    "        \n",
    "                model_out = self._evaluate_batch(\n",
    "                    curr_model_inputs,\n",
    "                    expanded_target,\n",
    "                    expanded_additional_args,\n",
    "                    device,\n",
    "                )\n",
    "                if show_progress:\n",
    "                    attr_progress.update()\n",
    "                outputs.append(model_out)\n",
    "        \n",
    "                curr_model_inputs = []\n",
    "\n",
    "                # =============================================================== #\n",
    "                # === MONITORING: re-fit & log when a checkpoint is reached ===== #\n",
    "                # =============================================================== #\n",
    "                if MONITOR and (\n",
    "                    len(interpretable_inps) % k_monitor_conv == 0\n",
    "                    or len(interpretable_inps) % k_monitor_delta == 0\n",
    "                ):\n",
    "                    ### MONITOR BEGIN\n",
    "                    # build DataLoader with *all* samples so far\n",
    "                    X = torch.cat(interpretable_inps).float()\n",
    "                    y = (\n",
    "                        torch.cat(outputs)\n",
    "                        if len(outputs[0].shape) > 0\n",
    "                        else torch.stack(outputs)\n",
    "                    ).float()\n",
    "                    w = torch.cat(similarities).float()\n",
    "        \n",
    "                    dl_mon = DataLoader(\n",
    "                        TensorDataset(X, y, w), batch_size=len(X)\n",
    "                    )\n",
    "        \n",
    "                    # one API call â†’ fast enough for â‰¤ few 100 samples\n",
    "                    self.interpretable_model.fit(dl_mon)\n",
    "        \n",
    "                    # ---------- obtain coefficients as a clean 1-D tensor -----------------\n",
    "                    rep = self.interpretable_model.representation()        # may be Tensor / np / list\n",
    "                    \n",
    "                    beta_cur = (\n",
    "                        rep.flatten().to(\"cpu\")                         # if already a Tensor\n",
    "                        if isinstance(rep, torch.Tensor)\n",
    "                        else torch.as_tensor(rep, dtype=torch.float32, device=\"cpu\").flatten() # keep these small vectors in CPU\n",
    "                    )\n",
    "                    phi0, phis = beta_cur[0].item(), beta_cur[1:]          # scalar + 1-D tensor\n",
    "                    # ----------------------------------------------------------------------\n",
    "                    \n",
    "                    # ---- Convergence distance -------------------------------------------\n",
    "                    if len(interpretable_inps) % k_monitor_conv == 0 and beta_prev is not None:\n",
    "                        dist = torch.norm(beta_cur - beta_prev, p=1).item()\n",
    "                        logf.write(json.dumps({\n",
    "                            \"iter\": len(interpretable_inps),\n",
    "                            \"conv_dist_L1\": dist\n",
    "                        }) + \"\\n\")\n",
    "\n",
    "        \n",
    "                    # ---- Local-accuracy residual ------------------------------ #\n",
    "                    if len(interpretable_inps) % k_monitor_delta == 0:\n",
    "                        # take the output of the unperturbed input (ASSUMES first sample is unperturbed, only true for KernelSHAP)\n",
    "                        model_fwd_original = outputs[0]\n",
    "                        fx = model_fwd_original.item() if torch.is_tensor(model_fwd_original) else model_fwd_original\n",
    "                        delta = abs(fx - (phi0 + sum(phis))).item()\n",
    "                        logf.write(json.dumps({\n",
    "                            \"iter\": len(interpretable_inps),\n",
    "                            \"delta_shap\": delta\n",
    "                        }) + \"\\n\")\n",
    "\n",
    "                    # flush the log file\n",
    "                    logf.flush()\n",
    "\n",
    "                    # DEBUG store whole dataset each 10 iterations\n",
    "                    import pickle\n",
    "                    if len(interpretable_inps) % 10 == 0:\n",
    "                        with open('dataset-autosave.pkl', 'wb') as file:\n",
    "                            pickle.dump(dl_mon, file)\n",
    "\n",
    "                    beta_prev = beta_cur.detach().cpu()\n",
    "                    ### MONITOR END\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------- #\n",
    "        # 4.  Flush any leftover mini-batch                                     #\n",
    "        # --------------------------------------------------------------------- #\n",
    "        if len(curr_model_inputs) > 0:\n",
    "            expanded_additional_args = _expand_additional_forward_args(\n",
    "                additional_forward_args_with_mask, len(curr_model_inputs)\n",
    "            )\n",
    "            expanded_target = _expand_target(target, len(curr_model_inputs))\n",
    "        \n",
    "            model_out = self._evaluate_batch(\n",
    "                curr_model_inputs,\n",
    "                expanded_target,\n",
    "                expanded_additional_args,\n",
    "                device,\n",
    "            )\n",
    "            if show_progress:\n",
    "                attr_progress.update()\n",
    "            outputs.append(model_out)\n",
    "        \n",
    "        if show_progress:\n",
    "            attr_progress.close()\n",
    "\n",
    "        # --------------------------------------------------------------------- #\n",
    "        # 5.  Final fit on *all* samples                                        #\n",
    "        # --------------------------------------------------------------------- #\n",
    "        combined_interp_inps = torch.cat(interpretable_inps).float()\n",
    "        combined_outputs = (\n",
    "            torch.cat(outputs) if len(outputs[0].shape) > 0 else torch.stack(outputs)\n",
    "        ).float()\n",
    "        combined_sim = (\n",
    "            torch.cat(similarities)\n",
    "            if len(similarities[0].shape) > 0\n",
    "            else torch.stack(similarities)\n",
    "        ).float()\n",
    "        \n",
    "        self.dataset = TensorDataset(combined_interp_inps, combined_outputs, combined_sim)\n",
    "        self.interpretable_model.fit(DataLoader(self.dataset, batch_size=batch_count))\n",
    "        \n",
    "        if MONITOR:\n",
    "            logf.close()\n",
    "        \n",
    "        return self.interpretable_model.representation()\n",
    "\n",
    "\n",
    "\n",
    "    def _get_perturbation_mask(\n",
    "        self,\n",
    "        interpretable_input: torch.Tensor,       # shape = (B, M)\n",
    "        original_inputs: TensorOrTupleOfTensorsGeneric, # shape = (B, C, D, H, W) or tuple thereof\n",
    "        feature_mask,\n",
    "    ) -> Union[torch.BoolTensor, Tuple[torch.BoolTensor, ...]]:\n",
    "        \"\"\"\n",
    "        Build a Boolean mask of shape (B, *input_dims) indicating which\n",
    "        elements should be perturbed (True) vs. left untouched (False).\n",
    "        \"\"\"\n",
    "    \n",
    "        # Case 1: singleâ€Tensor input\n",
    "        if isinstance(feature_mask, torch.Tensor):\n",
    "            # advanced indexing over the batch dimension\n",
    "            # result has shape (B, *feature_mask.shape)\n",
    "            mask = interpretable_input[:, feature_mask]\n",
    "            mask = ~mask.bool()\n",
    "\n",
    "            return mask\n",
    "    \n",
    "        # Case 2: multiâ€input (tuple) model\n",
    "        else:\n",
    "            masks = []\n",
    "            for fm_i in feature_mask:\n",
    "                mask_i = interpretable_input[:, fm_i]  # â†’ (B, *fm_i.shape)\n",
    "                masks.append(~mask_i.bool())\n",
    "            return tuple(masks)\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_perturb_generator_func(\n",
    "        self, inputs: TensorOrTupleOfTensorsGeneric, **kwargs: Any\n",
    "    ) -> Callable[\n",
    "        [], Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n",
    "    ]:\n",
    "        perturb_generator: Optional[Iterator[TensorOrTupleOfTensorsGeneric]]\n",
    "        perturb_generator = None\n",
    "        if inspect.isgeneratorfunction(self.perturb_func):\n",
    "            perturb_generator = self.perturb_func(inputs, **kwargs)\n",
    "\n",
    "        def generate_perturbation() -> (\n",
    "            Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n",
    "        ):\n",
    "            if perturb_generator:\n",
    "                curr_sample = next(perturb_generator)\n",
    "            else:\n",
    "                curr_sample = self.perturb_func(inputs, **kwargs)\n",
    "\n",
    "            if self.perturb_interpretable_space:\n",
    "                interpretable_inp = curr_sample\n",
    "                curr_model_input = self.from_interp_rep_transform(  # type: ignore\n",
    "                    curr_sample, inputs, **kwargs\n",
    "                )\n",
    "            else:\n",
    "                curr_model_input = curr_sample\n",
    "                interpretable_inp = self.to_interp_rep_transform(  # type: ignore\n",
    "                    curr_sample, inputs, **kwargs\n",
    "                )\n",
    "\n",
    "            return interpretable_inp, curr_model_input  # type: ignore\n",
    "\n",
    "        return generate_perturbation\n",
    "\n",
    "    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.)\n",
    "    def attribute_future(self) -> Callable:\n",
    "        r\"\"\"\n",
    "        This method is not implemented for LimeBase.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"LimeBase does not support attribution of future samples.\"\n",
    "        )\n",
    "\n",
    "    def _evaluate_batch(\n",
    "        self,\n",
    "        curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n",
    "        expanded_target: TargetType,\n",
    "        expanded_additional_args: object,\n",
    "        device: torch.device,\n",
    "    ) -> Tensor:\n",
    "        model_out = _run_forward(\n",
    "            self.forward_func,\n",
    "            #MOMENTANEAOUS---> sliding_window forward function only works with single items (no batch) --> take first\n",
    "            #_reduce_list(curr_model_inputs),\n",
    "            _reduce_list(curr_model_inputs)[0],\n",
    "            expanded_target,\n",
    "            expanded_additional_args,\n",
    "        )\n",
    "        if isinstance(model_out, Tensor):\n",
    "            assert model_out.numel() == len(curr_model_inputs), (\n",
    "                \"Number of outputs is not appropriate, must return \"\n",
    "                \"one output per perturbed input\"\n",
    "            )\n",
    "        if isinstance(model_out, Tensor):\n",
    "            return model_out.flatten()\n",
    "        return torch.tensor([model_out], device=device)\n",
    "\n",
    "    def has_convergence_delta(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def multiplies_by_inputs(self) -> bool:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Default transformations and methods\n",
    "# for Lime child implementation.\n",
    "\n",
    "\n",
    "# pyre-fixme[3]: Return type must be annotated.\n",
    "# pyre-fixme[2]: Parameter must be annotated.\n",
    "def default_from_interp_rep_transform(curr_sample, original_inputs, **kwargs):\n",
    "    assert (\n",
    "        \"feature_mask\" in kwargs\n",
    "    ), \"Must provide feature_mask to use default interpretable representation transform\"\n",
    "    assert (\n",
    "        \"baselines\" in kwargs\n",
    "    ), \"Must provide baselines to use default interpretable representation transform\"\n",
    "    feature_mask = kwargs[\"feature_mask\"]\n",
    "    if isinstance(feature_mask, Tensor):\n",
    "        binary_mask = curr_sample[0][feature_mask].bool()\n",
    "        input_space_transformed = (\n",
    "            binary_mask.to(original_inputs.dtype) * original_inputs\n",
    "            + (~binary_mask).to(original_inputs.dtype) * kwargs[\"baselines\"]\n",
    "        )\n",
    "        \n",
    "        return input_space_transformed\n",
    "    else:\n",
    "        binary_mask = tuple(\n",
    "            curr_sample[0][feature_mask[j]].bool() for j in range(len(feature_mask))\n",
    "        )\n",
    "        input_space_transformed = tuple(\n",
    "            binary_mask[j].to(original_inputs[j].dtype) * original_inputs[j]\n",
    "            + (~binary_mask[j]).to(original_inputs[j].dtype) * kwargs[\"baselines\"][j]\n",
    "            for j in range(len(feature_mask))\n",
    "        )\n",
    "        return input_space_transformed\n",
    "\n",
    "\n",
    "def get_exp_kernel_similarity_function(\n",
    "    distance_mode: str = \"cosine\",\n",
    "    kernel_width: float = 1.0,\n",
    ") -> Callable[..., float]:\n",
    "    r\"\"\"\n",
    "    This method constructs an appropriate similarity function to compute\n",
    "    weights for perturbed sample in LIME. Distance between the original\n",
    "    and perturbed inputs is computed based on the provided distance mode,\n",
    "    and the distance is passed through an exponential kernel with given\n",
    "    kernel width to convert to a range between 0 and 1.\n",
    "\n",
    "    The callable returned can be provided as the similarity_fn for\n",
    "    Lime or LimeBase.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        distance_mode (str, optional): Distance mode can be either \"cosine\" or\n",
    "                    \"euclidean\" corresponding to either cosine distance\n",
    "                    or Euclidean distance respectively. Distance is computed\n",
    "                    by flattening the original inputs and perturbed inputs\n",
    "                    (concatenating tuples of inputs if necessary) and computing\n",
    "                    distances between the resulting vectors.\n",
    "                    Default: \"cosine\"\n",
    "        kernel_width (float, optional):\n",
    "                    Kernel width for exponential kernel applied to distance.\n",
    "                    Default: 1.0\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        *Callable*:\n",
    "        - **similarity_fn** (*Callable*):\n",
    "            Similarity function. This callable can be provided as the\n",
    "            similarity_fn for Lime or LimeBase.\n",
    "    \"\"\"\n",
    "\n",
    "    # pyre-fixme[3]: Return type must be annotated.\n",
    "    # pyre-fixme[2]: Parameter must be annotated.\n",
    "    def default_exp_kernel(original_inp, perturbed_inp, __, **kwargs):\n",
    "        flattened_original_inp = _flatten_tensor_or_tuple(original_inp).float()\n",
    "        flattened_perturbed_inp = _flatten_tensor_or_tuple(perturbed_inp).float()\n",
    "        if distance_mode == \"cosine\":\n",
    "            cos_sim = CosineSimilarity(dim=0)\n",
    "            distance = 1 - cos_sim(flattened_original_inp, flattened_perturbed_inp)\n",
    "        elif distance_mode == \"euclidean\":\n",
    "            distance = torch.norm(flattened_original_inp - flattened_perturbed_inp)\n",
    "        else:\n",
    "            raise ValueError(\"distance_mode must be either cosine or euclidean.\")\n",
    "        return math.exp(-1 * (distance**2) / (2 * (kernel_width**2)))\n",
    "\n",
    "    return default_exp_kernel\n",
    "\n",
    "\n",
    "def default_perturb_func(\n",
    "    original_inp: TensorOrTupleOfTensorsGeneric, **kwargs: object\n",
    ") -> Tensor:\n",
    "    assert (\n",
    "        \"num_interp_features\" in kwargs\n",
    "    ), \"Must provide num_interp_features to use default interpretable sampling function\"\n",
    "    if isinstance(original_inp, Tensor):\n",
    "        device = original_inp.device\n",
    "    else:\n",
    "        device = original_inp[0].device\n",
    "\n",
    "    probs = torch.ones(1, cast(int, kwargs[\"num_interp_features\"])) * 0.5\n",
    "    return torch.bernoulli(probs).to(device=device).long()\n",
    "\n",
    "\n",
    "def construct_feature_mask(\n",
    "    feature_mask: Union[None, Tensor, Tuple[Tensor, ...]],\n",
    "    formatted_inputs: Tuple[Tensor, ...],\n",
    ") -> Tuple[Tuple[Tensor, ...], int]:\n",
    "    feature_mask_tuple: Tuple[Tensor, ...]\n",
    "    if feature_mask is None:\n",
    "        feature_mask_tuple, num_interp_features = _construct_default_feature_mask(\n",
    "            formatted_inputs\n",
    "        )\n",
    "    else:\n",
    "        feature_mask_tuple = _format_tensor_into_tuples(feature_mask)\n",
    "        min_interp_features = int(\n",
    "            min(\n",
    "                torch.min(single_mask).item()\n",
    "                for single_mask in feature_mask_tuple\n",
    "                if single_mask.numel()\n",
    "            )\n",
    "        )\n",
    "        if min_interp_features != 0:\n",
    "            warnings.warn(\n",
    "                \"Minimum element in feature mask is not 0, shifting indices to\"\n",
    "                \" start at 0.\",\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            feature_mask_tuple = tuple(\n",
    "                single_mask - min_interp_features for single_mask in feature_mask_tuple\n",
    "            )\n",
    "\n",
    "        num_interp_features = _get_max_feature_index(feature_mask_tuple) + 1\n",
    "    return feature_mask_tuple, num_interp_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ed45a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimeWithCustomArgumentToForwardFunc(LimeBaseWithCustomArgumentToForwardFunc):\n",
    "    r\"\"\"\n",
    "    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n",
    "    This will just inherit our modified LimeBase class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        forward_func: Callable[..., Tensor],\n",
    "        interpretable_model: Optional[Model] = None,\n",
    "        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n",
    "        similarity_func: Optional[Callable] = None,\n",
    "        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n",
    "        perturb_func: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "\n",
    "            forward_func (Callable): The forward function of the model or any\n",
    "                    modification of it\n",
    "            interpretable_model (Model, optional): Model object to train\n",
    "                    interpretable model.\n",
    "\n",
    "                    This argument is optional and defaults to SkLearnLasso(alpha=0.01),\n",
    "                    which is a wrapper around the Lasso linear model in SkLearn.\n",
    "                    This requires having sklearn version >= 0.23 available.\n",
    "\n",
    "                    Other predefined interpretable linear models are provided in\n",
    "                    captum._utils.models.linear_model.\n",
    "\n",
    "                    Alternatively, a custom model object must provide a `fit` method to\n",
    "                    train the model, given a dataloader, with batches containing\n",
    "                    three tensors:\n",
    "\n",
    "                    - interpretable_inputs: Tensor\n",
    "                      [2D num_samples x num_interp_features],\n",
    "                    - expected_outputs: Tensor [1D num_samples],\n",
    "                    - weights: Tensor [1D num_samples]\n",
    "\n",
    "                    The model object must also provide a `representation` method to\n",
    "                    access the appropriate coefficients or representation of the\n",
    "                    interpretable model after fitting.\n",
    "\n",
    "                    Note that calling fit multiple times should retrain the\n",
    "                    interpretable model, each attribution call reuses\n",
    "                    the same given interpretable model object.\n",
    "            similarity_func (Callable, optional): Function which takes a single sample\n",
    "                    along with its corresponding interpretable representation\n",
    "                    and returns the weight of the interpretable sample for\n",
    "                    training the interpretable model.\n",
    "                    This is often referred to as a similarity kernel.\n",
    "\n",
    "                    This argument is optional and defaults to a function which\n",
    "                    applies an exponential kernel to the cosine distance between\n",
    "                    the original input and perturbed input, with a kernel width\n",
    "                    of 1.0.\n",
    "\n",
    "                    A similarity function applying an exponential\n",
    "                    kernel to cosine / euclidean distances can be constructed\n",
    "                    using the provided get_exp_kernel_similarity_function in\n",
    "                    captum.attr._core.lime.\n",
    "\n",
    "                    Alternately, a custom callable can also be provided.\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> def similarity_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_interpretable_input:\n",
    "                    >>>        Tensor [2D 1 x num_interp_features],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> float or Tensor containing float scalar\n",
    "\n",
    "                    perturbed_input and original_input will be the same type and\n",
    "                    contain tensors of the same shape, with original_input\n",
    "                    being the same as the input provided when calling attribute.\n",
    "\n",
    "                    kwargs includes baselines, feature_mask, num_interp_features\n",
    "                    (integer, determined from feature mask).\n",
    "            perturb_func (Callable, optional): Function which returns a single\n",
    "                    sampled input, which is a binary vector of length\n",
    "                    num_interp_features, or a generator of such tensors.\n",
    "\n",
    "                    This function is optional, the default function returns\n",
    "                    a binary vector where each element is selected\n",
    "                    independently and uniformly at LimeWithCustomArgumentToForwardFuncrandom. Custom\n",
    "                    logic for selecting sampled binary vectors can\n",
    "                    be implemented by providing a function with the\n",
    "                    following expected signature:\n",
    "\n",
    "                    >>> perturb_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor [Binary 2D Tensor 1 x num_interp_features]\n",
    "                    >>>  or generator yielding such tensors\n",
    "\n",
    "                    kwargs includes baselines, feature_mask, num_interp_features\n",
    "                    (integer, determined from feature mask).\n",
    "\n",
    "        \"\"\"\n",
    "        if interpretable_model is None:\n",
    "            interpretable_model = SkLearnLasso(alpha=0.01)\n",
    "\n",
    "        if similarity_func is None:\n",
    "            similarity_func = get_exp_kernel_similarity_function()\n",
    "\n",
    "        if perturb_func is None:\n",
    "            perturb_func = default_perturb_func\n",
    "\n",
    "        LimeBaseWithCustomArgumentToForwardFunc.__init__(\n",
    "            self,\n",
    "            forward_func,\n",
    "            interpretable_model,\n",
    "            similarity_func,\n",
    "            perturb_func,\n",
    "            True,\n",
    "            default_from_interp_rep_transform,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    @log_usage(part_of_slo=True)\n",
    "    def attribute(  # type: ignore\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[object] = None,\n",
    "        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n",
    "        n_samples: int = 25,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        return_input_shape: bool = True,\n",
    "        show_progress: bool = False,\n",
    "    ) -> TensorOrTupleOfTensorsGeneric:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to the inputs of the model using the approach described above,\n",
    "        training an interpretable model and returning a representation of the\n",
    "        interpretable model.\n",
    "\n",
    "        It is recommended to only provide a single example as input (tensors\n",
    "        with first dimension or batch size = 1). This is because LIME is generally\n",
    "        used for sample-based interpretability, training a separate interpretable\n",
    "        model to explain a model's prediction on each individual example.\n",
    "\n",
    "        A batch of inputs can also be provided as inputs, similar to\n",
    "        other perturbation-based attribution methods. In this case, if forward_fn\n",
    "        returns a scalar per example, attributions will be computed for each\n",
    "        example independently, with a separate interpretable model trained for each\n",
    "        example. Note that provided similarity and pertforward_funcurbation functions will be\n",
    "        provided each example separately (first dimension = 1) in this case.\n",
    "        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n",
    "        still be computed using a single interpretable model for the full batch.\n",
    "        In this case, similarity and perturbation functions will be provided the\n",
    "        same original input containing the full batch.\n",
    "\n",
    "        The number of interpretable features is determined from the provided\n",
    "        feature mask, or if none is provided, from the default feature mask,\n",
    "        which considers each scalar input as a separate feature. It is\n",
    "        generally recommended to provide a feature mask which groups features\n",
    "        into a small number of interpretable features / components (e.g.\n",
    "        superpixels in images).\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n",
    "                        is computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n",
    "                        Baselines define reference value which replaces each\n",
    "                        feature when the corresponding interpretable feature\n",
    "                        is set to 0.\n",
    "                        Baselines can be provided as:\n",
    "\n",
    "                        - a single tensor, if inputs is a single tensor, with\n",
    "                          exactly the same dimensions as inputs or the first\n",
    "                          dimension is one and the remaining dimensions match\n",
    "                          with inputs.\n",
    "\n",
    "                        - a single scalar, if inputs is a single tensor, which will\n",
    "                          be broadcasted for each input value in input tensor.\n",
    "\n",
    "                        - a tuple of tensors or scalars, the baseline corresponding\n",
    "                          to each tensor in the inputs' tuple can be:\n",
    "\n",
    "                          - either a tensor with matching dimensions to\n",
    "                            corresponding tensor in the inputs' tuple\n",
    "                            or the first dimension is one and the remaining\n",
    "                            dimensions match with the corresponding\n",
    "                            input tensor.\n",
    "\n",
    "                          - or a scalar, corresponding to a tensor in the\n",
    "                            inputs' tuple. This scalar value is broadcasted\n",
    "                            for corresponding input tensor.\n",
    "\n",
    "                        In the cases when `baselines` iforward_funcs not provided, we internally\n",
    "                        use zero scalar corresponding to each input tensor.\n",
    "                        Default: None\n",
    "            target (int, tuple, Tensor, or list, optional): Output indices for\n",
    "                        which surrogate model is trained\n",
    "                        (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs with > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (Any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. It will be\n",
    "                        repeated for each of `n_steps` along the integrated\n",
    "                        path. For all other types, the given argument is used\n",
    "                        for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            feature_mask (Tensor or tuple[Tensor, ...], optional):\n",
    "                        feature_mask defines a mask for the input, grouping\n",
    "                        features which correspond to the same\n",
    "                        interpretable feature. feature_mask\n",
    "                        should contain the same number of tensors as inputs.\n",
    "                        Each tensor should\n",
    "                        be the same size as the corresponding input or\n",
    "                        broadcastable to match the inpuforward_funct tensor. Values across\n",
    "                        all tensors should be integers in the range 0 to\n",
    "                        num_interp_features - 1, and indices corresponding to the\n",
    "                        same feature should have the same value.\n",
    "                        Note that features are grouped across tensors\n",
    "                        (unlike feature ablation and occlusion), so\n",
    "                        if the same index is used in different tensors, those\n",
    "                        features are still grouped and added simultaneously.\n",
    "                        If None, then a feature mask is constructed which assigns\n",
    "                        each scalar within a tensor as a separate feature.\n",
    "                        Default: None\n",
    "            n_samples (int, optional): The number of samples of the original\n",
    "                        model used to train the surrogate interpretable model.\n",
    "                        Default: `50` if `n_samples` is not provided.\n",
    "            perturbations_per_eval (int, optional): Allows multiple samples\n",
    "                        to be processed simultaneously in one call to forward_fn.\n",
    "                        Each forward pass will contain a maximum of\n",
    "                        perturbations_per_eval * #examples samples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain at most\n",
    "                        (perturbations_per_eval * #examples) / num_devices\n",
    "                        samples.\n",
    "                        If the forward function returns a single scalar per batch,\n",
    "                        perturbations_per_eval must be set to 1.\n",
    "                        Default: 1\n",
    "            return_input_shape (bool, optional): Determines whether the returned\n",
    "                        tensor(s) only contain the coefficients for each interp-\n",
    "                        retable feature from the trained surrogate model, or\n",
    "                        whether the returned attributions match the input shape.\n",
    "                        When return_input_shape is True, the return type of attribute\n",
    "                        matches the input shape, with each element containing the\n",
    "                        coefficient of the corresponding interpretale feature.\n",
    "                        All elements with the same value in the feature mask\n",
    "                        will contain the same coefficient in the returned\n",
    "                        attributions.\n",
    "                        If forward_func returns a single element per batch, then the\n",
    "                        first dimension of each tensor will be 1, and the remaining\n",
    "                        dimensions will have the same shape as the original input\n",
    "                        tensor.\n",
    "                        If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpreatable models, with length\n",
    "                        num_interp_features.\n",
    "            show_progress (bool, optional): Displays the progress of computation.\n",
    "                        It will try to use tqdm if available for advanced features\n",
    "                        (e.g. time estimation). Otherwise, it will fallback to\n",
    "                        a simple output of progress.\n",
    "                        Default: False\n",
    "\n",
    "        Returns:\n",
    "            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n",
    "            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n",
    "                        The attributions with respect to each input feature.\n",
    "                        If return_input_shape = True, attributions will be\n",
    "                        the same size as the provided inputs, with each value\n",
    "                        providing the coefficient of the corresponding\n",
    "                        interpretale feature.\n",
    "                        If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpreatable models, with length\n",
    "                        num_interp_features.\n",
    "        Examples::\n",
    "\n",
    "            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n",
    "            >>> # and returns an Nx3 tensor of class probabilities.\n",
    "            >>> net = SimpleClassifier()\n",
    "\n",
    "            >>> # Generating random input with size 1 x 4 x 4\n",
    "            >>> input = torch.randn(1, 4, 4)\n",
    "\n",
    "            >>> # Defining Lime interpreter\n",
    "            >>> lime = Lime(net)\n",
    "            >>> # Computes attribution, with each of the 4 x 4 = 16\n",
    "            >>> # features as a separate interpretable feature\n",
    "            >>> attr = lime.attribute(input, target=1, n_samples=200)\n",
    "\n",
    "            >>> # Alternatively, we can group each 2x2 square of the inputs\n",
    "            >>> # as one 'interpretable' feature and perturb them together.\n",
    "            >>> # This can be done by creating a feature mask as follows, which\n",
    "            >>> # defines the feature groups, e.g.:\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # With this mask, all inputs with the same value are set to their\n",
    "            >>> # baseline value, when the corresponding binary interpretable\n",
    "            >>> # feature is set to 0.\n",
    "            >>> # The attributions can be calculated as follows:\n",
    "            >>> # feature mask has dimensions 1 x 4 x 4\n",
    "            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n",
    "            >>>                             [2,2,3,3],[2,2,3,3]]])\n",
    "\n",
    "            >>> # Computes interpretable model and returning attributions\n",
    "            >>> # matching input shape.\n",
    "            >>> attr = lime.attribute(input, target=1, feature_mask=feature_mask)\n",
    "        \"\"\"\n",
    "        return self._attribute_kwargs(\n",
    "            inputs=inputs,\n",
    "            baselines=baselines,\n",
    "            target=target,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            feature_mask=feature_mask,\n",
    "            n_samples=n_samples,\n",
    "            perturbations_per_eval=perturbations_per_eval,\n",
    "            return_input_shape=return_input_shape,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n",
    "    def attribute_future(self) -> Callable:\n",
    "        return super().attribute_future()\n",
    "\n",
    "    def _attribute_kwargs(  # type: ignore\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[object] = None,\n",
    "        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n",
    "        n_samples: int = 25,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        return_input_shape: bool = True,\n",
    "        monitor_log_path: str | None = None,\n",
    "        monitor_convergence_step: int | None = 20,\n",
    "        monitor_local_accuracy_step: int | None = 50,\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: object,\n",
    "    ) -> TensorOrTupleOfTensorsGeneric:\n",
    "        is_inputs_tuple = _is_tuple(inputs)\n",
    "        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n",
    "        bsz = formatted_inputs[0].shape[0]\n",
    "\n",
    "        feature_mask, num_interp_features = construct_feature_mask(\n",
    "            feature_mask, formatted_inputs\n",
    "        )\n",
    "\n",
    "        if num_interp_features > 10000:\n",
    "            warnings.warn(\n",
    "                \"Attempting to construct interpretable model with > 10000 features.\"\n",
    "                \"This can be very slow or lead to OOM issues. Please provide a feature\"\n",
    "                \"mask which groups input features to reduce the number of interpretable\"\n",
    "                \"features. \",\n",
    "                stacklevel=1,\n",
    "            )\n",
    "\n",
    "        coefs: Tensor\n",
    "        if bsz > 1:\n",
    "            test_output = _run_forward(\n",
    "                self.forward_func, inputs, target, additional_forward_args\n",
    "            )\n",
    "            if isinstance(test_output, Tensor) and torch.numel(test_output) > 1:\n",
    "                if torch.numel(test_output) == bsz:\n",
    "                    warnings.warn(\n",
    "                        \"You are providing multiple inputs for Lime / Kernel SHAP \"\n",
    "                        \"attributions. This trains a separate interpretable model \"\n",
    "                        \"for each example, which can be time consuming. It is \"\n",
    "                        \"recommended to compute attributions for one example at a \"\n",
    "                        \"time.\",\n",
    "                        stacklevel=1,\n",
    "                    )\n",
    "                    output_list = []\n",
    "                    for (\n",
    "                        curr_inps,\n",
    "                        curr_target,\n",
    "                        curr_additional_args,\n",
    "                        curr_baselines,\n",
    "                        curr_feature_mask,\n",
    "                    ) in _batch_example_iterator(\n",
    "                        bsz,\n",
    "                        formatted_inputs,\n",
    "                        target,\n",
    "                        additional_forward_args,# -----> CAN BE ALSO BATCHED AUTOMATICALLY BY THE LIBRARY ITERATOR\n",
    "                        baselines,\n",
    "                        feature_mask,\n",
    "                    ):\n",
    "                        coefs = super().attribute.__wrapped__(\n",
    "                            self,\n",
    "                            inputs=curr_inps if is_inputs_tuple else curr_inps[0],\n",
    "                            target=curr_target,\n",
    "                            additional_forward_args=curr_additional_args,\n",
    "                            n_samples=n_samples,\n",
    "                            perturbations_per_eval=perturbations_per_eval,\n",
    "                            baselines=(\n",
    "                                curr_baselines if is_inputs_tuple else curr_baselines[0]\n",
    "                            ),\n",
    "                            feature_mask=(\n",
    "                                curr_feature_mask\n",
    "                                if is_inputs_tuple\n",
    "                                else curr_feature_mask[0]\n",
    "                            ),\n",
    "                            num_interp_features=num_interp_features,\n",
    "                            show_progress=show_progress,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                        if return_input_shape:\n",
    "                            output_list.append(\n",
    "                                self._convert_output_shape(\n",
    "                                    curr_inps,\n",
    "                                    curr_feature_mask,\n",
    "                                    coefs,\n",
    "                                    num_interp_features,\n",
    "                                    is_inputs_tuple,\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            output_list.append(coefs.reshape(1, -1))  # type: ignore\n",
    "\n",
    "                    return _reduce_list(output_list)\n",
    "                else:\n",
    "                    raise AssertionError(\n",
    "                        \"Invalid number of outputs, forward function should return a\"\n",
    "                        \"scalar per example or a scalar per input batch.\"\n",
    "                    )\n",
    "            else:\n",
    "                assert perturbations_per_eval == 1, (\n",
    "                    \"Perturbations per eval must be 1 when forward function\"\n",
    "                    \"returns single value per batch!\"\n",
    "                )\n",
    "\n",
    "        coefs = super().attribute.__wrapped__(\n",
    "            self,\n",
    "            inputs=inputs,\n",
    "            target=target,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            n_samples=n_samples,\n",
    "            perturbations_per_eval=perturbations_per_eval,\n",
    "            baselines=baselines if is_inputs_tuple else baselines[0],\n",
    "            feature_mask=feature_mask if is_inputs_tuple else feature_mask[0],\n",
    "            num_interp_features=num_interp_features,\n",
    "            monitor_log_path = monitor_log_path,\n",
    "            monitor_convergence_step = monitor_convergence_step,\n",
    "            monitor_local_accuracy_step = monitor_local_accuracy_step,\n",
    "            show_progress=show_progress,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if return_input_shape:\n",
    "            # pyre-fixme[7]: Expected `TensorOrTupleOfTensorsGeneric` but got\n",
    "            #  `Tuple[Tensor, ...]`.\n",
    "            return self._convert_output_shape(\n",
    "                formatted_inputs,\n",
    "                feature_mask,\n",
    "                coefs,\n",
    "                num_interp_features,\n",
    "                is_inputs_tuple,\n",
    "    \n",
    "            leading_dim_one=(bsz > 1),\n",
    "            )\n",
    "        else:\n",
    "            return coefs\n",
    "\n",
    "    @typing.overload\n",
    "    def _convert_output_shape(\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: Literal[True],\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Tuple[Tensor, ...]: ...\n",
    "\n",
    "    @typing.overload\n",
    "    def _convert_output_shape(  # type: ignore\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: Literal[False],\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Tensor: ...\n",
    "\n",
    "    @typing.overload\n",
    "    def _convert_output_shape(\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: bool,\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Union[Tensor, Tuple[Tensor, ...]]: ...\n",
    "\n",
    "    def _convert_output_shape(\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: bool,\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Union[Tensor, Tuple[Tensor, ...]]:\n",
    "        coefs = coefs.flatten()\n",
    "        attr = [\n",
    "            torch.zeros_like(single_inp, dtype=torch.float)\n",
    "            for single_inp in formatted_inp\n",
    "        ]\n",
    "        for tensor_ind in range(len(formatted_inp)):\n",
    "            for single_feature in range(num_interp_features):\n",
    "                attr[tensor_ind] += (\n",
    "                    coefs[single_feature].item()\n",
    "                    * (feature_mask[tensor_ind] == single_feature).float()\n",
    "                )\n",
    "        if leading_dim_one:\n",
    "            for i in range(len(attr)):\n",
    "                attr[i] = attr[i][0:1]\n",
    "        return _format_output(is_inputs_tuple, tuple(attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "134b2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# pyre-strict\n",
    "\n",
    "from typing import Callable, cast, Generator, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from captum._utils.models.linear_model import SkLearnLinearRegression\n",
    "from captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\n",
    "from captum.attr._core.lime import construct_feature_mask, Lime\n",
    "from captum.attr._utils.common import _format_input_baseline\n",
    "from captum.log import log_usage\n",
    "from torch import Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "class KernelShapWithMask(LimeWithCustomArgumentToForwardFunc):\n",
    "    r\"\"\"\n",
    "    Kernel SHAP is a method that uses the LIME framework to compute\n",
    "    Shapley Values. Setting the loss function, weighting kernel and\n",
    "    regularization terms appropriately in the LIME framework allows\n",
    "    theoretically obtaining Shapley Values more efficiently than\n",
    "    directly computing Shapley Values.\n",
    "\n",
    "    More information regarding this method and proof of equivalence\n",
    "    can be found in the original paper here:\n",
    "    https://arxiv.org/abs/1705.07874\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 forward_func: Callable[..., Tensor],\n",
    "                 surrogate_model: str = \"linear regression\",\n",
    "                 alpha_surrogate: float = 0.01,\n",
    "                 max_iter_surrogate: int = 1000\n",
    "                ) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "\n",
    "            forward_func (Callable): The forward function of the model or\n",
    "                        any modification of it.\n",
    "        \"\"\"\n",
    "        if surrogate_model == \"linear regression\":\n",
    "            interpretable_model = SkLearnLinearRegression()\n",
    "        elif surrogate_model == \"lasso\": \n",
    "            interpretable_model = SkLearnLasso(alpha=alpha_surrogate, max_iter=max_iter_surrogate)\n",
    "            \n",
    "        LimeWithCustomArgumentToForwardFunc.__init__(\n",
    "            self,\n",
    "            forward_func,\n",
    "            interpretable_model=interpretable_model,\n",
    "            similarity_func=self.kernel_shap_similarity_kernel,\n",
    "            perturb_func=self.kernel_shap_perturb_generator,\n",
    "        )\n",
    "        self.inf_weight = 1000000.0\n",
    "\n",
    "    @log_usage(part_of_slo=True)\n",
    "    def attribute(  # type: ignore\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[object] = None,\n",
    "        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n",
    "        n_samples: int = 25,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        return_input_shape: bool = True,\n",
    "        monitor_log_path: str | None = None,\n",
    "        monitor_convergence_step: int | None = 20,\n",
    "        monitor_local_accuracy_step: int | None = 50,\n",
    "        show_progress: bool = False,\n",
    "    ) -> TensorOrTupleOfTensorsGeneric:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to the inputs of the model using the approach described above,\n",
    "        training an interpretable model based on KernelSHAP and returning a\n",
    "        representation of the interpretable model.\n",
    "\n",
    "        It is recommended to only provide a single example as input (tensors\n",
    "        with first dimension or batch size = 1). This is because LIME / KernelShap\n",
    "        is generally used for sample-based interpretability, training a separate\n",
    "        interpretable model to explain a model's prediction on each individual example.\n",
    "\n",
    "        A batch of inputs can also be provided as inputs, similar to\n",
    "        other perturbation-based attribution methods. In this case, if forward_fn\n",
    "        returns a scalar per example, attributions will be computed for each\n",
    "        example independently, with a separate interpretable model trained for each\n",
    "        example. Note that provided similarity and perturbation functions will be\n",
    "        provided each example separately (first dimension = 1) in this case.\n",
    "        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n",
    "        still be computed using a single interpretable model for the full batch.\n",
    "        In this case, similarity and perturbation functions will be provided the\n",
    "        same original input containing the full batch.\n",
    "\n",
    "        The number of interpretable features is determined from the provided\n",
    "        feature mask, or if none is provided, from the default feature mask,\n",
    "        which considers each scalar input as a separate feature. It is\n",
    "        generally recommended to provide a feature mask which groups features\n",
    "        into a small number of interpretable features / components (e.g.\n",
    "        superpixels in images).\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (Tensor or tuple[Tensor, ...]): Input for which KernelShap\n",
    "                        is computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n",
    "                        Baselines define the reference value which replaces eachconv_dist_L1 and delta_shap\n",
    "                        feature when the corresponding interpretable feature\n",
    "                        is set to 0.\n",
    "                        Baselines can be provided as:\n",
    "\n",
    "                        - a single tensor, if inputs is a single tensor, with\n",
    "                          exactly the same dimensions as inputs or the first\n",
    "                          dimension is one and the remaining dimensions match\n",
    "                          with inputs.\n",
    "\n",
    "                        - a single scalar, if inputs is a single tensor, which will\n",
    "                          be broadcasted for each input value in input tensor.\n",
    "\n",
    "                        - a tuple of tensors or scalars, the baseline corresponding\n",
    "                          to each tensor in the inputs' tuple can be:\n",
    "\n",
    "                          - either a tensor with matching dimensions to\n",
    "                            corresponding tensor in the inputs' tuple\n",
    "                            or the first dimension is one and the remaining\n",
    "                            dimensions match with the corresponding\n",
    "                            input tensor.\n",
    "\n",
    "                          - or a scalar, corresponding to a tensor in the\n",
    "                            inputs' tuple. This scalar value is broadcasted\n",
    "                            for corresponding input tensor.\n",
    "\n",
    "                        In the cases when `baselines` is not provided, we internally\n",
    "                        use zero scalar corresponding to each input tensor.\n",
    "                        Default: None\n",
    "            target (int, tuple, Tensor, or list, optional): Output indices for\n",
    "                        which surrogate model is trained\n",
    "                        (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs with > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (Any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. It will be\n",
    "                        repeated for each of `n_steps` along the integrated\n",
    "                        path. For all other types, the given argument is used\n",
    "                        for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            feature_mask (Tensor or tuple[Tensor, ...], optional):\n",
    "                        feature_mask defines a mask for the input, grouping\n",
    "                        features which correspond to the same\n",
    "                        interpretable feature. feature_mask\n",
    "                        should contain the same number of tensors as inputs.\n",
    "                        Each tensor should\n",
    "                        be the same size as the corresponding input or\n",
    "                        broadcastable to match the input tensor. Values across\n",
    "                        all tensors should be integers in the range 0 to\n",
    "                        num_interp_features - 1, and indices corresponding to the\n",
    "                        same feature should have the same value.\n",
    "                        Note that features are grouped across tensors\n",
    "                        (unlike feature ablation and occlusion), so\n",
    "                        if the same index is used in different tensors, those\n",
    "                        features are still grouped and added simultaneously.\n",
    "                        If None, then a feature mask is constructed which assigns\n",
    "                        each scalar within a tensor as a separate feature.\n",
    "                        Default: None\n",
    "            n_samples (int, optional): The number of samples of the original\n",
    "                        model used to train the surrogate interpretable model.\n",
    "                        Default: `50` if `n_samples` is not provided.\n",
    "            perturbations_per_eval (int, optional): Allows multiple samples\n",
    "                        to be processed simultaneously in one call to forward_fn.\n",
    "                        Each forward pass will contain a maximum of\n",
    "                        perturbations_per_eval * #examples samples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain at most\n",
    "                        (perturbations_per_eval * #examples) / num_devices\n",
    "                        samples.\n",
    "                        If the forward function returns a single scalar per batch,\n",
    "                        perturbations_per_eval must be set to 1.\n",
    "                        Default: 1\n",
    "            return_input_shape (bool, optional): Determines whether the returned\n",
    "                        tensor(s) only contain the coefficients for each interp-\n",
    "                        retable feature from the trained surrogate model, or\n",
    "                        whether the returned attributions match the input shape.\n",
    "                        When return_input_shape is True, the return type of attribute\n",
    "                        matches the input shape, with each element containing the\n",
    "                        coefficient of the corresponding interpretable feature.\n",
    "                        All elements with the same value in the feature mask\n",
    "                        will contain the same coefficient in the returned\n",
    "                        attributions. If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpretable model, with length\n",
    "                        num_interp_features.\n",
    "            monitor_log_path (str, optional): Path to the log file for monitoring convergence.\n",
    "                        if None, no monitoring is performed.\n",
    "                        Default: None\n",
    "            monitor_convergence_step (int, optional): Number of iterations over which\n",
    "                        the difference among two attribution is computerd.\n",
    "                        Default: 20\n",
    "            monitor_local_accuracy_step (int, optional): Number of iterations over which\n",
    "                        the local accuracy of an attribution is computerd.\n",
    "                        Default: 50\n",
    "            show_progress (bool, optional): Displays the progress of computation.\n",
    "                        It will try to use tqdm if available for advanced features\n",
    "                        (e.g. time estimation). Otherwise, it will fallback to\n",
    "                        a simple output of progress.\n",
    "                        Default: False\n",
    "\n",
    "        Returns:\n",
    "            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n",
    "            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n",
    "                        The attributions with respect to each input feature.\n",
    "                        If return_input_shape = True, attributions will be\n",
    "                        the same size as the provided inputs, with each value\n",
    "                        providing the coefficient of the corresponding\n",
    "                        interpretale feature.\n",
    "                        If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpreatable models, with length\n",
    "                        num_interp_features.\n",
    "        Examples::\n",
    "            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n",
    "            >>> # and returns an Nx3 tensor of class probabilities.\n",
    "            >>> net = SimpleClassifier()\n",
    "\n",
    "            >>> # Generating random input with size 1 x 4 x 4\n",
    "            >>> input = torch.randn(1, 4, 4)\n",
    "\n",
    "            >>> # Defining KernelShap interpreter\n",
    "            >>> ks = KernelShap(net)\n",
    "            >>> # Computes attribution, with each of the 4 x 4 = 16\n",
    "            >>> # features as a separate interpretable feature\n",
    "            >>> attr = ks.attribute(input, target=1, n_samples=200)\n",
    "\n",
    "            >>> # Alternatively, we can group each 2x2 square of the inputs\n",
    "            >>> # as one 'interpretable' feature and perturb them together.\n",
    "            >>> # This can be done by creating a feature mask as follows, which\n",
    "            >>> # defines the feature groups, e.g.:\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # With this mask, all inputs with the same value are set to their\n",
    "            >>> # baseline value, when the corresponding binary interpretable\n",
    "            >>> # feature is set to 0.\n",
    "            >>> # The attributions can be calculated as follows:\n",
    "            >>> # feature mask has dimensions 1 x 4 x 4\n",
    "            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n",
    "            >>>                             [2,2,3,3],[2,2,3,3]]])\n",
    "\n",
    "            >>> # Computes KernelSHAP attributions with feature mask.\n",
    "            >>> attr = ks.attribute(input, target=1, feature_mask=feature_mask)\n",
    "        \"\"\"\n",
    "        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n",
    "        feature_mask, num_interp_features = construct_feature_mask(\n",
    "            feature_mask, formatted_inputs\n",
    "        )\n",
    "        num_features_list = torch.arange(num_interp_features, dtype=torch.float)\n",
    "        denom = num_features_list * (num_interp_features - num_features_list)\n",
    "        probs = torch.tensor((num_interp_features - 1)) / denom\n",
    "        probs[0] = 0.0\n",
    "        return self._attribute_kwargs(\n",
    "            inputs=inputs,\n",
    "            baselines=baselines,\n",
    "            target=target,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            feature_mask=feature_mask,\n",
    "            n_samples=n_samples,\n",
    "            perturbations_per_eval=perturbations_per_eval,\n",
    "            return_input_shape=return_input_shape,\n",
    "            num_select_distribution=Categorical(probs),\n",
    "            monitor_log_path=monitor_log_path,\n",
    "            monitor_convergence_step=monitor_convergence_step,\n",
    "            monitor_local_accuracy_step=monitor_local_accuracy_step,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n",
    "    def attribute_future(self) -> Callable:\n",
    "        r\"\"\"\n",
    "        This method is not implemented for KernelShap.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"attribute_future is not implemented for KernelShap\")\n",
    "\n",
    "    def kernel_shap_similarity_kernel(\n",
    "        self,\n",
    "        _,\n",
    "        __,\n",
    "        interpretable_sample: Tensor,\n",
    "        **kwargs: object,\n",
    "    ) -> Tensor:\n",
    "        assert (\n",
    "            \"num_interp_features\" in kwargs\n",
    "        ), \"Must provide num_interp_features to use default similarity kernel\"\n",
    "        num_selected_features = int(interpretable_sample.sum(dim=1).item())\n",
    "        num_features = kwargs[\"num_interp_features\"]\n",
    "        if num_selected_features == 0 or num_selected_features == num_features:\n",
    "            # weight should be theoretically infinite when\n",
    "            # num_selected_features = 0 or num_features\n",
    "            # enforcing that trained linear model must satisfy\n",
    "            # end-point criteria. In practice, it is sufficient to\n",
    "            # make this weight substantially larger so setting this\n",
    "            # weight to 1000000 (all other weights are 1).\n",
    "            similarities = self.inf_weight\n",
    "        else:\n",
    "            similarities = 1.0\n",
    "        return torch.tensor([similarities])\n",
    "\n",
    "    def kernel_shap_perturb_generator(\n",
    "        self,\n",
    "        original_inp: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        **kwargs: object,\n",
    "    ) -> Generator[Tensor, None, None]:\n",
    "        r\"\"\"\n",
    "        Perturbations are sampled by the following process:\n",
    "         - Choose k (number of selected features), based on the distribution\n",
    "                p(k) = (M - 1) / (k * (M - k))\n",
    "\n",
    "            where M is the total number of features in the interpretable space\n",
    "\n",
    "         - Randomly select a binary vector with k ones, each sample is equally\n",
    "            likely. This is done by generating a random vector of normal\n",
    "            values and thresholding based on the top k elements.\n",
    "\n",
    "         Since there are M choose k vectors with k ones, this weighted sampling\n",
    "         is equivalent to applying the Shapley kernel for the sample weight,\n",
    "         defined as:\n",
    "         k(M, k) = (M - 1) / (k * (M - k) * (M choose k))\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            \"num_select_distribution\" in kwargs and \"num_interp_features\" in kwargs\n",
    "        ), (\n",
    "            \"num_select_distribution and num_interp_features are necessary\"\n",
    "            \" to use kernel_shap_perturb_func\"\n",
    "        )\n",
    "        if isinstance(original_inp, Tensor):\n",
    "            device = original_inp.device\n",
    "        else:\n",
    "            device = original_inp[0].device\n",
    "        num_features = cast(int, kwargs[\"num_interp_features\"])\n",
    "        yield torch.ones(1, num_features, device=device, dtype=torch.long)\n",
    "        yield torch.zeros(1, num_features, device=device, dtype=torch.long)\n",
    "        while True:\n",
    "            num_selected_features = cast(\n",
    "                Categorical, kwargs[\"num_select_distribution\"]\n",
    "            ).sample()\n",
    "            rand_vals = torch.randn(1, num_features)\n",
    "            threshold = torch.kthvalue(\n",
    "                rand_vals, num_features - num_selected_features\n",
    "            ).values.item()\n",
    "            yield (rand_vals > threshold).to(device=device).long()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75dd2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an utility for annoying nnunetv2 preprocessing\n",
    "def nnunetv2_default_preprocessing(ct_img_path, \n",
    "                                   predictor, \n",
    "                                   dataset_json_path,\n",
    "                                   other_volumes: Union[np.ndarray, None] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses the CT image and other volumes using nnunetv2's default preprocessing\n",
    "    pipeline. This function reads the CT image, applies the preprocessor, and returns\n",
    "    the preprocessed image.\n",
    "    \"\"\"\n",
    "    plans_manager = predictor.plans_manager\n",
    "    configuration_manager = predictor.configuration_manager\n",
    "    \n",
    "    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n",
    "    rw = plans_manager.image_reader_writer_class()\n",
    "    if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "        rw = rw()\n",
    "    img_np, img_props = rw.read_images([str(ct_img_path)])\n",
    "    \n",
    "    preprocessed, other_volumes_preprocessed, _ = preprocessor.run_case_npy(\n",
    "        img_np, seg=other_volumes, properties=img_props,\n",
    "        plans_manager=plans_manager,\n",
    "        configuration_manager=configuration_manager,\n",
    "        dataset_json=dataset_json_path\n",
    "    )\n",
    "    if other_volumes:\n",
    "        return preprocessed, other_volumes_preprocessed\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe99033",
   "metadata": {},
   "source": [
    "# Next step: define regular, fixed size superpixels and try to compute the attributions of each of them\n",
    "So we also need to define a metric to compare, since segmentation explanations, differently from classification, is intrinsically ambiguous. For example, let's select a priori a single region of the segmentation output, and use the average of these pixels to compute the impact of perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5cf9e",
   "metadata": {},
   "source": [
    "### 4. Face-centered cubic (FCC) lattice induced supervoxel assignment\n",
    "-> more *isotropic* than simple cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99105ae",
   "metadata": {},
   "source": [
    "### 4.1 affine transformation to translate isotropy from voxel space into the original geometrical space (measured in mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0cf54",
   "metadata": {},
   "source": [
    "### 4.2 Try to apply the original algorithm FCC it to an affine transformed volume that has the same proportion as the .nii in the physical space. Transform->apply the algorithm to derive the map-> back transform the map onto the voxel space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def generate_FCC_supervoxel_map(img, S=200.0):\n",
    "    \"\"\"\n",
    "    Generate a supervoxel map using FCC tessellation in physical space (original version).\n",
    "    \n",
    "    Args:\n",
    "        img: Nibabel NIfTI image object\n",
    "        S (float): Desired supervoxel size in millimeters (default: 200.0)\n",
    "    \n",
    "    Returns:\n",
    "        supervoxel_map: 3D NumPy array with integer labels for supervoxels\n",
    "    \"\"\"\n",
    "    # Load volume and affine from image\n",
    "    volume = img.get_fdata()\n",
    "    affine = img.affine\n",
    "    W, H, D = volume.shape\n",
    "\n",
    "    # Compute the physical bounding box of the volume\n",
    "    corners_voxel = np.array([\n",
    "        [0, 0, 0],\n",
    "        [W-1, 0, 0],\n",
    "        [0, H-1, 0],\n",
    "        [0, 0, D-1],\n",
    "        [W-1, H-1, 0],\n",
    "        [W-1, 0, D-1],\n",
    "        [0, H-1, D-1],\n",
    "        [W-1, H-1, D-1]\n",
    "    ])\n",
    "    corners_hom = np.hstack((corners_voxel, np.ones((8, 1))))\n",
    "    corners_physical = (affine @ corners_hom.T).T[:, :3]\n",
    "    min_xyz = corners_physical.min(axis=0)\n",
    "    max_xyz = corners_physical.max(axis=0)\n",
    "\n",
    "    # Generate FCC lattice centers in physical space\n",
    "    a = S * np.sqrt(2)\n",
    "    factor = 2 / a\n",
    "    p_min = int(np.floor(factor * min_xyz[0])) - 1\n",
    "    p_max = int(np.ceil(factor * max_xyz[0])) + 1\n",
    "    q_min = int(np.floor(factor * min_xyz[1])) - 1\n",
    "    q_max = int(np.ceil(factor * max_xyz[1])) + 1\n",
    "    r_min = int(np.floor(factor * min_xyz[2])) - 1\n",
    "    r_max = int(np.ceil(factor * max_xyz[2])) + 1\n",
    "\n",
    "    # Create grid of possible indices\n",
    "    p_vals = np.arange(p_min, p_max + 1)\n",
    "    q_vals = np.arange(q_min, q_max + 1)\n",
    "    r_vals = np.arange(r_min, r_max + 1)\n",
    "    P, Q, R = np.meshgrid(p_vals, q_vals, r_vals, indexing='ij')\n",
    "    P = P.flatten()\n",
    "    Q = Q.flatten()\n",
    "    R = R.flatten()\n",
    "\n",
    "    # Filter for FCC lattice points (sum of indices is even)\n",
    "    mask = (P + Q + R) % 2 == 0\n",
    "    P = P[mask]\n",
    "    Q = Q[mask]\n",
    "    R = R[mask]\n",
    "\n",
    "    # Compute physical coordinates of centers\n",
    "    centers = np.column_stack((P * a / 2, Q * a / 2, R * a / 2))\n",
    "\n",
    "    # Keep only centers within the bounding box\n",
    "    inside = ((centers[:, 0] >= min_xyz[0]) & (centers[:, 0] <= max_xyz[0]) &\n",
    "              (centers[:, 1] >= min_xyz[1]) & (centers[:, 1] <= max_xyz[1]) &\n",
    "              (centers[:, 2] >= min_xyz[2]) & (centers[:, 2] <= max_xyz[2]))\n",
    "    centers = centers[inside]\n",
    "\n",
    "    # Check if any centers were generated\n",
    "    print(f\"Number of supervoxel centers: {len(centers)}\")\n",
    "    if len(centers) == 0:\n",
    "        raise ValueError(\"No supervoxel centers generated. Try reducing S.\")\n",
    "\n",
    "    # Generate voxel indices and transform to physical coordinates\n",
    "    voxel_indices = np.indices((W, H, D)).reshape(3, -1).T  # shape (W*H*D, 3)\n",
    "    voxel_indices_hom = np.hstack((voxel_indices, np.ones((voxel_indices.shape[0], 1))))  # shape (W*H*D, 4)\n",
    "    physical_coords = (affine @ voxel_indices_hom.T).T[:, :3]  # shape (W*H*D, 3)\n",
    "\n",
    "    # Assign each voxel to the nearest supervoxel center\n",
    "    tree = cKDTree(centers)\n",
    "    _, labels = tree.query(physical_coords)\n",
    "\n",
    "    # Create the supervoxel map\n",
    "    supervoxel_map = labels.reshape((W, H, D)).astype(np.int32)\n",
    "\n",
    "    return supervoxel_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c80b34b",
   "metadata": {},
   "source": [
    "## Try SLIC for visual context aware supervoxels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c40f92",
   "metadata": {},
   "source": [
    "### define a preprocessing routine to enhance SLIC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "203adfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.exposure import equalize_adapthist\n",
    "\n",
    "def preprocessing_for_SLIC(data: np.ndarray):\n",
    "    # 2ï¸âƒ£ Clip extreme intensities (e.g. 0.5%â€“99.5% quantiles) for contrast enhancement\n",
    "    vmin, vmax = np.quantile(data, (0.005, 0.995))\n",
    "    data = np.clip(data, vmin, vmax)\n",
    "    data = exposure.rescale_intensity(data, in_range=(vmin, vmax), out_range=(0, 1))  #  [oai_citation:0â€¡scikit-image.org](https://scikit-image.org/docs/0.25.x/api/skimage.segmentation.html?utm_source=chatgpt.com) [oai_citation:1â€¡researchgate.net](https://www.researchgate.net/publication/330691413_A_novel_technique_for_analysing_histogram_equalized_medical_images_using_superpixels?utm_source=chatgpt.com) [oai_citation:2â€¡arxiv.org](https://arxiv.org/abs/2204.05278?utm_source=chatgpt.com) [oai_citation:3â€¡scikit-image.org](https://scikit-image.org/skimage-tutorials/lectures/three_dimensional_image_processing.html?utm_source=chatgpt.com)\n",
    "\n",
    "    sigma_vox = np.array([1.0 / s for s in spacing])  # blur by 1 mm across axes\n",
    "    data = gaussian_filter(data, sigma=sigma_vox)\n",
    "\n",
    "    data = exposure.equalize_hist(data)\n",
    "\n",
    "    # Apply slice-wise CLAHE for 3D volume\n",
    "    data = np.stack([equalize_adapthist(slice_, clip_limit=0.03)\n",
    "                       for slice_ in data], axis=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65dba8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f953b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri SLIC: n_segments definisce quanti supervoxels circa si voglion\n",
    "apply_SLIC = lambda data, spacing, n_supervoxels: slic(\n",
    "                preprocessing_for_SLIC(data), \n",
    "                n_segments=n_supervoxels, \n",
    "                compactness=0.2,\n",
    "                spacing=spacing,\n",
    "                start_label=0,\n",
    "                max_num_iter=10, \n",
    "                channel_axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c5189",
   "metadata": {},
   "source": [
    "### Initialize predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43579e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n",
      "  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n"
     ]
    }
   ],
   "source": [
    "# 2) Initialise predictor ------------------------\n",
    "predictor = CustomNNUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=False, # == test time augmentation\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda', 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=False #it interfere with SHAP loading bar\n",
    ")\n",
    "# initializes the network architecture, loads the checkpoint\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    model_dir,\n",
    "    use_folds=(0,),\n",
    "    checkpoint_name='checkpoint_final.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea278ef9",
   "metadata": {},
   "source": [
    "### some utils for Nifti geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ee2218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT image spacing: (1.171875, 1.171875, 5.0)\n",
      "CT image origin: [  300.           185.30000305 -1434.19995117]\n"
     ]
    }
   ],
   "source": [
    "def get_spacing(ct_img_data: nib.nifti1.Nifti1Image):\n",
    "    \"\"\"affine = ct_img_data.affine\n",
    "    spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))\"\"\"\n",
    "    # this is equivalent to\n",
    "    spacing = ct_img_data.header.get_zooms() # get_zooms returns (x, y, z) spacing\n",
    "    return spacing\n",
    "\n",
    "def get_origin(ct_img_data: nib.nifti1.Nifti1Image):\n",
    "    affine = ct_img_data.affine\n",
    "    origin = affine[:3, 3]\n",
    "    return origin\n",
    "\n",
    "\n",
    "# test\n",
    "ct_img = nib.load(ct_img_paths[volume_codes[1]])\n",
    "ct_data = ct_img.get_fdata()\n",
    "spacing = get_spacing(ct_img)\n",
    "origin = get_origin(ct_img)\n",
    "print(\"CT image spacing:\", spacing)\n",
    "print(\"CT image origin:\", origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089768e",
   "metadata": {},
   "source": [
    "# Define a ROI to explain segmentation in. \n",
    "Maybe this will provide a more useful attribution map, highlighting nearby organs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a0a43ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# if true, we will use a json file containing the bounding box for the Region of Interest (ROI), otherwise we will use a masked segmentation\\nROI_TYPE = \"BoundingBox\"  # \"BoundingBox\" or \"MaskedSegmentation'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# if true, we will use a json file containing the bounding box for the Region of Interest (ROI), otherwise we will use a masked segmentation\n",
    "ROI_TYPE = \"BoundingBox\"  # \"BoundingBox\" or \"MaskedSegmentation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f927bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BB_ROI = json.load(open(BB_ROI_paths[volume_codes[1]], \"r\")) if ROI_TYPE == \"BoundingBox\" else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c67e3b",
   "metadata": {},
   "source": [
    "### ROI mask is a binary mask highlighting the lymphnodes of interest. We need a bounding box to crop the volume accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b53e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def get_mask_bbox_slices(mask_nii_path):\n",
    "    \"\"\"\n",
    "    Load a binary ROI mask NIfTI and compute the minimal 3D bounding\n",
    "    box slices containing all positive voxels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask_nii_path : str or Path\n",
    "        Path to the input binary ROI mask NIfTI (.nii or .nii.gz).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3-tuple of Python slice objects (x_slice, y_slice, z_slice)\n",
    "        defining the minimal bounding box.\n",
    "    \"\"\"\n",
    "    # 1) Load mask\n",
    "    nii = nib.load(str(mask_nii_path))\n",
    "    data = nii.get_fdata()\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input NIfTI must be a 3D volume\")\n",
    "    \n",
    "    # 2) Find indices of positive voxels\n",
    "    pos_voxels = np.argwhere(data > 0)\n",
    "    if pos_voxels.size == 0:\n",
    "        raise ValueError(\"No positive voxels found in mask\")\n",
    "    \n",
    "    # 3) Compute min/max per axis\n",
    "    x_min, y_min, z_min = pos_voxels.min(axis=0)\n",
    "    x_max, y_max, z_max = pos_voxels.max(axis=0)\n",
    "    \n",
    "    # 4) Build slice objects (end is exclusive, hence +1)\n",
    "    bbox_slices = (\n",
    "        slice(int(x_min), int(x_max) + 1),\n",
    "        slice(int(y_min), int(y_max) + 1),\n",
    "        slice(int(z_min), int(z_max) + 1),\n",
    "    )\n",
    "    \n",
    "    return bbox_slices\n",
    "\n",
    "def get_slices_from_BB_ROI(BB_ROI: dict) -> Tuple[slice, slice, slice]:\n",
    "    \"\"\"\n",
    "    Extracts the bounding box slices from the BB_ROI dictionary.\n",
    "    Parameters\n",
    "    ----------\n",
    "    BB_ROI : dict\n",
    "        Dictionary containing the bounding box coordinates with keys:\n",
    "        example:\n",
    "        {'FileFormat': array('MITK ROI', dtype='<U8'), 'Version': array(2), 'Geometry': array({'Size': [512.0, 512.0, 221.0], 'Transform': [1.3671875, -0.0, -0.0, 0, -0.0, 1.3671875, -0.0, 0, 0.0, 0.0, 5.0, 0, -350.0, -278.6000061035156, -432.239990234375, 1]},\n",
    "      dtype=object), 'ROIs': array([{'ID': 0, 'Max': [370.5, 291.5, 114.5], 'Min': [276.5, 193.49999999999994, 92.5], 'Properties': {'ColorProperty': {'color': [1.0, 0.0, 0.0]}, 'StringProperty': {'name': 'AUTOMI_00004_0000 Bounding Box'}}}],\n",
    "      dtype=object)}\n",
    "    \"\"\"\n",
    "    # take the ceil of both min and max (ROI in MITK apprently is shifted by 0.5 voxels)\n",
    "    x_slice = slice(int(np.ceil(BB_ROI['ROIs'][0]['Min'][0])), int(np.ceil(BB_ROI['ROIs'][0]['Max'][0])))\n",
    "    y_slice = slice(int(np.ceil(BB_ROI['ROIs'][0]['Min'][1])), int(np.ceil(BB_ROI['ROIs'][0]['Max'][1])))\n",
    "    z_slice = slice(int(np.ceil(BB_ROI['ROIs'][0]['Min'][2])), int(np.ceil(BB_ROI['ROIs'][0]['Max'][2])))\n",
    "    return x_slice, y_slice, z_slice\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "def load_roi_slices(\n",
    "    ROI_type: str,\n",
    "    ROI_BB_path: Optional[str] = None,\n",
    "    ROI_segmentation_mask_path: Optional[str] = None\n",
    ") -> Tuple[slice, slice, slice]:\n",
    "    \"\"\"\n",
    "    Load ROI definition and return bounding-box slices in x, y, z order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ROI_type : {\"BoundingBox\", \"MaskedSegmentation\"}\n",
    "        Type of ROI to load.\n",
    "    ROI_BB_path : str, optional\n",
    "        Path to JSON file defining the bounding box ROI.\n",
    "    ROI_segmentation_mask_path : str, optional\n",
    "        Path to NIfTI file defining the ROI segmentation mask.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_slice, y_slice, z_slice : slice\n",
    "        Slices along each axis for cropping operations.\n",
    "    \"\"\"\n",
    "    if ROI_type == \"BoundingBox\":\n",
    "        if ROI_BB_path is None:\n",
    "            raise ValueError(\"ROI_BB_path must be provided when ROI_type is 'BoundingBox'.\")\n",
    "        BB_ROI = json.load(open(ROI_BB_path, \"r\"))\n",
    "        x_slice, y_slice, z_slice = get_slices_from_BB_ROI(BB_ROI)\n",
    "\n",
    "    elif ROI_type == \"MaskedSegmentation\":\n",
    "        if ROI_segmentation_mask_path is None:\n",
    "            raise ValueError(\"ROI_segmentation_mask_path must be provided when ROI_type is 'MaskedSegmentation'.\")\n",
    "        # Optionally inspect mask details if needed:\n",
    "        mask_img = nib.load(ROI_segmentation_mask_path)\n",
    "        print(f\"Loaded mask shape: {mask_img.shape}, affine: {mask_img.affine}\")\n",
    "        x_slice, y_slice, z_slice = get_mask_bbox_slices(ROI_segmentation_mask_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ROI_type: {ROI_type}\")\n",
    "\n",
    "    return x_slice, y_slice, z_slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bde5f",
   "metadata": {},
   "source": [
    "### the identified region is our ROI bounding box in case we use the masked segmentation as manually derived ROI\n",
    "### otherwise we just use the ROI bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55c5b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def slices_to_binary_mask(volume_shape, bbox_slices, dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Create a binary mask of given shape where voxels inside the provided\n",
    "    3D boundingâ€box slices are set to 1, and all others to 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume_shape : tuple of int\n",
    "        The full 3D volume dimensions, e.g. (X, Y, Z).\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3â€tuple of slice objects (x_slice, y_slice, z_slice) defining\n",
    "        the region to mask.\n",
    "    dtype : dataâ€type, optional\n",
    "        The desired dataâ€type of the output mask (default: np.uint8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : np.ndarray\n",
    "        A binary mask array of shape `volume_shape`, with ones in the\n",
    "        region defined by `bbox_slices` and zeros elsewhere.\n",
    "    \"\"\"\n",
    "    if len(volume_shape) != len(bbox_slices):\n",
    "        raise ValueError(f\"volume_shape has {len(volume_shape)} dimensions, \"\n",
    "                         f\"but bbox_slices has {len(bbox_slices)} slices\")\n",
    "\n",
    "    # Initialize mask to zeros\n",
    "    mask = np.zeros(volume_shape, dtype=dtype)\n",
    "    # Set the boundingâ€box region to 1\n",
    "    mask[bbox_slices] = 1\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef654af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROI_mask_path = \"ROI_binary_mask.nii.gz\"\\npath = ct_img_paths[volume_codes[1]]\\nimage = nib.load(path)\\nshape = image.get_fdata().shape\\naffine = image.affine\\nprint(\"shape\", shape)\\nbbox_slices = (x_slice, y_slice, z_slice)\\nprint(\"bbox_slices:\", bbox_slices)\\nROI_mask = slices_to_binary_mask(\\n    volume_shape=shape,\\n    bbox_slices=bbox_slices,\\n)\\nROI_mask_nii = nib.Nifti1Image(ROI_mask, affine)\\n# Save the binary mask as a NIfTI file\\nnib.save(\\n        ROI_mask_nii,\\n        ROI_mask_path\\n        )\\n\\nprint(f\"Saved binary ROI mask to {ROI_mask_path}\")\\nprint(\"ROI mask shape:\", ROI_mask.shape)\\nprint(\"ROI mask unique values:\", np.unique(ROI_mask))'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_roi_mask(\n",
    "    volume_path: str,\n",
    "    bbox_slices: Tuple[slice, slice, slice],\n",
    "    output_path: Optional[str] = \"ROI_binary_mask.nii.gz\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a binary ROI mask based on bounding-box slices and save as NIfTI.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume_path : str\n",
    "        Path to the input volume NIfTI file.\n",
    "    bbox_slices : tuple of slice\n",
    "        Slices (x_slice, y_slice, z_slice) defining the ROI bounding box.\n",
    "    output_path : str, optional\n",
    "        Path where the binary mask NIfTI will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_path : str\n",
    "        Path to the saved binary mask file.\n",
    "    \"\"\"\n",
    "    img = nib.load(volume_path)\n",
    "    volume_shape = img.get_fdata().shape\n",
    "    affine = img.affine\n",
    "\n",
    "    # Generate binary mask array\n",
    "    mask_array = slices_to_binary_mask(\n",
    "        volume_shape=volume_shape,\n",
    "        bbox_slices=bbox_slices\n",
    "    )\n",
    "\n",
    "    # Create and save NIfTI mask\n",
    "    mask_nii = nib.Nifti1Image(mask_array.astype(np.uint8), affine)\n",
    "    nib.save(mask_nii, output_path)\n",
    "    print(f\"Saved binary ROI mask to {output_path}\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d507c7c",
   "metadata": {},
   "source": [
    "### to **crop** correctly the volume around the *ROI*, we need to derive the **receptive field** of the sliding window inference, that depends on the *patch size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c65d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patch_size = np.array(predictor.configuration_manager.patch_size)\\nprint(\"Patch size: \", patch_size)\\n\\n# Receptive field is twice the patch size-1\\nRF = 2*(patch_size-1)\\nprint(\"Receptive field: \", RF)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"patch_size = np.array(predictor.configuration_manager.patch_size)\n",
    "print(\"Patch size: \", patch_size)\n",
    "\n",
    "# Receptive field is twice the patch size-1\n",
    "RF = 2*(patch_size-1)\n",
    "print(\"Receptive field: \", RF)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee54246",
   "metadata": {},
   "source": [
    "### Consider the receptive field to compute the final slices for cropping\n",
    "Remember that model metadata are related to transposed volume (nnunetv2 takes (D, H, W) shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d2ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'volume_path = ct_img_paths[volume_codes[1]]\\nvolume_shape = nib.load(volume_path).shape # (W, H, D) -> x, y, z\\n# RF shape -> (D, H, W) -> z, y, x (model input shape)\\nprint(\"Original volume shape:\", volume_shape)\\n\\nW, H, D = volume_shape\\n\\n# backward sorted receptive field axes\\nRF_x, RF_y, RF_z = RF[2],RF[1],RF[0]\\n\\nx_slice_RF = slice(int(max(x_slice.start - RF_x/2, 0)), int(min(x_slice.stop + RF_x/2, W)))\\ny_slice_RF = slice(int(max(y_slice.start - RF_y/2, 0)), int(min(y_slice.stop + RF_y/2, H)))\\nz_slice_RF = slice(int(max(z_slice.start - RF_z/2, 0)), int(min(z_slice.stop + RF_z/2, D)))\\n\\nprint(\"new  x:\", x_slice_RF)\\nprint(\"new  y:\", y_slice_RF)\\nprint(\"new  z:\", z_slice_RF)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_rf_slices(\n",
    "    bbox_slices: Tuple[slice, slice, slice],\n",
    "    patch_size: np.ndarray,\n",
    "    volume_shape: Tuple[int, int, int]\n",
    ") -> Tuple[slice, slice, slice]:\n",
    "    \"\"\"\n",
    "    Compute bounding-box slices expanded by the model's receptive field.\n",
    "\n",
    "    RF is defined as 2*(patch_size - 1), and applied symmetrically.\n",
    "    \"\"\"\n",
    "    x_slice, y_slice, z_slice = bbox_slices\n",
    "    W, H, D = volume_shape\n",
    "\n",
    "    # receptive field along each axis (model input axes reversed)\n",
    "    RF = 2 * (patch_size - 1)\n",
    "    RF_z, RF_y, RF_x = RF  # expect patch_size as [D,H,W]\n",
    "\n",
    "    x_start = max(x_slice.start - RF_x // 2, 0)\n",
    "    x_stop = min(x_slice.stop + RF_x // 2, W)\n",
    "    y_start = max(y_slice.start - RF_y // 2, 0)\n",
    "    y_stop = min(y_slice.stop + RF_y // 2, H)\n",
    "    z_start = max(z_slice.start - RF_z // 2, 0)\n",
    "    z_stop = min(z_slice.stop + RF_z // 2, D)\n",
    "\n",
    "    return (slice(x_start, x_stop), slice(y_start, y_stop), slice(z_start, z_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdaa31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def crop_volume_and_affine(nii_path, bbox_slices, save_cropped_nii_path=None):\n",
    "    \"\"\"\n",
    "    Crop a 3D NIfTI volume using the given bounding-box slices and\n",
    "    recompute the affine so the cropped volume retains correct world coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nii_path : str or Path\n",
    "        Path to the input NIfTI volume (.nii or .nii.gz).\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3-tuple (x_slice, y_slice, z_slice) as returned by get_mask_bbox_slices().\n",
    "    save_cropped_nii_path : str or Path, optional\n",
    "        If provided, the cropped volume will be saved here as a new NIfTI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_data : np.ndarray\n",
    "        The volume data cropped to the bounding box.\n",
    "    new_affine : np.ndarray\n",
    "        The updated 4Ã—4 affine transform for the cropped volume.\n",
    "    \"\"\"\n",
    "    # 1) Load the original image\n",
    "    img = nib.load(str(nii_path))\n",
    "    data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "\n",
    "    # 2) Crop the data array\n",
    "    cropped_data = data[bbox_slices]\n",
    "\n",
    "    # 3) Extract the voxelâ€offsets for x, y, z from the slice starts\n",
    "    x_slice, y_slice, z_slice = bbox_slices\n",
    "    z0, y0, x0 = z_slice.start, y_slice.start, x_slice.start\n",
    "\n",
    "    # 4) Compute the new affine translation: shift the origin by the voxel offsets\n",
    "    # Note voxel coordinates are (i, j, k) = (x, y, z)\n",
    "    offset_vox = np.array([x0, y0, z0])\n",
    "    new_affine = affine.copy()\n",
    "    new_affine[:3, 3] += affine[:3, :3].dot(offset_vox)\n",
    "\n",
    "    # 5) Optionally save the cropped volume\n",
    "    if save_cropped_nii_path is not None:\n",
    "        cropped_img = nib.Nifti1Image(cropped_data, new_affine)\n",
    "        nib.save(cropped_img, str(save_cropped_nii_path))\n",
    "\n",
    "    return cropped_data, new_affine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc8614",
   "metadata": {},
   "source": [
    "### We need a way to check original mask overlapping in the new cropped volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90728d9e",
   "metadata": {},
   "source": [
    "### We also need a mask to correctly ignoring out-of ROI context in our aggregation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb83d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from pathlib import Path\\n\\nslices = (x_slice_RF, y_slice_RF, z_slice_RF)\\n\\nnii_path = ct_img_paths[volume_codes[1]]\\n\\ncropped_volume_path = \"cropped_volume.nii.gz\"\\ncropped_volume, affine_cropped_volume = crop_volume_and_affine(\\n    nii_path=nii_path,\\n    bbox_slices=slices,\\n    save_cropped_nii_path=Path(cropped_volume_path)\\n)\\n\\nprint(\"Cropped data shape:\", cropped_volume.shape)\\nprint(\"New affine:\\n\", affine_cropped_volume)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "def crop_volume_with_rf(\n",
    "    volume_path: str,\n",
    "    bbox_slices: Tuple[slice, slice, slice],\n",
    "    patch_size: np.ndarray,\n",
    "    output_dir: Optional[str] = \".\"\n",
    ") -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Crop the volume and binary ROI mask to the ROI padded by receptive field.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "      - 'cropped_volume'\n",
    "      - 'affine_cropped_volume'\n",
    "      - 'cropped_roi_mask'\n",
    "      - 'affine_cropped_mask'\n",
    "    \"\"\"\n",
    "    volume_img = nib.load(volume_path)\n",
    "    volume_shape = volume_img.get_fdata().shape\n",
    "\n",
    "    # compute RF-expanded slices\n",
    "    padded_slices = compute_rf_slices(bbox_slices, patch_size, volume_shape)\n",
    "\n",
    "    # ensure output directory exists\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # crop volume\n",
    "    cropped_vol_path = out_dir / \"cropped_volume.nii.gz\"\n",
    "    cropped_volume, affine_cropped_volume = crop_volume_and_affine(\n",
    "        nii_path=volume_path,\n",
    "        bbox_slices=padded_slices,\n",
    "        save_cropped_nii_path=cropped_vol_path\n",
    "    )\n",
    "\n",
    "    # crop ROI mask\n",
    "    roi_mask_path = out_dir / \"ROI_binary_mask.nii.gz\"\n",
    "    cropped_mask, affine_cropped_mask = crop_volume_and_affine(\n",
    "        nii_path=str(roi_mask_path),\n",
    "        bbox_slices=padded_slices,\n",
    "        save_cropped_nii_path=out_dir / \"cropped_mask_with_RF.nii.gz\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"cropped_volume\": (cropped_volume, affine_cropped_volume),\n",
    "        \"cropped_roi_mask\": (cropped_mask, affine_cropped_mask)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5bdb8",
   "metadata": {},
   "source": [
    "## We execute SHAP on this cropped image, and we only consider our ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05a1ca",
   "metadata": {},
   "source": [
    "### set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7fbaabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6e74e",
   "metadata": {},
   "source": [
    "### Preprocessing (skipped for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459dce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NNUNET_PREPROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# (a) load + cropped volume  (1, C, D, H, W) â€“ nnU-Net order\\nnii_path_cropped = \"cropped_volume.nii.gz\"\\ndataset_json_path = Path(model_dir) / \"dataset.json\"\\n\\nif NNUNET_PREPROCESSING:\\n    volume_np = nnunetv2_default_preprocessing(nii_path_cropped, predictor, dataset_json_path)\\n    # debug: save nifti\\n    volume_nii = nib.Nifti1Image(volume_np.squeeze().transpose(2,1,0), affine_cropped_volume)\\n    nib.save(volume_nii, \"preprocessed_volume.nii.gz\")\\nelse:\\n    # load the cropped volume and convert to numpy array\\n    cropped_volume_nii = nib.load(nii_path_cropped)\\n    volume_np = cropped_volume_nii.get_fdata()\\n    # convert to nnU-Net order (D, H, W)\\n    volume_np = np.transpose(volume_np, (2, 1, 0))\\n\\n    \\n    # add channel dimension (C=1)\\n    volume_np = np.expand_dims(volume_np, axis=0)  # shape (C, D, H, W)\\n\\n# (b) convert to torch tensor and add batch dimension\\nvolume = torch.from_numpy(volume_np.astype(np.float32)).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\\n\\nprint(\"Volume shape:\", volume.shape)             # (1, C, D, H, W)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_volume(\n",
    "    cropped_volume_path: str,\n",
    "    predictor,\n",
    "    dataset_json_path: str,\n",
    "    use_nnunet: bool = True\n",
    ") -> Tuple[np.ndarray, nib.Nifti1Image]:\n",
    "    \"\"\"\n",
    "    Preprocess the cropped volume for nnU-Net inference.\n",
    "\n",
    "    If `use_nnunet` is True, applies nnU-Net's default preprocessing.\n",
    "    Otherwise, loads the volume directly and reformats axes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    volume_np : np.ndarray\n",
    "        Array of shape (1, C, D, H, W) ready for torch conversion.\n",
    "    saved_volume_nii : nib.Nifti1Image\n",
    "        NIfTI image object of the preprocessed volume (for debugging or saving).\n",
    "    \"\"\"\n",
    "    if use_nnunet:\n",
    "        # nnU-Net preprocessing utility, assumed imported\n",
    "        volume_np = nnunetv2_default_preprocessing(\n",
    "            cropped_volume_path,\n",
    "            predictor,\n",
    "            dataset_json_path\n",
    "        )  # expects (C, D, H, W)\n",
    "        # Save NIfTI for debugging\n",
    "        volume_nii = nib.Nifti1Image(\n",
    "            volume_np.squeeze().transpose(2, 1, 0),  # back to (W,H,D)\n",
    "            nib.load(cropped_volume_path).affine\n",
    "        )\n",
    "        nib.save(volume_nii, Path(cropped_volume_path).replace(suffix=\"_preproc.nii.gz\"))\n",
    "    else:\n",
    "        cropped = nib.load(cropped_volume_path)\n",
    "        data = cropped.get_fdata()\n",
    "        data = np.transpose(data, (2, 1, 0))  # (D,H,W)\n",
    "        data = np.expand_dims(data, axis=0)   # (1, D,H,W)\n",
    "        volume_np = data\n",
    "        volume_nii = None\n",
    "\n",
    "    return volume_np, volume_nii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f894e7d",
   "metadata": {},
   "source": [
    "### Supervoxels subdivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90db2a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUPERVOXEL_TYPE = \"FCC\"\\nUSE_SAVED_MAP = False'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"SUPERVOXEL_TYPE = \"FCC\"\n",
    "USE_SAVED_MAP = False\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f194fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# (b) super-voxel / organ-id map  (W, H, D)\\n# Load the image\\nimg = nib.load(nii_path_cropped)\\n\\nif SUPERVOXEL_TYPE == \"FCC\":\\n    supervoxel_map_path = \\'FCC-supervoxel_map.nii.gz\\'\\n    if USE_SAVED_MAP and os.path.exists(supervoxel_map_path):\\n        supervoxel_map = nib.load(supervoxel_map_path).get_fdata()\\n    else:\\n        # Generate and save supervoxel map using the FCC\\n        cube_side = 100.00 # [mm]\\n        supervoxel_map = generate_supervoxel_map(img, S=cube_side) # (W, H, D)\\n        # Save the result\\n        supervoxel_map_img = nib.Nifti1Image(supervoxel_map.astype(np.int32), affine=img.affine)\\n        nib.save(supervoxel_map_img, supervoxel_map_path)\\n        \\n\\nelif SUPERVOXEL_TYPE == \"SLIC\":\\n    supervoxel_map_path = \\'SLIC_supervoxel_map.nii.gz\\'\\n    if USE_SAVED_MAP and os.path.exists(supervoxel_map_path):\\n        supervoxel_map = nib.load(supervoxel_map_path).get_fdata()\\n    else:\\n        # compute spacing for SLIC\\n        data = img.get_fdata()\\n        affine = img.affine\\n        data = np.array(data, dtype=np.float32)\\n        \\n        # derive slic spacing from affine\\n        spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))\\n        spacing = tuple(spacing)  # convert to tuple for slic\\n        \\n        # Generate and save supervoxel map using SLIC\\n        n_supervoxels = 380\\n        supervoxel_map = apply_SLIC(data, spacing, n_supervoxels)\\n        \\n        # Save the result\\n        supervoxel_map_img = nib.Nifti1Image(supervoxel_map.astype(np.int32), affine=affine)\\n        nib.save(supervoxel_map_img, \\'SLIC_supervoxel_map.nii.gz\\')\\n        \\nelse:\\n    raise ValueError()\\n\\nprint(\"Mappa supervoxel shape:\", supervoxel_map.shape)\\nn_supervoxels = len(np.unique(supervoxel_map))\\nprint(\"Numero di supervoxels:\", n_supervoxels)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_supervoxel_map(\n",
    "    cropped_volume_path: str,\n",
    "    supervoxel_type: str = \"FCC\",\n",
    "    fcc_cube_side: float = 100.0,\n",
    "    slic_n_supervoxels: int = 380\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a supervoxel map for the cropped volume.\n",
    "    Returns an array of shape (D, H, W) with consecutive integer labels.\n",
    "    \"\"\"\n",
    "    img = nib.load(cropped_volume_path)\n",
    "    data = img.get_fdata().astype(np.float32)\n",
    "    affine = img.affine\n",
    "\n",
    "    if supervoxel_type == \"FCC\":\n",
    "        sv_map = generate_FCC_supervoxel_map(img, S=fcc_cube_side)\n",
    "    elif supervoxel_type == \"SLIC\":\n",
    "        spacing = tuple(np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0)))\n",
    "        sv_map = apply_SLIC(data, spacing, slic_n_supervoxels)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported supervoxel_type: {supervoxel_type}\")\n",
    "\n",
    "    # reorder axes (W,H,D) -> (D,H,W)\n",
    "    sv_map = np.transpose(sv_map, (2, 1, 0))\n",
    "    # remap labels to consecutive ints\n",
    "    unique_vals, inverse = np.unique(sv_map, return_inverse=True)\n",
    "    sv_map = inverse.reshape(sv_map.shape)\n",
    "    return sv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c35fa166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\\n# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\\nsv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\\n\\nsupervoxel_map = indexes.reshape(supervoxel_map.shape)\\nprint(\"number of supervoxels: \", np.unique(supervoxel_map).size)\\n\\n# IMPORTANT ðŸ”¸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\\nsupervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\\n\\nprint(\"Mask shape:\", supervoxel_map.shape)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\n",
    "# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\n",
    "sv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\n",
    "\n",
    "supervoxel_map = indexes.reshape(supervoxel_map.shape)\n",
    "print(\"number of supervoxels: \", np.unique(supervoxel_map).size)\n",
    "\n",
    "# IMPORTANT ðŸ”¸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\n",
    "supervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\n",
    "\n",
    "print(\"Mask shape:\", supervoxel_map.shape)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec773cb9",
   "metadata": {},
   "source": [
    "### in case we still don't have it, obtain cropped binary segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c23d1e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if ROI_TYPE == \"BoundingBox\":\\n    cropped_logits = predictor.predict_sliding_window_return_logits(volume[0])                              # (C, D, H, W)\\n    cropped_segmentation_mask = (torch.argmax(cropped_logits, dim=0) == 1)'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if ROI_TYPE == \"BoundingBox\":\n",
    "    cropped_logits = predictor.predict_sliding_window_return_logits(volume[0])                              # (C, D, H, W)\n",
    "    cropped_segmentation_mask = (torch.argmax(cropped_logits, dim=0) == 1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61673022",
   "metadata": {},
   "source": [
    "### derive baseline cached dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a029518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# this cause the death, try to isolate the problem\n",
    "\n",
    "import math\n",
    "import multiprocessing\n",
    "import shutil\n",
    "from time import sleep\n",
    "from typing import Tuple\n",
    "\n",
    "import SimpleITK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nnunetv2\n",
    "from nnunetv2.paths import nnUNet_preprocessed, nnUNet_raw\n",
    "from nnunetv2.preprocessing.cropping.cropping import crop_to_nonzero\n",
    "from nnunetv2.preprocessing.resampling.default_resampling import compute_new_shape\n",
    "from nnunetv2.training.dataloading.nnunet_dataset import nnUNetDatasetBlosc2\n",
    "from nnunetv2.utilities.dataset_name_id_conversion import maybe_convert_to_dataset_name\n",
    "from nnunetv2.utilities.find_class_by_name import recursive_find_python_class\n",
    "from nnunetv2.utilities.plans_handling.plans_handler import PlansManager, ConfigurationManager\n",
    "from nnunetv2.utilities.utils import get_filenames_of_train_images_and_targets\n",
    "\n",
    "def run_case_npy(preprocessor, data: np.ndarray, seg: Union[np.ndarray, None], properties: dict,\n",
    "                     plans_manager: PlansManager, configuration_manager: ConfigurationManager,\n",
    "                     dataset_json: Union[dict, str]):\n",
    "        # let's not mess up the inputs!\n",
    "        print(1)\n",
    "        data = data.astype(np.float32)  # this creates a copy\n",
    "        if seg is not None:\n",
    "            assert data.shape[1:] == seg.shape[1:], \"Shape mismatch between image and segmentation. Please fix your dataset and make use of the --verify_dataset_integrity flag to ensure everything is correct\"\n",
    "            seg = np.copy(seg)\n",
    "\n",
    "        has_seg = seg is not None\n",
    "\n",
    "        # apply transpose_forward, this also needs to be applied to the spacing!\n",
    "        print(2)\n",
    "        data = data.transpose([0, *[i + 1 for i in plans_manager.transpose_forward]])\n",
    "        print(3)\n",
    "        if seg is not None:\n",
    "            seg = seg.transpose([0, *[i + 1 for i in plans_manager.transpose_forward]])\n",
    "        print(4)\n",
    "        original_spacing = [properties['spacing'][i] for i in plans_manager.transpose_forward]\n",
    "\n",
    "        print(5)\n",
    "        # crop, remember to store size before cropping!\n",
    "        shape_before_cropping = data.shape[1:]\n",
    "        properties['shape_before_cropping'] = shape_before_cropping\n",
    "        # this command will generate a segmentation. This is important because of the nonzero mask which we may need\n",
    "        print(6)\n",
    "        data, seg, bbox = crop_to_nonzero(data, seg)\n",
    "        properties['bbox_used_for_cropping'] = bbox\n",
    "        # print(data.shape, seg.shape)\n",
    "        properties['shape_after_cropping_and_before_resampling'] = data.shape[1:]\n",
    "\n",
    "        # resample\n",
    "        target_spacing = configuration_manager.spacing  # this should already be transposed\n",
    "\n",
    "        if len(target_spacing) < len(data.shape[1:]):\n",
    "            # target spacing for 2d has 2 entries but the data and original_spacing have three because everything is 3d\n",
    "            # in 2d configuration we do not change the spacing between slices\n",
    "            target_spacing = [original_spacing[0]] + target_spacing\n",
    "\n",
    "        print(7)\n",
    "        new_shape = compute_new_shape(data.shape[1:], original_spacing, target_spacing)\n",
    "\n",
    "        # normalize\n",
    "        # normalization MUST happen before resampling or we get huge problems with resampled nonzero masks no\n",
    "        # longer fitting the images perfectly!\n",
    "        print(8)\n",
    "        data = preprocessor._normalize(data, seg, configuration_manager,\n",
    "                               plans_manager.foreground_intensity_properties_per_channel)\n",
    "\n",
    "        # print('current shape', data.shape[1:], 'current_spacing', original_spacing,\n",
    "        #       '\\ntarget shape', new_shape, 'target_spacing', target_spacing)\n",
    "        print(9)\n",
    "        old_shape = data.shape[1:]\n",
    "        data = configuration_manager.resampling_fn_data(data, new_shape, original_spacing, target_spacing)\n",
    "        print(10)\n",
    "        seg = configuration_manager.resampling_fn_seg(seg, new_shape, original_spacing, target_spacing)\n",
    "        if preprocessor.verbose:\n",
    "            print(f'old shape: {old_shape}, new_shape: {new_shape}, old_spacing: {original_spacing}, '\n",
    "                  f'new_spacing: {target_spacing}, fn_data: {configuration_manager.resampling_fn_data}')\n",
    "\n",
    "        # if we have a segmentation, sample foreground locations for oversampling and add those to properties\n",
    "        if has_seg:\n",
    "            # reinstantiating LabelManager for each case is not ideal. We could replace the dataset_json argument\n",
    "            # with a LabelManager Instance in this function because that's all its used for. Dunno what's better.\n",
    "            # LabelManager is pretty light computation-wise.\n",
    "            print(11)\n",
    "            label_manager = plans_manager.get_label_manager(dataset_json)\n",
    "            print(12)\n",
    "            collect_for_this = label_manager.foreground_regions if label_manager.has_regions \\\n",
    "                else label_manager.foreground_labels\n",
    "\n",
    "            # when using the ignore label we want to sample only from annotated regions. Therefore we also need to\n",
    "            # collect samples uniformly from all classes (incl background)\n",
    "            if label_manager.has_ignore_label:\n",
    "                collect_for_this.append([-1] + label_manager.all_labels)\n",
    "\n",
    "            # no need to filter background in regions because it is already filtered in handle_labels\n",
    "            # print(all_labels, regions)\n",
    "            print(13)\n",
    "            properties['class_locations'] = preprocessor._sample_foreground_locations(seg, collect_for_this,\n",
    "                                                                                   verbose=preprocessor.verbose)\n",
    "            print(14)\n",
    "            seg = preprocessor.modify_seg_fn(seg, plans_manager, dataset_json, configuration_manager)\n",
    "        print(15)\n",
    "        if np.max(seg) > 127:\n",
    "            print(16)\n",
    "            seg = seg.astype(np.int16)\n",
    "        else:\n",
    "            print(17)\n",
    "            seg = seg.astype(np.int8)\n",
    "        print(18)\n",
    "        return data, seg, properties\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f96b4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_output_dictionary(volume_file: Path,\n",
    "                                 predictor: CustomNNUNetPredictor,\n",
    "                                 preprocess_before_run: bool = True,\n",
    "                                verbose: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Return a dictionary indexed by the slices for the sliding window, of the output of the inference for each patch\n",
    "        of the original volume\n",
    "        \"\"\"\n",
    "        rw = predictor.plans_manager.image_reader_writer_class()\n",
    "\n",
    "        # If nnU-Net returns a class instead of an instance, instantiate it\n",
    "        if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "            rw = rw()\n",
    "\n",
    "        orig_image, orig_props = rw.read_images(\n",
    "            [str(volume_file)]\n",
    "        )             # (C, Z, Y, X)\n",
    "\n",
    "        if preprocess_before_run:\n",
    "        \n",
    "            preprocessor = predictor.configuration_manager.preprocessor_class()\n",
    "            # the following cause the kernel death at first notebook run\n",
    "            data_pp, _, _ = preprocessor.run_case_npy(\n",
    "                    orig_image,\n",
    "                    seg=None,\n",
    "                    properties=orig_props,\n",
    "                    plans_manager=predictor.plans_manager,\n",
    "                    configuration_manager=predictor.configuration_manager,\n",
    "                    dataset_json=predictor.dataset_json\n",
    "                )\n",
    "        \n",
    "            # to torch, channel-first is already true\n",
    "            inp_tensor = torch.from_numpy(data_pp)\n",
    "        else:\n",
    "            inp_tensor = torch.from_numpy(orig_image)\n",
    "\n",
    "        slicers = predictor._internal_get_sliding_window_slicers(inp_tensor.shape[1:])\n",
    "       \n",
    "        if verbose:\n",
    "            print(\"first 3 slicers of Iterator object: \", slicers[:3])\n",
    "\n",
    "        dictionary = predictor.get_output_dictionary_sliding_window(inp_tensor, slicers)\n",
    "\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87ac185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE_STORED_DICTIONARY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36795cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pickle as pkl\\n\\nif USE_STORED_DICTIONARY and os.path.exists(\"cropped_baseline_output_dictionary_cache.pkl\"):\\n    with open(\"cropped_baseline_output_dictionary_cache.pkl\", \"rb\") as f:\\n        cropped_baseline_pred_cache = pkl.load(f)\\nelse:\\n    cropped_baseline_pred_cache = get_cached_output_dictionary(\\n        volume_file = nii_path_cropped,\\n        predictor = predictor,\\n        preprocess_before_run = NNUNET_PREPROCESSING,\\n        verbose = True,\\n    )\\n    # Write to file\\n    with open(\"cropped_baseline_output_dictionary_cache.pkl\", \"wb\") as f:\\n        pkl.dump(cropped_baseline_pred_cache, f)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_baseline_prediction(\n",
    "    volume_tensor: \"torch.Tensor\",\n",
    "    predictor,\n",
    "    cropped_volume_path: str,\n",
    "    use_nnunet: bool = True\n",
    ") -> Tuple[\"torch.Tensor\", Dict]:\n",
    "    \"\"\"\n",
    "    Compute baseline segmentation mask and cache full prediction outputs.\n",
    "\n",
    "    Returns the segmentation mask tensor and a cache dictionary.\n",
    "    \"\"\"\n",
    "    # Predict logits\n",
    "    logits = predictor.predict_sliding_window_return_logits(volume_tensor[0])\n",
    "    # Convert to binary mask (assuming class 1 is positive)\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)\n",
    "\n",
    "    # Cache prediction outputs for SHAP\n",
    "    cache_dict = get_cached_output_dictionary(\n",
    "        volume_file=cropped_volume_path,\n",
    "        predictor=predictor,\n",
    "        preprocess_before_run=use_nnunet,\n",
    "        verbose=True\n",
    "    )\n",
    "    return seg_mask, cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487311a",
   "metadata": {},
   "source": [
    "### Include masking in the forward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eb94f",
   "metadata": {},
   "source": [
    "### **Chrabaszcz aggregation** Â \n",
    "\n",
    "Let\n",
    "\n",
    "* $z_1^{(i)}(x)$ â€“ class-1 logit at voxel $x$ after perturbation *i*\n",
    "* $P_i(x)=\\mathbf 1\\!\\left[\\arg\\max_c z_c^{(i)}(x)=1\\right]$ â€“ binary mask of voxels currently predicted as lymph-node\n",
    "* no ROI, total volume considered\n",
    "\n",
    "$$\n",
    "S_{\\text{Chr}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x} P_i(x)\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the `scaling_factor`.\n",
    "\n",
    "* **Counts evidence only from voxels the model *currently* labels as class 1.**\n",
    "* *False positives (FP):* contribute **positively** (they are in $P_i$).\n",
    "* *False negatives (FN):* contribute **zero** (their logit is absent).\n",
    "\n",
    "Source: Chrabaszcz et al., *Aggregated Attributions for Explanatory Analysis of 3-D Segmentation Models*, 2024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2b3555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrabaszcz_aggregation(logits: torch.Tensor,\n",
    "                           scaling_factor: float = 1.0,\n",
    "                          ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n",
    "    Explanatory Analysis of 3D Segmentation Models\"\n",
    "    \"\"\"\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)\n",
    "    aggregate = torch.sum(logits[1].double() * seg_mask)\n",
    "\n",
    "    return aggregate / scaling_factor  # normalize to avoid overflows in SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ddf1c",
   "metadata": {},
   "source": [
    "\n",
    "### **True positive aggregation** (Chrabaszcz aggregation + baseline-mask filtering)\n",
    "\n",
    "Introduce the unperturbed prediction $P_0$, and, optionally, the ROI mask $R(x)$.\n",
    "Keep only voxels that are **still** class 1 *and* were class 1 before:\n",
    "\n",
    "$$\n",
    "S_{\\text{Chr\\,keep}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x} \\bigl[P_i(x)\\land P_0(x)\\land R(x)\\bigr]\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "* **True positives preserved** (TP core) add positive evidence.\n",
    "* **FP created by the perturbation** are **ignored** (masked out).\n",
    "* **FN** lower the score indirectly because their logits disappear from the sum.\n",
    "\n",
    "Conceptually this is the **positive part** of a signed logit-difference metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c45bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive_aggregation(logits: torch.Tensor,\n",
    "                          unperturbed_binary_mask: torch.Tensor,\n",
    "                          ROI_mask: torch.Tensor,\n",
    "                           scaling_factor: float = 1.0,\n",
    "                          ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n",
    "    Explanatory Analysis of 3D Segmentation Models\", with the  addition of filtering by the unperturbed segmentation.\n",
    "    We can use this to ignore \"false positive\" voxels -> only account for true positive contribution;\n",
    "    so this corresponds conceptually to the positive part of a logits difference metric\n",
    "    \"\"\"\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)          # (D,H,W)\n",
    "    seg_mask = seg_mask.bool() & ROI_mask.bool() & unperturbed_binary_mask.bool()  # prefer boolean indexing for reletively sparse tensors\n",
    "    aggregate = torch.sum(logits[1].double()[seg_mask])\n",
    "\n",
    "    return aggregate / scaling_factor  # normalize to avoid overflows in SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48f967",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **False-positive aggregation**\n",
    "\n",
    "Directly sum class-1 evidence from **new** positives inside ROI:\n",
    "\n",
    "$$\n",
    "S_{\\text{FP}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x}\n",
    "\\bigl[P_i(x)\\land\\neg P_0(x)\\land R(x)\\bigr]\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "* Measures **only** the spurious lymph-node evidence a perturbation introduces.\n",
    "* Higher value â‡’ stronger tendency to hallucinate extra nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6284667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_aggregation(logits: torch.Tensor,\n",
    "                              unperturbed_binary_mask: torch.Tensor,\n",
    "                              ROI_mask: torch.Tensor,\n",
    "                              scaling_factor: float = 1.0,\n",
    "                              ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Negative part of signed logit-difference objective, returned with positive sign;\n",
    "    Only accounts for false positive voxels in segmentation (spurious lymph nodes)\n",
    "    \"\"\"\n",
    "    # current segmentation (prevailing class)\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)     # (D,H,W) âˆˆ {0,1}\n",
    "\n",
    "    fp_mask  = seg_mask * ROI_mask * torch.logical_not(unperturbed_binary_mask.bool()).float()   # prefer float multiplication for dense tensors\n",
    "    aggregate = torch.sum(logits[1].double() * fp_mask) \n",
    "\n",
    "    return aggregate / scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54518c56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Dice aggregation (prediction-vs-baseline, ROI-restricted)**\n",
    "\n",
    "Let $R(x)$ be the ROI mask.\n",
    "\n",
    "$$\n",
    "P_i' = P_i \\odot R, \\qquad\n",
    "P_0' = P_0 \\odot R\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{\\text{Dice}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\;\n",
    "\\frac{2\\,\\langle P_i',\\,P_0'\\rangle}{\\lVert P_i'\\rVert_1 + \\lVert P_0'\\rVert_1 + \\varepsilon}\n",
    "$$\n",
    "\n",
    "* Drops when either **FP** ($P_i'=1,\\,P_0'=0$) or **FN** ($P_i'=0,\\,P_0'=1$) appear â†’ penalises both error types symmetrically.\n",
    "\n",
    "Based on the â€œself-consistency Diceâ€ used in MiSuRe (Hasany et al., 2024).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "694f3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_aggregation(logits: torch.Tensor,\n",
    "                    unperturbed_binary_mask: torch.Tensor,\n",
    "                    ROI_mask: torch.Tensor,\n",
    "                    scaling_factor: float = 1.0,\n",
    "                    eps: float = 1e-9,  # small value to avoid division by zero\n",
    "                    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Use Dice score, the same aggregation measure from \"Hasany et al. - 2024 - MiSuRe is all you need to explain your image segmentation\"\n",
    "    Dice score provides a single aggregation metric that accounts for both false negatives and false positives penalization.\n",
    "    Specificly, we instead score each perturbation supervoxels by that Dice => supervoxels that contribute the most in reproducing\n",
    "    the baseline segmentation, will get an higher score\n",
    "    \"\"\"\n",
    "    # 1. Boolean masks restricted to ROI\n",
    "    pred = (logits.argmax(dim=0) == 1).float() * ROI_mask.float()\n",
    "    base = unperturbed_binary_mask.float()      * ROI_mask.float()\n",
    "\n",
    "    # 2. Intersection and denominator\n",
    "    inter = (pred * base).sum()\n",
    "    denom = pred.sum() + base.sum() + eps       # |P| + |B|\n",
    "\n",
    "    # 3. Dice coefficient\n",
    "    dice = (2.0 * inter) / denom\n",
    "\n",
    "    return dice / scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be36dd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Signed logit-difference (masked)**\n",
    "\n",
    "Define a signed weight\n",
    "\n",
    "$$\n",
    "w(x)=\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } P_0(x)=1\\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases},\n",
    "\\qquad w(x)\\leftarrow w(x)\\,R(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{\\text{LD}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x} P_i(x)\\;w(x)\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "* **Positive attribution:** voxels that *keep* the baseline TP (support segmentation).\n",
    "* **Negative attribution:** voxels that become class 1 **only** after perturbation (generate FP inside ROI).\n",
    "* FN reduce the positive term (logits disappear) but do **not** add negative mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "551ec010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def logit_difference_aggregation(\n",
    "        logits: torch.Tensor,\n",
    "        unperturbed_binary_mask: torch.Tensor,\n",
    "        ROI_mask: torch.Tensor,\n",
    "        scaling_factor: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Signed logit-difference objective *masked by the prevailing class*.\n",
    "    \"\"\"\n",
    "    # current segmentation (prevailing class)\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)     # (D,H,W) âˆˆ {0,1}\n",
    "\n",
    "    # +1 inside baseline positives, âˆ’1 elsewhere...\n",
    "    signed_weight = torch.where(unperturbed_binary_mask.bool(),\n",
    "                                torch.tensor(1.0, device=logits.device),\n",
    "                                torch.tensor(-1.0, device=logits.device))\n",
    "\n",
    "    # ... but we only care of false positives inside the ROI (we don't even have the segmentation mask outside the ROI)\n",
    "    signed_weight = signed_weight * ROI_mask\n",
    "\n",
    "    # aggregate signed class-1 evidence, restricted to voxels\n",
    "    # that are *currently* predicted as class-1 (seg_mask)\n",
    "    aggregate = torch.sum(logits[1] * seg_mask * signed_weight)\n",
    "    return aggregate / scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f588c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# â·  Forward wrapper that nnU-Net expects\n",
    "# ------------------------------------------------------------\n",
    "if False:\n",
    "    @torch.inference_mode()\n",
    "    def forward_segmentation_output_to_explain(\n",
    "            input_image:         torch.Tensor,\n",
    "            perturbation_mask:   torch.BoolTensor | None,\n",
    "            segmentation_mask:      torch.Tensor,   # remember that must be cropped to the same size of the other tensors\n",
    "            ROI_bounding_box_mask:      torch.Tensor,\n",
    "            baseline_prediction_dict: dict\n",
    "    ) -> torch.Tensor:           # returns a scalar per sample\n",
    "        \"\"\"\n",
    "        Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n",
    "        by the network â€“ adapt to your real metric as needed.\n",
    "        \"\"\"\n",
    "        logits = predictor.predict_sliding_window_return_logits_with_caching(\n",
    "            input_image, perturbation_mask, baseline_prediction_dict,\n",
    "        )                              # (C, D, H, W)\n",
    "        # we now mask both by the segmentation prevalent class, and by ROI\n",
    "        D,H,W = logits.shape[1:]\n",
    "        aggregate = true_positive_aggregation(\n",
    "            logits = logits,\n",
    "            unperturbed_binary_mask = segmentation_mask,\n",
    "            ROI_mask = ROI_bounding_box_mask,\n",
    "            scaling_factor = (D*H*W)\n",
    "        )\n",
    "\n",
    "        return aggregate\n",
    "\n",
    "    # c) wrap your cachedâ€forward method:\n",
    "    explainer = KernelShapWithMask(\n",
    "        forward_func=lambda vol, _perturbation_mask: forward_segmentation_output_to_explain(\n",
    "            input_image=vol,\n",
    "            perturbation_mask=_perturbation_mask,\n",
    "            segmentation_mask=segmentation_mask,\n",
    "            ROI_bounding_box_mask=ROI_mask,\n",
    "            baseline_prediction_dict=cropped_baseline_pred_cache),\n",
    "        #surrogate_model = \"lasso\",\n",
    "        #alpha_surrogate = 0.003,\n",
    "        #max_iter_surrogate = 10000\n",
    "    )\n",
    "\n",
    "    # d) compute SHAP\n",
    "    attr = explainer.attribute(\n",
    "        inputs=volume,       # (1,C,D,H,W)\n",
    "        baselines=0.0, \n",
    "        feature_mask=supervoxel_map,\n",
    "        n_samples=10,\n",
    "        return_input_shape=True,\n",
    "        monitor_log_path=\"monitor.jsonl\",\n",
    "        monitor_convergence_step=10,\n",
    "        monitor_local_accuracy_step=20,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(\"Attributions:\", attr.shape)  # â†’ (1,C,D,H,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08336c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\nwith open('dataset-tp.pkl', 'wb') as file:\\n    pickle.dump(explainer.dataset, file)\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pickle\n",
    "with open('dataset-tp.pkl', 'wb') as file:\n",
    "    pickle.dump(explainer.dataset, file)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd1f37a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"attr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\\nattr_img = nib.Nifti1Image(attr_postprocessed, affine_cropped_volume)\\nnib.save(attr_img, 'attribution_map-TP.nii.gz')\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"attr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\n",
    "attr_img = nib.Nifti1Image(attr_postprocessed, affine_cropped_volume)\n",
    "nib.save(attr_img, 'attribution_map-TP.nii.gz')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcd5cf",
   "metadata": {},
   "source": [
    "### Define a function to prepare all the steps for SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a2ba563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def prepare_data_for_shap(volume_path: str,\n",
    "                          patch_size: np.ndarray,\n",
    "                          predictor: nnUNetPredictor,\n",
    "                          dataset_json_path: str,\n",
    "                          ROI_BB_path: str | None = None,\n",
    "                          ROI_segmentation_mask_path: str | None = None,\n",
    "                          ROI_type: str = \"BoundingBox\",\n",
    "                          nnunet_preprocessing: bool = True,\n",
    "                          supervoxel_type: str = \"FCC\",):\n",
    "    \"\"\"\n",
    "    Prepare data for SHAP analysis by cropping the volume and creating a binary mask for the ROI.\n",
    "    This function handles both bounding box and masked segmentation types of ROIs.\n",
    "    It crops the volume to the specified ROI and applies receptive field adjustments.\n",
    "\n",
    "    Parameters\n",
    "    volume_path : str\n",
    "        Path to the input volume file (NIfTI format).\n",
    "    patch_size : np.ndarray\n",
    "        Patch size used for the model, which determines the receptive field.\n",
    "    ROI_BB_path : str | None, optional\n",
    "        Path to the bounding box ROI JSON file. Required if `ROI_type` is \"BoundingBox\".\n",
    "    ROI_segmentation_mask_path : str | None, optional\n",
    "        Path to the segmentation mask file. Required if `ROI_type` is \"MaskedSegmentation\".\n",
    "    ROI_type : str, optional\n",
    "        Type of ROI to use, either \"BoundingBox\" or \"MaskedSegmentation\". Default is \"BoundingBox\".\n",
    "    nnunet_preprocessing : bool, optional\n",
    "        Whether to apply nnU-Net preprocessing to the cropped volume. Default is True.\n",
    "    supervoxel_type : str, optional\n",
    "        Type of supervoxel segmentation to use, either \"FCC\" or \"SLIC\".\n",
    "        Default is \"FCC\".\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # load ROI bounding box json or segmentation mask, depending on the ROI type\n",
    "    if ROI_type == \"BoundingBox\":\n",
    "        \n",
    "        BB_ROI = json.load(open(ROI_BB_path, \"r\"))\n",
    "        # test the bounding box extraction\n",
    "        x_slice, y_slice, z_slice = get_slices_from_BB_ROI(BB_ROI)\n",
    "        print(\"Bounding box slices from BB_ROI:\")\n",
    "        print(\"  x:\", x_slice)\n",
    "        print(\"  y:\", y_slice)\n",
    "        print(\"  z:\", z_slice)\n",
    "\n",
    "    elif ROI_type == \"MaskedSegmentation\":\n",
    "\n",
    "        ROI_segmentation_mask = nib.load(ROI_segmentation_mask_path)\n",
    "\n",
    "        print(ROI_segmentation_mask.get_fdata().shape)\n",
    "        print(ROI_segmentation_mask.affine)\n",
    "\n",
    "        x_slice, y_slice, z_slice = get_mask_bbox_slices(ROI_segmentation_mask_path)\n",
    "        print(\"Bounding box slices:\")\n",
    "        print(\"  x:\", x_slice)\n",
    "        print(\"  y:\", y_slice)\n",
    "        print(\"  z:\", z_slice)\n",
    "        \n",
    "    # --------------------------------------------\n",
    "    # obtain a binary mask for the ROI\n",
    "\n",
    "    ROI_mask_path = \"ROI_binary_mask.nii.gz\"\n",
    "    image = nib.load(volume_path)\n",
    "    shape = image.get_fdata().shape\n",
    "    affine = image.affine\n",
    "    print(\"shape\", shape)\n",
    "    bbox_slices = (x_slice, y_slice, z_slice)\n",
    "    print(\"bbox_slices:\", bbox_slices)\n",
    "    ROI_mask = slices_to_binary_mask(\n",
    "        volume_shape=shape,\n",
    "        bbox_slices=bbox_slices,\n",
    "    )\n",
    "    ROI_mask_nii = nib.Nifti1Image(ROI_mask, affine)\n",
    "    # Save the binary mask as a NIfTI file\n",
    "    nib.save(\n",
    "            ROI_mask_nii,\n",
    "            ROI_mask_path\n",
    "            )\n",
    "\n",
    "    print(f\"Saved binary ROI mask to {ROI_mask_path}\")\n",
    "    print(\"ROI mask shape:\", ROI_mask.shape)\n",
    "    print(\"ROI mask unique values:\", np.unique(ROI_mask))\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # use receptive field to crop the volume and ROI mask\n",
    "    print(\"Patch size: \", patch_size)\n",
    "\n",
    "    # Receptive field is twice the patch size-1\n",
    "    RF = 2*(patch_size-1)\n",
    "    print(\"Receptive field: \", RF)\n",
    "\n",
    "    volume_shape = nib.load(volume_path).shape # (W, H, D) -> x, y, z\n",
    "    # RF shape -> (D, H, W) -> z, y, x (model input shape)\n",
    "    print(\"Original volume shape:\", volume_shape)\n",
    "\n",
    "    W, H, D = volume_shape\n",
    "\n",
    "    # backward sorted receptive field axes\n",
    "    RF_x, RF_y, RF_z = RF[2],RF[1],RF[0]\n",
    "\n",
    "    x_slice_RF = slice(int(max(x_slice.start - RF_x/2, 0)), int(min(x_slice.stop + RF_x/2, W)))\n",
    "    y_slice_RF = slice(int(max(y_slice.start - RF_y/2, 0)), int(min(y_slice.stop + RF_y/2, H)))\n",
    "    z_slice_RF = slice(int(max(z_slice.start - RF_z/2, 0)), int(min(z_slice.stop + RF_z/2, D)))\n",
    "\n",
    "    print(\"new  x:\", x_slice_RF)\n",
    "    print(\"new  y:\", y_slice_RF)\n",
    "    print(\"new  z:\", z_slice_RF)\n",
    "\n",
    "\n",
    "    slices = (x_slice_RF, y_slice_RF, z_slice_RF)\n",
    "\n",
    "\n",
    "    cropped_volume_path = \"cropped_volume.nii.gz\"\n",
    "    cropped_volume, affine_cropped_volume = crop_volume_and_affine(\n",
    "        nii_path=volume_path,\n",
    "        bbox_slices=slices,\n",
    "        save_cropped_nii_path=Path(cropped_volume_path)\n",
    "    )\n",
    "\n",
    "    print(\"Cropped data shape:\", cropped_volume.shape)\n",
    "    print(\"New affine:\\n\", affine_cropped_volume)\n",
    "\n",
    "    if ROI_type == \"MaskedSegmentation\":\n",
    "        cropped_ROI_segmentation_mask, affine_ROI_segmentation_cropped_mask = crop_volume_and_affine(\n",
    "            nii_path=ROI_segmentation_mask_path,\n",
    "            bbox_slices=slices,\n",
    "            save_cropped_nii_path=Path(\"cropped_segmentation_mask.nii.gz\")\n",
    "        )\n",
    "\n",
    "        print(\"Cropped mask shape:\", cropped_ROI_segmentation_mask.shape)\n",
    "        print(\"New mask affine:\\n\", affine_ROI_segmentation_cropped_mask)\n",
    "\n",
    "    cropped_ROI_mask, affine_ROI_cropped_mask = crop_volume_and_affine(\n",
    "        nii_path=ROI_mask_path,\n",
    "        bbox_slices=slices,\n",
    "        save_cropped_nii_path=Path(\"cropped_mask_with_RF.nii.gz\")\n",
    "    )\n",
    "\n",
    "    print(\"Cropped mask shape:\", cropped_ROI_mask.shape)\n",
    "    print(\"New mask affine:\\n\", affine_ROI_cropped_mask)\n",
    "\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # prepare the volume for nnU-Net\n",
    "    \n",
    "    if nnunet_preprocessing:\n",
    "        # TODO : pass other volumes, masks... through the same transformation\n",
    "        # preprocessing is not usable for now\n",
    "        volume_np = nnunetv2_default_preprocessing(cropped_volume_path, predictor, dataset_json_path)\n",
    "        # debug: save nifti\n",
    "        volume_nii = nib.Nifti1Image(volume_np.squeeze().transpose(2,1,0), affine_cropped_volume)\n",
    "        nib.save(volume_nii, \"preprocessed_volume.nii.gz\")\n",
    "    else:\n",
    "        # load the cropped volume and convert to numpy array\n",
    "        cropped_volume_nii = nib.load(cropped_volume_path)\n",
    "        volume_np = cropped_volume_nii.get_fdata()\n",
    "        # convert to nnU-Net order (D, H, W)\n",
    "        volume_np = np.transpose(volume_np, (2, 1, 0))\n",
    "        # add channel dimension (C=1)\n",
    "        volume_np = np.expand_dims(volume_np, axis=0)  # shape (C, D, H, W)\n",
    "\n",
    "    # (b) convert to torch tensor and add batch dimension\n",
    "    volume = torch.from_numpy(volume_np.astype(np.float32)).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\n",
    "\n",
    "    print(\"Volume shape:\", volume.shape)             # (1, C, D, H, W)\n",
    "\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # prepare the supervoxel map\n",
    "\n",
    "    # (b) super-voxel / organ-id map  (W, H, D)\n",
    "    # Load the image\n",
    "    img = nib.load(cropped_volume_path)\n",
    "\n",
    "    if supervoxel_type == \"FCC\":\n",
    "        supervoxel_map_path = 'FCC-supervoxel_map.nii.gz'\n",
    "        # Generate and save supervoxel map using the FCC\n",
    "        cube_side = 100.00 # [mm]\n",
    "        supervoxel_map = generate_supervoxel_map(img, S=cube_side) # (W, H, D)\n",
    "        # Save the result\n",
    "        supervoxel_map_img = nib.Nifti1Image(supervoxel_map.astype(np.int32), affine=img.affine)\n",
    "        nib.save(supervoxel_map_img, supervoxel_map_path)\n",
    "            \n",
    "\n",
    "    elif supervoxel_type == \"SLIC\":\n",
    "        supervoxel_map_path = 'SLIC_supervoxel_map.nii.gz'\n",
    "        # compute spacing for SLIC\n",
    "        data = img.get_fdata()\n",
    "        affine = img.affine\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        \n",
    "        # derive slic spacing from affine\n",
    "        spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))\n",
    "        spacing = tuple(spacing)  # convert to tuple for slic\n",
    "        \n",
    "        # Generate and save supervoxel map using SLIC\n",
    "        n_supervoxels = 380\n",
    "        supervoxel_map = apply_SLIC(data, spacing, n_supervoxels)\n",
    "        \n",
    "        # Save the result\n",
    "        supervoxel_map_img = nib.Nifti1Image(supervoxel_map.astype(np.int32), affine=affine)\n",
    "        nib.save(supervoxel_map_img, 'SLIC_supervoxel_map.nii.gz')\n",
    "            \n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    print(\"Mappa supervoxel shape:\", supervoxel_map.shape)\n",
    "    n_supervoxels = len(np.unique(supervoxel_map))\n",
    "    print(\"Numero di supervoxels:\", n_supervoxels)\n",
    "    supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\n",
    "    # we need features of feature mask ordered from 0 (or 1) to M-1 (M)\n",
    "    sv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\n",
    "\n",
    "    supervoxel_map = indexes.reshape(supervoxel_map.shape)\n",
    "    print(\"number of supervoxels: \", np.unique(supervoxel_map).size)\n",
    "\n",
    "    # IMPORTANT ðŸ”¸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\n",
    "    supervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\n",
    "\n",
    "    print(\"Mask shape:\", supervoxel_map.shape)\n",
    "\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # obtain the cropped segmentation mask in case of BoundingBox\n",
    "\n",
    "    if ROI_type == \"BoundingBox\":\n",
    "        cropped_logits = predictor.predict_sliding_window_return_logits(volume[0])                              # (C, D, H, W)\n",
    "        cropped_segmentation_mask = (torch.argmax(cropped_logits, dim=0) == 1)\n",
    "\n",
    "   # --------------------------------------------\n",
    "    # obtain the cached output dictionary for the cropped volume\n",
    "\n",
    "    cropped_baseline_pred_cache = get_cached_output_dictionary(\n",
    "        volume_file = cropped_volume_path,\n",
    "        predictor = predictor,\n",
    "        preprocess_before_run = nnunet_preprocessing,\n",
    "        verbose = True,\n",
    "    )\n",
    "    \"\"\"# Write to file\n",
    "    with open(\"cropped_baseline_output_dictionary_cache.pkl\", \"wb\") as f:\n",
    "        pkl.dump(cropped_baseline_pred_cache, f)\"\"\"\n",
    "\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # prepare the segmentation mask and ROI mask for Captum SHAP\n",
    "\n",
    "    if ROI_type == \"SegmentationMask\":\n",
    "        # segmentation mask cropped to ROI, with background extended to rf\n",
    "        segmentation_mask = np.transpose(cropped_ROI_segmentation_mask, (2, 1, 0))\n",
    "        segmentation_mask = torch.from_numpy(ROI_segmentation_mask).to(device)\n",
    "\n",
    "        print(ROI_segmentation_mask.shape)\n",
    "    elif ROI_type == \"BoundingBox\":\n",
    "        # \"full\" segmentation mask cropped to RF\n",
    "        # already in torch, with correct shape (D, H, W)\n",
    "        segmentation_mask = cropped_segmentation_mask.to(device)\n",
    "\n",
    "    # ROI bounding box with background extended to RF\n",
    "    ROI_mask = np.transpose(cropped_ROI_mask, (2, 1, 0))\n",
    "    ROI_mask = torch.from_numpy(ROI_mask).to(device)\n",
    "\n",
    "    print(ROI_mask.shape)  \n",
    "\n",
    "    # --------------------------------------------\n",
    "    # return all the prepared data\n",
    "    ##############\n",
    "\n",
    "    return volume, img.affine, supervoxel_map, segmentation_mask, ROI_mask, cropped_baseline_pred_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fccb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_shap(\n",
    "    volume_path: str,\n",
    "    patch_size: np.ndarray,\n",
    "    predictor,\n",
    "    dataset_json_path: str,\n",
    "    ROI_BB_path: Optional[str] = None,\n",
    "    ROI_segmentation_mask_path: Optional[str] = None,\n",
    "    ROI_type: str = \"BoundingBox\",\n",
    "    nnunet_preprocessing: bool = True,\n",
    "    supervoxel_type: str = \"FCC\",\n",
    "    device: Optional[str] = 'cuda:0'\n",
    ") -> Tuple[\"torch.Tensor\", np.ndarray, \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", Dict]:\n",
    "    \"\"\"\n",
    "    Orchestrate data preparation by invoking modular utilities.\n",
    "    Returns:\n",
    "      - volume_tensor (1,C,D,H,W)\n",
    "      - affine of cropped volume\n",
    "      - supervoxel_map_tensor (D,H,W)\n",
    "      - segmentation_mask_tensor (D,H,W)\n",
    "      - ROI_mask_tensor (D,H,W)\n",
    "      - baseline_cache dict\n",
    "    \"\"\"\n",
    "    # 1. Load ROI slices\n",
    "    x_slice, y_slice, z_slice = load_roi_slices(\n",
    "        ROI_type,\n",
    "        ROI_BB_path,\n",
    "        ROI_segmentation_mask_path\n",
    "    )\n",
    "\n",
    "    # 2. Create binary ROI mask\n",
    "    roi_mask_path = create_roi_mask(\n",
    "        volume_path,\n",
    "        (x_slice, y_slice, z_slice)\n",
    "    )\n",
    "\n",
    "    # 3. Crop volume and ROI mask with receptive field\n",
    "    cropped = crop_volume_with_rf(\n",
    "        volume_path,\n",
    "        (x_slice, y_slice, z_slice),\n",
    "        patch_size\n",
    "    )\n",
    "    cropped_volume, affine_cropped = cropped['cropped_volume']\n",
    "    cropped_mask, _ = cropped['cropped_roi_mask']\n",
    "    cropped_vol_path = Path('cropped_volume.nii.gz')\n",
    "    cropped_mask_path = Path('cropped_mask_with_RF.nii.gz')\n",
    "\n",
    "    # 4. Preprocess volume\n",
    "    volume_np, _ = preprocess_volume(\n",
    "        str(cropped_vol_path),\n",
    "        predictor,\n",
    "        dataset_json_path,\n",
    "        use_nnunet=nnunet_preprocessing\n",
    "    )\n",
    "\n",
    "    # 5. Convert to torch tensor\n",
    "    volume_tensor = torch.from_numpy(\n",
    "        volume_np.astype(np.float32)\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "    # 6. Generate supervoxel map\n",
    "    sv_array = generate_supervoxel_map(\n",
    "        str(cropped_vol_path),\n",
    "        supervoxel_type=supervoxel_type\n",
    "    )\n",
    "    sv_tensor = torch.from_numpy(sv_array).long().to(device)\n",
    "\n",
    "    # 7. Compute baseline segmentation and cache\n",
    "    seg_mask, cache_dict = compute_baseline_prediction(\n",
    "        volume_tensor,\n",
    "        predictor,\n",
    "        str(cropped_vol_path),\n",
    "        use_nnunet=nnunet_preprocessing\n",
    "    )\n",
    "\n",
    "    # 8. Prepare ROI mask tensor\n",
    "    # transpose (W,H,D)->(D,H,W)\n",
    "    roi_mask_tensor = torch.from_numpy(\n",
    "        np.transpose(cropped_mask, (2,1,0))\n",
    "    ).to(device)\n",
    "\n",
    "    return volume_tensor, affine_cropped, sv_tensor, seg_mask.to(device), roi_mask_tensor, cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5f78e",
   "metadata": {},
   "source": [
    "## Iterate over volumes and run SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65d5356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters for the experiment\n",
    "\n",
    "# if true, we will use a json file containing the bounding box for the Region of Interest (ROI), otherwise we will use a masked segmentation\n",
    "ROI_TYPE = \"BoundingBox\"  # \"BoundingBox\" or \"MaskedSegmentation\"\n",
    "\n",
    "NNUNET_PREPROCESSING = False\n",
    "SUPERVOXEL_TYPE = \"FCC\"\n",
    "# Forbidden for multiple (different) volumes, only for single volume\n",
    "#USE_SAVED_MAP = True\n",
    "#USE_STORED_CACHE_DICTIONARY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b467494",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_json_path = Path(model_dir) / \"dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8d7d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ROI_TYPE == \"BoundingBox\":\n",
    "    BB_ROI_paths = {}\n",
    "    for volume_code in volume_codes:\n",
    "        BB_ROI_paths[volume_code] = nnUNet_results + \"/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/BB-ROI/\" + f\"AUTOMI_{volume_code}.json\"\n",
    "elif ROI_TYPE == \"MaskedSegmentation\":\n",
    "    # get the manually derived ROI mask from the dataset, where we manually added it\n",
    "    if IN_KAGGLE:\n",
    "        ROI_segmentation_mask_path = \"/kaggle/input/segmentation-masked-ROI.nii\"\n",
    "    else:\n",
    "        ROI_segmentation_mask_path = nnUNet_raw + \"/segmentation-masked-ROI.nii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6781c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = np.array(predictor.configuration_manager.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "734131bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def forward_segmentation_output_to_explain(\n",
    "        input_image:         torch.Tensor,\n",
    "        perturbation_mask:   torch.BoolTensor | None,\n",
    "        segmentation_mask:      torch.Tensor,   # remember that must be cropped to the same size of the other tensors\n",
    "        ROI_bounding_box_mask:      torch.Tensor,\n",
    "        baseline_prediction_dict: dict\n",
    ") -> torch.Tensor:           # returns a scalar per sample\n",
    "    \"\"\"\n",
    "    Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n",
    "    by the network â€“ adapt to your real metric as needed.\n",
    "    \"\"\"\n",
    "    logits = predictor.predict_sliding_window_return_logits_with_caching(\n",
    "        input_image, perturbation_mask, baseline_prediction_dict,\n",
    "    )                              # (C, D, H, W)\n",
    "    # we now mask both by the segmentation prevalent class, and by ROI\n",
    "    D,H,W = logits.shape[1:]\n",
    "    aggregate = true_positive_aggregation(\n",
    "        logits = logits,\n",
    "        unperturbed_binary_mask = segmentation_mask,\n",
    "        ROI_mask = ROI_bounding_box_mask,\n",
    "        scaling_factor = (D*H*W)\n",
    "    )\n",
    "\n",
    "    return aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f67c6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_codes = [\"00004\", \"00005\", \"00024\", \"00027\", \"00029\", \"00034\", \"00044\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798193d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on volume code 0: 00004\n",
      "  Shape: (512, 512, 221)\n",
      "Bounding box slices from BB_ROI:\n",
      "  x: slice(277, 371, None)\n",
      "  y: slice(194, 292, None)\n",
      "  z: slice(93, 115, None)\n",
      "shape (512, 512, 221)\n",
      "bbox_slices: (slice(277, 371, None), slice(194, 292, None), slice(93, 115, None))\n",
      "Saved binary ROI mask to ROI_binary_mask.nii.gz\n",
      "ROI mask shape: (512, 512, 221)\n",
      "ROI mask unique values: [0 1]\n",
      "Patch size:  [ 72 160 160]\n",
      "Receptive field:  [142 318 318]\n",
      "Original volume shape: (512, 512, 221)\n",
      "new  x: slice(118, 512, None)\n",
      "new  y: slice(35, 451, None)\n",
      "new  z: slice(22, 186, None)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for i, volume_code in enumerate(volume_codes):\n",
    "    print(f\"Working on volume code {i}: {volume_code}\")\n",
    "    print(f\"  Shape: {nib.load(ct_img_paths[volume_code]).shape}\")\n",
    "\n",
    "    os.mkdir(volume_code) if not os.path.exists(volume_code) else None\n",
    "\n",
    "    volume, affine, supervoxel_map, segmentation_mask, ROI_mask, cache_dict = prepare_data_for_shap(\n",
    "        volume_path=ct_img_paths[volume_code],\n",
    "        patch_size=patch_size,\n",
    "        predictor=predictor,\n",
    "        dataset_json_path=dataset_json_path,\n",
    "        ROI_BB_path=BB_ROI_paths[volume_code] if ROI_TYPE == \"BoundingBox\" else None,\n",
    "        ROI_segmentation_mask_path=ROI_segmentation_mask_path if ROI_TYPE == \"MaskedSegmentation\" else None,\n",
    "        ROI_type=ROI_TYPE,\n",
    "        nnunet_preprocessing=NNUNET_PREPROCESSING,\n",
    "        supervoxel_type=SUPERVOXEL_TYPE,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # c) wrap your cachedâ€forward method:\n",
    "    explainer = KernelShapWithMask(\n",
    "        forward_func=lambda vol, _perturbation_mask: forward_segmentation_output_to_explain(\n",
    "            input_image=vol,\n",
    "            perturbation_mask=_perturbation_mask,\n",
    "            segmentation_mask=segmentation_mask,\n",
    "            ROI_bounding_box_mask=ROI_mask,\n",
    "            baseline_prediction_dict=cache_dict)\n",
    "    )\n",
    "\n",
    "    # d) compute SHAP\n",
    "    attr = explainer.attribute(\n",
    "        inputs=volume,       # (1,C,D,H,W)\n",
    "        baselines=0.0, \n",
    "        feature_mask=supervoxel_map,\n",
    "        n_samples=4000,\n",
    "        return_input_shape=True,\n",
    "        monitor_log_path=join(volume_code, \"monitor.jsonl\"),\n",
    "        monitor_convergence_step=10,\n",
    "        monitor_local_accuracy_step=20,\n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(\"Attributions:\", attr.shape)  # â†’ (1,C,D,H,W)\n",
    "\n",
    "\n",
    "    with open(join(volume_code, 'dataset.pkl'), 'wb') as file:\n",
    "        pickle.dump(explainer.dataset, file)\n",
    "\n",
    "    attr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\n",
    "    attr_img = nib.Nifti1Image(attr_postprocessed, affine)\n",
    "    nib.save(attr_img, join(volume_code, 'attribution_map.nii.gz'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
