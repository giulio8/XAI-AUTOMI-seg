{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6a9f52",
   "metadata": {},
   "source": [
    "# 1. Import Packages for the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8bae1c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:50.637753Z",
     "iopub.status.busy": "2025-08-08T14:06:50.636815Z",
     "iopub.status.idle": "2025-08-08T14:06:50.641620Z",
     "shell.execute_reply": "2025-08-08T14:06:50.640837Z",
     "shell.execute_reply.started": "2025-08-08T14:06:50.637727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import basic packages for later use\n",
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ef0dfc9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:50.643048Z",
     "iopub.status.busy": "2025-08-08T14:06:50.642865Z",
     "iopub.status.idle": "2025-08-08T14:06:50.655459Z",
     "shell.execute_reply": "2025-08-08T14:06:50.654850Z",
     "shell.execute_reply.started": "2025-08-08T14:06:50.643034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5c406cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:50.656600Z",
     "iopub.status.busy": "2025-08-08T14:06:50.656343Z",
     "iopub.status.idle": "2025-08-08T14:06:50.665424Z",
     "shell.execute_reply": "2025-08-08T14:06:50.664749Z",
     "shell.execute_reply.started": "2025-08-08T14:06:50.656579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IN_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IN_COLAB = not IN_KAGGLE and os.path.exists('/content')\n",
    "IN_DEIB = not IN_KAGGLE and not IN_COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "870339bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:50.666895Z",
     "iopub.status.busy": "2025-08-08T14:06:50.666652Z",
     "iopub.status.idle": "2025-08-08T14:06:57.482302Z",
     "shell.execute_reply": "2025-08-08T14:06:57.481487Z",
     "shell.execute_reply.started": "2025-08-08T14:06:50.666873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not IN_DEIB:\n",
    "    !pip install nnunetv2\n",
    "    !pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778345b",
   "metadata": {},
   "source": [
    "# 2. Mount the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "73794372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.483490Z",
     "iopub.status.busy": "2025-08-08T14:06:57.483277Z",
     "iopub.status.idle": "2025-08-08T14:06:57.841187Z",
     "shell.execute_reply": "2025-08-08T14:06:57.840437Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.483468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir: /workspace/output\n"
     ]
    }
   ],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Google Colab\n",
    "    # for colab users only - mounting the drive\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount = True)\n",
    "\n",
    "    drive_dir = \"/content/drive/My Drive\"\n",
    "    mount_dir = join(drive_dir, \"tesi\", \"automi\")\n",
    "    base_dir = os.getcwd()\n",
    "elif IN_KAGGLE:\n",
    "    # Kaggle\n",
    "    mount_dir = \"/kaggle/input/automi-seg\"\n",
    "    base_dir = os.getcwd()\n",
    "    print(base_dir)\n",
    "    !ls '/kaggle/input'\n",
    "    !cd \"/kaggle/input/automi-seg\" ; ls\n",
    "else:\n",
    "    mount_dir = \"/workspace/data\"\n",
    "    base_dir = \"/workspace/output\"\n",
    "    os.chdir(base_dir)\n",
    "    print(\"base_dir:\", base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0894ea9",
   "metadata": {},
   "source": [
    "# 3. Setting up nnU-Nets folder structure and environment variables\n",
    "nnUnet expects a certain folder structure and environment variables.\n",
    "\n",
    "Roughly they tell nnUnet:\n",
    "1. Where to look for stuff\n",
    "2. Where to put stuff\n",
    "\n",
    "For more information about this please check: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da019dc2",
   "metadata": {},
   "source": [
    "## 3.1 Set environment Variables and creating folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "604985dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.842574Z",
     "iopub.status.busy": "2025-08-08T14:06:57.842268Z",
     "iopub.status.idle": "2025-08-08T14:06:57.849043Z",
     "shell.execute_reply": "2025-08-08T14:06:57.848358Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.842548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: /workspace/data/nnunet_raw\n",
      "nnUNet_results: /workspace/data/results\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# ğŸ“¦ SETUP nnUNet ENVIRONMENT\n",
    "# ===========================\n",
    "\n",
    "# Definisci i path da settare\n",
    "path_dict = {\n",
    "    \"nnUNet_raw\": join(mount_dir, \"nnunet_raw\"),\n",
    "    \"nnUNet_preprocessed\": join(mount_dir, \"preprocessed_files\"),#\"nnUNet_preprocessed\"),\n",
    "    \"nnUNet_results\": join(mount_dir, \"results\"),#\"nnUNet_results\"),\n",
    "    # \"RAW_DATA_PATH\": join(mount_dir, \"RawData\"),  # Facoltativo, se ti serve salvare zips\n",
    "}\n",
    "\n",
    "# Scrivi i path nelle variabili di ambiente, che vengono lette dal modulo paths di nnunetv2\n",
    "for env_var, path in path_dict.items():\n",
    "    os.environ[env_var] = path\n",
    "\n",
    "from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    if nnUNet_raw == None:\n",
    "        nnUNet_raw = \"/kaggle/input/nnunet_raw\"\n",
    "    if nnUNet_results == None:\n",
    "        nnUNet_results = \"/kaggle/input/results\"\n",
    "    # Kaggle has some very unconsistent behaviors in dataset mounting...\n",
    "    #nnUNet_raw = \"/kaggle/input/automi-seg/nnunet_raw\"\n",
    "    #nnUNet_results = \"/kaggle/input/automi-seg/results\"\n",
    "    \n",
    "print(\"nnUNet_raw:\", nnUNet_raw)\n",
    "print(\"nnUNet_results:\", nnUNet_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "7dadb438",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results_path = join(mount_dir, \"experiment_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a822df",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f0711042",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.849921Z",
     "iopub.status.busy": "2025-08-08T14:06:57.849753Z",
     "iopub.status.idle": "2025-08-08T14:06:57.861358Z",
     "shell.execute_reply": "2025-08-08T14:06:57.860628Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.849908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# all volumes of fold 0 test set\n",
    "volume_codes = [\"00004\", \"00005\", \"00024\", \"00027\", \"00029\", \"00034\", \"00039\", \"00044\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7ab74e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.863465Z",
     "iopub.status.busy": "2025-08-08T14:06:57.863211Z",
     "iopub.status.idle": "2025-08-08T14:06:57.917174Z",
     "shell.execute_reply": "2025-08-08T14:06:57.916624Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.863449Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume 00004:\n",
      "CT shape: (512, 512, 221)\n",
      "Organ shape: (512, 512, 221)\n",
      "Spacing: (1.3671875, 1.3671875, 5.0)\n",
      "Organ spacing: (1.3671875, 1.3671875, 5.0)\n",
      "Volume 00005:\n",
      "CT shape: (512, 512, 219)\n",
      "Organ shape: (512, 512, 219)\n",
      "Spacing: (1.171875, 1.171875, 5.0)\n",
      "Organ spacing: (1.171875, 1.171875, 5.0)\n",
      "Volume 00024:\n",
      "CT shape: (512, 512, 321)\n",
      "Organ shape: (512, 512, 321)\n",
      "Spacing: (0.976562, 0.976562, 3.0)\n",
      "Organ spacing: (0.976562, 0.976562, 3.0)\n",
      "Volume 00027:\n",
      "CT shape: (512, 512, 236)\n",
      "Organ shape: (512, 512, 236)\n",
      "Spacing: (1.3671875, 1.3671875, 5.0)\n",
      "Organ spacing: (1.3671875, 1.3671875, 5.0)\n",
      "Volume 00029:\n",
      "CT shape: (512, 512, 259)\n",
      "Organ shape: (512, 512, 259)\n",
      "Spacing: (1.367188, 1.367188, 7.5)\n",
      "Organ spacing: (1.367188, 1.367188, 7.5)\n",
      "Volume 00034:\n",
      "CT shape: (512, 512, 249)\n",
      "Organ shape: (512, 512, 249)\n",
      "Spacing: (1.171875, 1.171875, 5.0)\n",
      "Organ spacing: (1.171875, 1.171875, 5.0)\n",
      "Volume 00039:\n",
      "CT shape: (512, 512, 283)\n",
      "Organ shape: (512, 512, 283)\n",
      "Spacing: (1.171875, 1.171875, 5.0)\n",
      "Organ spacing: (1.171875, 1.171875, 5.0)\n",
      "Volume 00044:\n",
      "CT shape: (512, 512, 232)\n",
      "Organ shape: (512, 512, 232)\n",
      "Spacing: (1.1712891, 1.1712891, 5.0)\n",
      "Organ spacing: (1.1712891, 1.1712891, 5.0)\n"
     ]
    }
   ],
   "source": [
    "ct_img_paths = {}\n",
    "organ_map_paths = {}\n",
    "\n",
    "for volume_code in volume_codes:\n",
    "    if IN_KAGGLE:\n",
    "        ct_img_paths[volume_code] = join(nnUNet_raw, \"imagesTr\", f\"AUTOMI_{volume_code}_0000.nii\")\n",
    "        organ_map_paths[volume_code] = join(nnUNet_raw, \"total_segmentator_structures\", f\"AUTOMI_{volume_code}_0000\", \"mask_mask_add_input_20_total_segmentator.nii\")\n",
    "    else:\n",
    "        ct_img_paths[volume_code] = join(nnUNet_raw, \"imagesTr\", f\"AUTOMI_{volume_code}_0000.nii.gz\")\n",
    "        organ_map_paths[volume_code] = join(nnUNet_raw, \"total_segmentator_structures\", f\"AUTOMI_{volume_code}_0000\", \"mask_mask_add_input_20_total_segmentator.nii.gz\")\n",
    "    ct_img = nib.load(ct_img_paths[volume_code])\n",
    "    organ_map = nib.load(organ_map_paths[volume_code])\n",
    "    print(f\"Volume {volume_code}:\")\n",
    "    print(\"CT shape:\", ct_img.shape)\n",
    "    print(\"Organ shape:\", organ_map.shape)\n",
    "    print(\"Spacing:\", ct_img.header.get_zooms())\n",
    "    print(\"Organ spacing:\", organ_map.header.get_zooms())\n",
    "    assert np.all(ct_img.affine == organ_map.affine), \"CT and organ mask affine matrices do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9b07b79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.918051Z",
     "iopub.status.busy": "2025-08-08T14:06:57.917865Z",
     "iopub.status.idle": "2025-08-08T14:06:57.924014Z",
     "shell.execute_reply": "2025-08-08T14:06:57.923483Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.918037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affine: [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  3.00000000e+02]\n",
      " [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.85300003e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.43419995e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "ct_img = nib.load(ct_img_paths[volume_codes[1]])\n",
    "print(\"affine:\", ct_img.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "06e3be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "# define an utility for Nifty file saving\n",
    "def save_nifty(data: Union[np.ndarray, torch.Tensor], affine, path, dtype=np.float32):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.detach().cpu().numpy()\n",
    "    # set fixed type float32\n",
    "    data = data.astype(dtype)\n",
    "    nib.save(nib.Nifti1Image(data, affine), path)\n",
    "\n",
    "def save_nifty_binary(data: Union[np.ndarray, torch.Tensor], affine, path):\n",
    "    return save_nifty(data, affine, path, dtype=np.uint8)\n",
    "\n",
    "\n",
    "# define an utility for annoying nnunetv2 preprocessing\n",
    "def nnunetv2_default_preprocessing(ct_img_path, \n",
    "                                   predictor, \n",
    "                                   dataset_json_path,\n",
    "                                   other_volumes: Union[np.ndarray, None] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses the CT image and other volumes using nnunetv2's default preprocessing\n",
    "    pipeline. This function reads the CT image, applies the preprocessor, and returns\n",
    "    the preprocessed image.\n",
    "    \"\"\"\n",
    "    plans_manager = predictor.plans_manager\n",
    "    configuration_manager = predictor.configuration_manager\n",
    "    \n",
    "    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n",
    "    rw = plans_manager.image_reader_writer_class()\n",
    "    if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "        rw = rw()\n",
    "    img_np, img_props = rw.read_images([str(ct_img_path)])\n",
    "    \n",
    "    preprocessed, other_volumes_preprocessed, _ = preprocessor.run_case_npy(\n",
    "        img_np, seg=other_volumes, properties=img_props,\n",
    "        plans_manager=plans_manager,\n",
    "        configuration_manager=configuration_manager,\n",
    "        dataset_json=dataset_json_path\n",
    "    )\n",
    "    if other_volumes:\n",
    "        return preprocessed, other_volumes_preprocessed\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "2220c578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.947718Z",
     "iopub.status.busy": "2025-08-08T14:06:57.947472Z",
     "iopub.status.idle": "2025-08-08T14:06:57.957047Z",
     "shell.execute_reply": "2025-08-08T14:06:57.956319Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.947702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model directory; note that this is readonly in Kaggle environment\n",
    "if IN_KAGGLE:\n",
    "    model_dir = join(nnUNet_results, 'Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres')\n",
    "else:\n",
    "    model_dir = join(nnUNet_results, 'nnUNetTrainer__nnUNetPlans__3d_fullres')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5438ea",
   "metadata": {},
   "source": [
    "## Utility to export logits to a visualizable segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "0ee84330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.957955Z",
     "iopub.status.busy": "2025-08-08T14:06:57.957749Z",
     "iopub.status.idle": "2025-08-08T14:06:57.968924Z",
     "shell.execute_reply": "2025-08-08T14:06:57.968174Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.957941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from nnunetv2.configuration import default_num_processes\n",
    "from nnunetv2.inference.export_prediction import export_prediction_from_logits\n",
    "\n",
    "def export_logits_to_nifty_segmentation(\n",
    "    predictor,\n",
    "    volume_file: Path,\n",
    "    model_dir: str,\n",
    "    logits: Union[str, np.ndarray, torch.Tensor],\n",
    "    npz_dir: str | None,\n",
    "    output_dir: str = \"\",\n",
    "    fold: int = 0,\n",
    "    save_probs: bool = False,\n",
    "    from_file: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a saved .npz logits file into a native-space NIfTI segmentation using nnU-Net's helper.\n",
    "\n",
    "    Args:\n",
    "        predictor: An instantiated nnU-Net predictor object with loaded plans/configs.\n",
    "        volume_file: An Path object pointing to the raw image file.\n",
    "        model_dir (str): Path to the nnU-Net mode1Introductionl directory containing dataset.json.\n",
    "        logits (str or tensor): Base name of the .npz logits file (no extension), when from_file=True\n",
    "        npz_dir (str): Directory where the .npz file is stored.\n",
    "        output_dir (str): Directory where the .nii.gz segmentation will be saved.\n",
    "        fold (int): The fold number used for prediction (default is 0).\n",
    "        save_probs (bool): Whether to save softmax probabilities as a .npz file.\n",
    "        from_file (bool). Whether to convert from a file instead of from the logits (default true)\n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        npz_logits = Path(npz_dir) / f\"{logits}.npz\"\n",
    "        output_nii = Path(output_dir) / f\"{logits}_seg.nii.gz\"\n",
    "        logits = np.load(npz_logits)[\"logits\"]\n",
    "    else:\n",
    "        output_nii = Path(output_dir) / \"exported_seg.nii.gz\"\n",
    "\n",
    "    plans_manager = predictor.plans_manager\n",
    "    configuration_manager = predictor.configuration_manager\n",
    "    dataset_json = Path(model_dir) / \"dataset.json\"\n",
    "\n",
    "    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n",
    "    rw = plans_manager.image_reader_writer_class()\n",
    "    if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "        rw = rw()\n",
    "    img_np, img_props = rw.read_images([str(volume_file)])\n",
    "\n",
    "    _, _, data_props = preprocessor.run_case_npy(\n",
    "        img_np, seg=None, properties=img_props,\n",
    "        plans_manager=plans_manager,\n",
    "        configuration_manager=configuration_manager,\n",
    "        dataset_json=dataset_json\n",
    "    )\n",
    "\n",
    "\n",
    "    export_prediction_from_logits(\n",
    "        predicted_array_or_file=logits,\n",
    "        properties_dict=data_props,\n",
    "        configuration_manager=configuration_manager,\n",
    "        plans_manager=plans_manager,\n",
    "        dataset_json_dict_or_file=str(dataset_json),\n",
    "        output_file_truncated=os.path.splitext(str(output_nii))[0],\n",
    "        save_probabilities=save_probs,\n",
    "        num_threads_torch=default_num_processes\n",
    "    )\n",
    "\n",
    "    print(f\"âœ…  NIfTI segmentation written â†’ {output_nii}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c7e5ae55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:57.969930Z",
     "iopub.status.busy": "2025-08-08T14:06:57.969651Z",
     "iopub.status.idle": "2025-08-08T14:06:57.982253Z",
     "shell.execute_reply": "2025-08-08T14:06:57.981719Z",
     "shell.execute_reply.started": "2025-08-08T14:06:57.969905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'export_logits_to_nifty_segmentation(\\n    predictor=predictor,\\n    plan=plan,\\n    model_dir=Path(model_dir),\\n    logits_filename=\"pred_00007\",\\n    npz_dir=\"SHAP/shap_run\",\\n    output_dir=\"SHAP/shap_run\",\\n    fold=0,\\n    save_probs=False\\n)'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"export_logits_to_nifty_segmentation(\n",
    "    predictor=predictor,\n",
    "    plan=plan,\n",
    "    model_dir=Path(model_dir),\n",
    "    logits_filename=\"pred_00007\",\n",
    "    npz_dir=\"SHAP/shap_run\",\n",
    "    output_dir=\"SHAP/shap_run\",\n",
    "    fold=0,\n",
    "    save_probs=False\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ab0ae",
   "metadata": {},
   "source": [
    "## We define a sliding window caching for faster multi-inference scenario, like SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b1194",
   "metadata": {},
   "source": [
    "### Try to override the sliding_window_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b9dcd232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:35:31.984994Z",
     "iopub.status.busy": "2025-08-08T15:35:31.984719Z",
     "iopub.status.idle": "2025-08-08T15:35:32.013428Z",
     "shell.execute_reply": "2025-08-08T15:35:32.012797Z",
     "shell.execute_reply.started": "2025-08-08T15:35:31.984975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from acvl_utils.cropping_and_padding.padding import pad_nd_image\n",
    "from nnunetv2.utilities.helpers import empty_cache, dummy_context\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.inference.sliding_window_prediction import compute_gaussian, compute_steps_for_sliding_window\n",
    "\n",
    "class CustomNNUNetPredictor(nnUNetPredictor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def set_wandb_logging(self, wandb_logging: bool, wandb_label: str = \"\", wandb_commit: bool = True):\n",
    "        self.wandb_logging = wandb_logging\n",
    "        self.wandb_label = wandb_label\n",
    "        self.wandb_commit = wandb_commit\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def predict_sliding_window_return_logits_with_caching(self, input_image: torch.Tensor,\n",
    "                                                          perturbation_mask: torch.BoolTensor | None,\n",
    "                                                          baseline_prediction_dict: dict) \\\n",
    "            -> Union[np.ndarray, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Method predict_sliding_window_return_logits taken from official nnunetv2 documentation:\n",
    "        https://github.com/MIC-DKFZ/nnUNet/blob/58a3b121a6d1846a978306f6c79a7c005b7d669b/nnunetv2/inference/predict_from_raw_data.py\n",
    "        We add a perturbation_mask parameter to check each patch for the actual presence of a perturbation\n",
    "        \"\"\"\n",
    "        # fallback to original method if perturbation_mask is None\n",
    "        if perturbation_mask is None:\n",
    "            return self.predict_sliding_window_return_logits(input_image)\n",
    "                \n",
    "        assert isinstance(input_image, torch.Tensor)\n",
    "        self.network = self.network.to(self.device)\n",
    "        self.network.eval()\n",
    "\n",
    "        # Autocast can be annoying\n",
    "        # If the device_type is 'cpu' then it's slow as heck on some CPUs (no auto bfloat16 support detection)\n",
    "        # and needs to be disabled.\n",
    "        # If the device_type is 'mps' then it will complain that mps is not implemented, even if enabled=False\n",
    "        # is set. Whyyyyyyy. (this is why we don't make use of enabled=False)\n",
    "        # So autocast will only be active if we have a cuda device.\n",
    "        with torch.autocast(self.device.type, enabled=True) if self.device.type == 'cuda' else dummy_context():\n",
    "            assert input_image.ndim == 4, 'input_image must be a 4D np.ndarray or torch.Tensor (c, x, y, z)'\n",
    "\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'Input shape: {input_image.shape}')\n",
    "                print(\"step_size:\", self.tile_step_size)\n",
    "                print(\"mirror_axes:\", self.allowed_mirroring_axes if self.use_mirroring else None)\n",
    "                print(f'Perturbation mask shape: {perturbation_mask.shape}')\n",
    "\n",
    "            # DEBUG\n",
    "            \"\"\"# save as nifti the input image and the perturbation map\n",
    "            affine = nib.load(\"supervoxel_map.nii.gz\").affine\n",
    "            save_nifty(torch.permute(input_image[0], (2,1,0)), affine, \"input_image_debug.nii.gz\")\n",
    "            save_nifty(torch.permute(perturbation_mask[0], (2,1,0)), affine, \"perturbation_mask_debug.nii.gz\")\"\"\"\n",
    "\n",
    "            # if input_image is smaller than tile_size we need to pad it to tile_size.\n",
    "            data, slicer_revert_padding = pad_nd_image(input_image, self.configuration_manager.patch_size,\n",
    "                                                       'constant', {'value': 0}, True,\n",
    "                                                       None)\n",
    "\n",
    "            # slicers can be applied to both perturbed volume and \n",
    "            slicers = self._internal_get_sliding_window_slicers(data.shape[1:])\n",
    "\n",
    "            if self.perform_everything_on_device and self.device != 'cpu':\n",
    "                # behavior changed\n",
    "                try:\n",
    "                    predicted_logits = self._internal_predict_sliding_window_return_logits(\n",
    "                        data, slicers, True, perturbation_mask, baseline_prediction_dict, caching=True\n",
    "                    )\n",
    "                except RuntimeError as e:\n",
    "                    if \"CUDA out of memory\" in str(e):\n",
    "                        print(\"âš ï¸  CUDA OOM, cambiare batch size o patch size!\")\n",
    "                        raise\n",
    "                    else:\n",
    "                        # Mostra l'errore reale e aborta: niente CPU fallback\n",
    "                        raise\n",
    "            else:\n",
    "                predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n",
    "                                                                                       self.perform_everything_on_device)\n",
    "\n",
    "            empty_cache(self.device)\n",
    "            # revert padding\n",
    "            predicted_logits = predicted_logits[(slice(None), *slicer_revert_padding[1:])]\n",
    "        return predicted_logits\n",
    "                \n",
    "\n",
    "    def _slice_key(self, slicer_tuple):\n",
    "        # make slicer object hashable to use it for cache lookup\n",
    "        return tuple((s.start, s.stop, s.step) for s in slicer_tuple)\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _internal_predict_sliding_window_return_logits(self,\n",
    "                                                       data: torch.Tensor,\n",
    "                                                       slicers,\n",
    "                                                       do_on_device: bool = True,\n",
    "                                                       perturbation_mask: torch.BoolTensor | None = None,\n",
    "                                                       baseline_prediction_dict: dict | None = None,\n",
    "                                                       caching: bool = False,\n",
    "                                                       ):\n",
    "        \"\"\"\n",
    "        Modified to manage the caching of patches\n",
    "        \"\"\"\n",
    "        predicted_logits = n_predictions = prediction = gaussian = workon = None\n",
    "        results_device = self.device if do_on_device else torch.device('cpu')\n",
    "        if next(self.network.parameters()).device != results_device:\n",
    "            self.network = self.network.to(results_device)\n",
    "\n",
    "        def producer(d, slh, q):\n",
    "            for s in slh:\n",
    "                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(results_device), s))\n",
    "            q.put('end')\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data to device\n",
    "            if self.verbose:\n",
    "                print(f'move image to device {results_device}')\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            # preallocate arrays\n",
    "            if self.verbose:\n",
    "                print(f'preallocating results arrays on device {results_device}')\n",
    "            predicted_logits = torch.zeros((self.label_manager.num_segmentation_heads, *data.shape[1:]),\n",
    "                                           dtype=torch.half,\n",
    "                                           device=results_device)\n",
    "            n_predictions = torch.zeros(data.shape[1:], dtype=torch.half, device=results_device)\n",
    "\n",
    "            if self.use_gaussian:\n",
    "                gaussian = compute_gaussian(tuple(self.configuration_manager.patch_size), sigma_scale=1. / 8,\n",
    "                                            value_scaling_factor=10,\n",
    "                                            device=results_device)\n",
    "            else:\n",
    "                gaussian = 1\n",
    "\n",
    "        \n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            # Before starting queue processing:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                cache_hits = 0\n",
    "                cache_misses = 0\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    try:\n",
    "                        if caching and not self.check_overlapping(sl, perturbation_mask):\n",
    "                            prediction = baseline_prediction_dict[self._slice_key(sl)].to(results_device)\n",
    "                            cache_hits += 1\n",
    "                        else:\n",
    "                            prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "                            cache_misses += 1\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(\"Errore nella predizione del patch\") from e\n",
    "            \n",
    "                    assert prediction.device == predicted_logits.device\n",
    "            \n",
    "                    if self.use_gaussian:\n",
    "                        prediction *= gaussian\n",
    "                    predicted_logits[sl] += prediction\n",
    "                    n_predictions[sl[1:]] += gaussian\n",
    "            \n",
    "                    del prediction, workon\n",
    "                    queue.task_done()\n",
    "                    pbar.set_postfix(\n",
    "                        cache=f\"{cache_hits}\",\n",
    "                        mem=f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\"\n",
    "                    )\n",
    "                    pbar.update(1)\n",
    "            \n",
    "            queue.join()\n",
    "            \n",
    "            if caching:\n",
    "                # Final metrics\n",
    "                hit_ratio = (cache_hits / len(slicers) * 100) if len(slicers) > 0 else 0.0\n",
    "                elapsed_time = time.time() - start_time\n",
    "            \n",
    "                if self.verbose and not self.allow_tqdm:\n",
    "                    print(f\"Cache hits: {cache_hits}\\\\{len(slicers)}\")\n",
    "                    print(f\"Inference time: {elapsed_time:.2f} sec\")\n",
    "\n",
    "                # Log to W&B (only if you are logged and there is a run)\n",
    "                if self.wandb_logging:\n",
    "                    wandb.log({\n",
    "                        f\"{self.wandb_label}cache_hit_ratio_percent\": hit_ratio,\n",
    "                        f\"{self.wandb_label}inference_time_sec\": elapsed_time\n",
    "                    }, commit=self.wandb_commit)\n",
    "\n",
    "\n",
    "            # predicted_logits /= n_predictions\n",
    "            torch.div(predicted_logits, n_predictions, out=predicted_logits)\n",
    "            # check for infs\n",
    "            if torch.any(torch.isinf(predicted_logits)):\n",
    "                raise RuntimeError('Encountered inf in predicted array. Aborting... If this problem persists, '\n",
    "                                   'reduce value_scaling_factor in compute_gaussian or increase the dtype of '\n",
    "                                   'predicted_logits to fp32')\n",
    "        except Exception as e:\n",
    "            del predicted_logits, n_predictions, prediction, gaussian, workon\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return predicted_logits\n",
    "  \n",
    "\n",
    "\n",
    "    def get_output_dictionary_sliding_window(self, data: torch.Tensor, slicers,\n",
    "                                            do_on_device: bool = True,\n",
    "                                            ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        # create a dictionary that associates the output of the inference, to each slicer of the sliding window module\n",
    "        # this way we can set ready for cache the output for the untouched patches.\n",
    "        \"\"\"\n",
    "        \n",
    "        dictionary = dict()\n",
    "        prediction = workon = None\n",
    "        results_device = self.device if do_on_device else torch.device('cpu')\n",
    "        if next(self.network.parameters()).device != results_device:\n",
    "            self.network = self.network.to(results_device)\n",
    "\n",
    "        def producer(d, slh, q):\n",
    "            for s in slh:\n",
    "                #tqdm.write(f\"put patch {s} on queue\")    # dentro producer\n",
    "                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(self.device), s))\n",
    "            q.put('end')\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data and network to device\n",
    "            if self.verbose:\n",
    "                print(f'move image and model to device {results_device}')\n",
    "\n",
    "            self.network = self.network.to(results_device)\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "\n",
    "                    pred_cpu = pred_gpu.cpu()\n",
    "                    # save prediction in the dictionary\n",
    "                    dictionary[self._slice_key(sl)] = pred_cpu\n",
    "                    # immediately free gpu memory\n",
    "                    del pred_gpu\n",
    "                    \n",
    "                    queue.task_done()\n",
    "                    pbar.update()\n",
    "            queue.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            del workon#, prediction\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return dictionary\n",
    "\n",
    "\n",
    "\n",
    "    def check_overlapping(self, slicer, perturbation_mask: torch.BoolTensor) -> bool:\n",
    "        \"\"\"\n",
    "        Restituisce True se la patch definita da `slicer`\n",
    "        contiene almeno un voxel perturbato.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        slicer : tuple\n",
    "            Quello prodotto da `_internal_get_sliding_window_slicers`,\n",
    "            cioÃ¨ (slice(None), slice(x0,x1), slice(y0,y1), slice(z0,z1)).\n",
    "        perturbation_mask : torch.BoolTensor\n",
    "            Maschera (C, X, Y, Z) con True nei voxel da perturbare\n",
    "            (di solito C==1 o replicata sui canali).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True â†” almeno un voxel True nella patch.out\n",
    "        \"\"\"\n",
    "        # NB: il primo elemento del tuple Ã¨ sempre slice(None) (canali).\n",
    "        #     Lo manteniamo: non ha overhead e semplifica.\n",
    "        return perturbation_mask[slicer].any().item()\n",
    "\n",
    "    from datetime import datetime\n",
    "    \n",
    "    def log_cache_stats(log_file: str,\n",
    "                        total_patches: int,\n",
    "                        cache_hits: int,\n",
    "                        cache_misses: int,\n",
    "                        extra_info: dict | None = None):\n",
    "        \"\"\"\n",
    "        Append caching statistics to a JSON log file.\n",
    "    \n",
    "        Args:\n",
    "            log_file (str): Path to the JSON file.\n",
    "            total_patches (int): Total number of patches processed.\n",
    "            cache_hits (int): Number of cache hits.\n",
    "            cache_misses (int): Number of cache misses.\n",
    "            extra_info (dict, optional): Additional run-specific information to log.\n",
    "        \"\"\"\n",
    "        hit_ratio = (cache_hits / total_patches * 100) if total_patches > 0 else 0.0\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "            \"total_patches\": total_patches,\n",
    "            \"cache_hits\": cache_hits,\n",
    "            \"cache_misses\": cache_misses,\n",
    "            \"hit_ratio_percent\": round(hit_ratio, 2)\n",
    "        }\n",
    "    \n",
    "        if extra_info:\n",
    "            log_entry.update(extra_info)\n",
    "    \n",
    "        # Load existing logs\n",
    "        if os.path.exists(log_file):\n",
    "            try:\n",
    "                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    logs = json.load(f)\n",
    "                if not isinstance(logs, list):\n",
    "                    logs = []\n",
    "            except json.JSONDecodeError:\n",
    "                logs = []\n",
    "        else:\n",
    "            logs = []\n",
    "    \n",
    "        logs.append(log_entry)\n",
    "    \n",
    "        with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(logs, f, indent=2)\n",
    "    \n",
    "        print(f\"[CACHE] Logged stats to '{log_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe99033",
   "metadata": {},
   "source": [
    "# Next step: define regular, fixed size superpixels and try to compute the attributions of each of them\n",
    "So we also need to define a metric to compare, since segmentation explanations, differently from classification, is intrinsically ambiguous. For example, let's select a priori a single region of the segmentation output, and use the average of these pixels to compute the impact of perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5cf9e",
   "metadata": {},
   "source": [
    "### 4. Face-centered cubic (FCC) lattice induced supervoxel assignment\n",
    "-> more *isotropic* than simple cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99105ae",
   "metadata": {},
   "source": [
    "### 4.1 affine transformation to translate isotropy from voxel space into the original geometrical space (measured in mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0cf54",
   "metadata": {},
   "source": [
    "### 4.2 Try to apply the original algorithm FCC it to an affine transformed volume that has the same proportion as the .nii in the physical space. Transform->apply the algorithm to derive the map-> back transform the map onto the voxel space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "1503343f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:58.342392Z",
     "iopub.status.busy": "2025-08-08T14:06:58.341798Z",
     "iopub.status.idle": "2025-08-08T14:06:58.356891Z",
     "shell.execute_reply": "2025-08-08T14:06:58.356239Z",
     "shell.execute_reply.started": "2025-08-08T14:06:58.342370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def generate_FCC_supervoxel_map(img, S=200.0):\n",
    "    \"\"\"\n",
    "    Generate a supervoxel map using FCC tessellation in physical space (original version).\n",
    "    \n",
    "    Args:\n",
    "        img: Nibabel NIfTI image object\n",
    "        S (float): Desired supervoxel size in millimeters (default: 200.0)\n",
    "    \n",
    "    Returns:\n",
    "        supervoxel_map: 3D NumPy array with integer labels for supervoxels\n",
    "    \"\"\"\n",
    "    # Load volume and affine from image\n",
    "    volume = img.get_fdata()\n",
    "    affine = img.affine\n",
    "    W, H, D = volume.shape\n",
    "\n",
    "    # Compute the physical bounding box of the volume\n",
    "    corners_voxel = np.array([\n",
    "        [0, 0, 0],\n",
    "        [W-1, 0, 0],\n",
    "        [0, H-1, 0],\n",
    "        [0, 0, D-1],\n",
    "        [W-1, H-1, 0],\n",
    "        [W-1, 0, D-1],\n",
    "        [0, H-1, D-1],\n",
    "        [W-1, H-1, D-1]\n",
    "    ])\n",
    "    corners_hom = np.hstack((corners_voxel, np.ones((8, 1))))\n",
    "    corners_physical = (affine @ corners_hom.T).T[:, :3]\n",
    "    min_xyz = corners_physical.min(axis=0)\n",
    "    max_xyz = corners_physical.max(axis=0)\n",
    "\n",
    "    # Generate FCC lattice centers in physical space\n",
    "    a = S * np.sqrt(2)\n",
    "    factor = 2 / a\n",
    "    p_min = int(np.floor(factor * min_xyz[0])) - 1\n",
    "    p_max = int(np.ceil(factor * max_xyz[0])) + 1\n",
    "    q_min = int(np.floor(factor * min_xyz[1])) - 1\n",
    "    q_max = int(np.ceil(factor * max_xyz[1])) + 1\n",
    "    r_min = int(np.floor(factor * min_xyz[2])) - 1\n",
    "    r_max = int(np.ceil(factor * max_xyz[2])) + 1\n",
    "\n",
    "    # Create grid of possible indices\n",
    "    p_vals = np.arange(p_min, p_max + 1)\n",
    "    q_vals = np.arange(q_min, q_max + 1)\n",
    "    r_vals = np.arange(r_min, r_max + 1)\n",
    "    P, Q, R = np.meshgrid(p_vals, q_vals, r_vals, indexing='ij')\n",
    "    P = P.flatten()\n",
    "    Q = Q.flatten()\n",
    "    R = R.flatten()\n",
    "\n",
    "    # Filter for FCC lattice points (sum of indices is even)\n",
    "    mask = (P + Q + R) % 2 == 0\n",
    "    P = P[mask]\n",
    "    Q = Q[mask]\n",
    "    R = R[mask]\n",
    "\n",
    "    # Compute physical coordinates of centers\n",
    "    centers = np.column_stack((P * a / 2, Q * a / 2, R * a / 2))\n",
    "\n",
    "    # Keep only centers within the bounding box\n",
    "    inside = ((centers[:, 0] >= min_xyz[0]) & (centers[:, 0] <= max_xyz[0]) &\n",
    "              (centers[:, 1] >= min_xyz[1]) & (centers[:, 1] <= max_xyz[1]) &\n",
    "              (centers[:, 2] >= min_xyz[2]) & (centers[:, 2] <= max_xyz[2]))\n",
    "    centers = centers[inside]\n",
    "\n",
    "    # Check if any centers were generated\n",
    "    print(f\"Number of supervoxel centers: {len(centers)}\")\n",
    "    if len(centers) == 0:\n",
    "        raise ValueError(\"No supervoxel centers generated. Try reducing S.\")\n",
    "\n",
    "    # Generate voxel indices and transform to physical coordinates\n",
    "    voxel_indices = np.indices((W, H, D)).reshape(3, -1).T  # shape (W*H*D, 3)\n",
    "    voxel_indices_hom = np.hstack((voxel_indices, np.ones((voxel_indices.shape[0], 1))))  # shape (W*H*D, 4)\n",
    "    physical_coords = (affine @ voxel_indices_hom.T).T[:, :3]  # shape (W*H*D, 3)\n",
    "\n",
    "    # Assign each voxel to the nearest supervoxel center\n",
    "    tree = cKDTree(centers)\n",
    "    _, labels = tree.query(physical_coords)\n",
    "\n",
    "    # Create the supervoxel map\n",
    "    supervoxel_map = labels.reshape((W, H, D)).astype(np.int32)\n",
    "\n",
    "    return supervoxel_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0d8e7",
   "metadata": {},
   "source": [
    "## Combine FCC regularity with organ context: Organ-aware FCC supervoxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "fb6e218a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T16:10:36.624276Z",
     "iopub.status.busy": "2025-08-08T16:10:36.624066Z",
     "iopub.status.idle": "2025-08-08T16:10:36.647185Z",
     "shell.execute_reply": "2025-08-08T16:10:36.646435Z",
     "shell.execute_reply.started": "2025-08-08T16:10:36.624259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_FCC_organs_supervoxel_map(volume_img, organ_img, S, compute_statistics=False):\n",
    "    \"\"\"\n",
    "    Generate organ-aware supervoxel map using FCC tessellation.\n",
    "    \n",
    "    Args:\n",
    "        volume_img: Nibabel NIfTI image object (intensity volume)\n",
    "        organ_img: Nibabel NIfTI image object (organ labels, 0=background)\n",
    "        S: Desired supervoxel size in millimeters\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (supervoxel_map, organ_table, statistics)\n",
    "            - supervoxel_map: 3D array with supervoxel IDs (0=background)\n",
    "            - organ_table: dict {supervoxel_id: organ_label}\n",
    "            - statistics: dict with per-supervoxel statistics\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    volume = volume_img.get_fdata()\n",
    "    organs = organ_img.get_fdata().astype(np.int32)\n",
    "    affine = volume_img.affine\n",
    "    W, H, D = volume.shape\n",
    "    \n",
    "    # Verify dimensions match\n",
    "    if volume.shape != organs.shape:\n",
    "        raise ValueError(\"Volume and organ images must have the same dimensions\")\n",
    "    \n",
    "    # Compute physical bounding box\n",
    "    corners_voxel = np.array([\n",
    "        [0, 0, 0], [W-1, 0, 0], [0, H-1, 0], [0, 0, D-1],\n",
    "        [W-1, H-1, 0], [W-1, 0, D-1], [0, H-1, D-1], [W-1, H-1, D-1]\n",
    "    ])\n",
    "    corners_hom = np.hstack((corners_voxel, np.ones((8, 1))))\n",
    "    corners_physical = (affine @ corners_hom.T).T[:, :3]\n",
    "    min_xyz = corners_physical.min(axis=0)\n",
    "    max_xyz = corners_physical.max(axis=0)\n",
    "\n",
    "    # Generate FCC lattice centers in physical space\n",
    "    a = S * np.sqrt(2)\n",
    "    factor = 2 / a\n",
    "    p_min = int(np.floor(factor * min_xyz[0])) - 1\n",
    "    p_max = int(np.ceil(factor * max_xyz[0])) + 1\n",
    "    q_min = int(np.floor(factor * min_xyz[1])) - 1\n",
    "    q_max = int(np.ceil(factor * max_xyz[1])) + 1\n",
    "    r_min = int(np.floor(factor * min_xyz[2])) - 1\n",
    "    r_max = int(np.ceil(factor * max_xyz[2])) + 1\n",
    "\n",
    "    # Create FCC lattice points efficiently\n",
    "    centers = []\n",
    "    for p in range(p_min, p_max + 1):\n",
    "        for q in range(q_min, q_max + 1):\n",
    "            for r in range(r_min, r_max + 1):\n",
    "                if (p + q + r) % 2 == 0:  # FCC constraint\n",
    "                    center = np.array([p * a / 2, q * a / 2, r * a / 2])\n",
    "                    # Check if within bounding box\n",
    "                    if (min_xyz[0] <= center[0] <= max_xyz[0] and\n",
    "                        min_xyz[1] <= center[1] <= max_xyz[1] and\n",
    "                        min_xyz[2] <= center[2] <= max_xyz[2]):\n",
    "                        centers.append(center)\n",
    "    \n",
    "    centers = np.array(centers)\n",
    "    print(f\"Generated {len(centers)} FCC centers\")\n",
    "    \n",
    "    if len(centers) == 0:\n",
    "        raise ValueError(\"No FCC centers generated. Try reducing S.\")\n",
    "    \n",
    "    # Process volume slice by slice to save memory\n",
    "    supervoxel_map = np.zeros((W, H, D), dtype=np.int32)\n",
    "    fcc_to_organs = defaultdict(set)  # Maps FCC center index to set of organ labels\n",
    "    \n",
    "    tree = cKDTree(centers)\n",
    "    \n",
    "    print(\"Assigning voxels to FCC centers...\")\n",
    "    for w in range(W):\n",
    "        # Generate coordinates for this slice\n",
    "        y_coords, z_coords = np.meshgrid(np.arange(H), np.arange(D), indexing='ij')\n",
    "        slice_voxels = np.column_stack([\n",
    "            np.full(H * D, w),\n",
    "            y_coords.ravel(),\n",
    "            z_coords.ravel()\n",
    "        ])\n",
    "        \n",
    "        # Transform to physical space\n",
    "        slice_physical = (affine @ np.column_stack([slice_voxels, np.ones(H * D)]).T).T[:, :3]\n",
    "        \n",
    "        # Assign to nearest FCC center\n",
    "        _, fcc_labels = tree.query(slice_physical)\n",
    "        fcc_labels = fcc_labels.reshape(H, D)\n",
    "        \n",
    "        # Store FCC assignments and collect organ labels per FCC center\n",
    "        for h in range(H):\n",
    "            for d in range(D):\n",
    "                fcc_idx = fcc_labels[h, d]\n",
    "                organ_label = organs[w, h, d]\n",
    "                \n",
    "                if organ_label != 0:  # Skip background\n",
    "                    fcc_to_organs[fcc_idx].add(organ_label)\n",
    "                \n",
    "                # Temporarily store FCC index (will be converted to supervoxel ID later)\n",
    "                supervoxel_map[w, h, d] = fcc_idx\n",
    "    \n",
    "    # Create supervoxel IDs: each (FCC_center, organ) pair gets unique ID\n",
    "    print(\"Creating organ-aware supervoxels...\")\n",
    "    fcc_organ_to_supervoxel = {}\n",
    "    supervoxel_to_organ = {}\n",
    "    supervoxel_id = 1  # Start from 1 (0 reserved for background)\n",
    "    \n",
    "    for fcc_idx in sorted(fcc_to_organs.keys()):\n",
    "        for organ_label in sorted(fcc_to_organs[fcc_idx]):\n",
    "            fcc_organ_to_supervoxel[(fcc_idx, organ_label)] = supervoxel_id\n",
    "            supervoxel_to_organ[supervoxel_id] = organ_label\n",
    "            supervoxel_id += 1\n",
    "    \n",
    "    print(f\"Created {len(supervoxel_to_organ)} organ-aware supervoxels\")\n",
    "    \n",
    "    # Convert FCC assignments to final supervoxel IDs\n",
    "    final_supervoxel_map = np.zeros((W, H, D), dtype=np.int32)\n",
    "    \n",
    "    for w in range(W):\n",
    "        for h in range(H):\n",
    "            for d in range(D):\n",
    "                fcc_idx = supervoxel_map[w, h, d]\n",
    "                organ_label = organs[w, h, d]\n",
    "                \n",
    "                if organ_label != 0:  # Non-background\n",
    "                    key = (fcc_idx, organ_label)\n",
    "                    if key in fcc_organ_to_supervoxel:\n",
    "                        final_supervoxel_map[w, h, d] = fcc_organ_to_supervoxel[key]\n",
    "                # Background voxels remain 0\n",
    "\n",
    "    if compute_statistics:\n",
    "        # Compute statistics\n",
    "        print(\"Computing supervoxel statistics...\")\n",
    "        statistics = {}\n",
    "        voxel_volume_mm3 = np.abs(np.linalg.det(affine[:3, :3]))\n",
    "        \n",
    "        for sv_id in supervoxel_to_organ.keys():\n",
    "            mask = final_supervoxel_map == sv_id\n",
    "            if np.any(mask):\n",
    "                voxels = volume[mask]\n",
    "                statistics[sv_id] = {\n",
    "                    'organ_label': supervoxel_to_organ[sv_id],\n",
    "                    'voxel_count': int(np.sum(mask)),\n",
    "                    'volume_mm3': float(np.sum(mask) * voxel_volume_mm3),\n",
    "                    'mean_intensity': float(np.mean(voxels)),\n",
    "                    'std_intensity': float(np.std(voxels)),\n",
    "                    'min_intensity': float(np.min(voxels)),\n",
    "                    'max_intensity': float(np.max(voxels)),\n",
    "                    'centroid_voxel': [float(x) for x in np.array(np.where(mask)).mean(axis=1)]\n",
    "                }\n",
    "        \n",
    "        # Add background statistics\n",
    "        background_mask = final_supervoxel_map == 0\n",
    "        if np.any(background_mask):\n",
    "            bg_voxels = volume[background_mask]\n",
    "            statistics[0] = {\n",
    "                'organ_label': 0,\n",
    "                'voxel_count': int(np.sum(background_mask)),\n",
    "                'volume_mm3': float(np.sum(background_mask) * voxel_volume_mm3),\n",
    "                'mean_intensity': float(np.mean(bg_voxels)),\n",
    "                'std_intensity': float(np.std(bg_voxels)),\n",
    "                'min_intensity': float(np.min(bg_voxels)),\n",
    "                'max_intensity': float(np.max(bg_voxels)),\n",
    "                'centroid_voxel': [float(x) for x in np.array(np.where(background_mask)).mean(axis=1)]\n",
    "            }\n",
    "        \n",
    "        # Print summary\n",
    "        organ_counts = defaultdict(int)\n",
    "        for sv_id, organ_label in supervoxel_to_organ.items():\n",
    "            organ_counts[organ_label] += 1\n",
    "        \n",
    "        print(\"\\nSupervoxel summary:\")\n",
    "        print(f\"Background supervoxels: 1 (ID: 0)\")\n",
    "        for organ_label in sorted(organ_counts.keys()):\n",
    "            print(f\"Organ {organ_label}: {organ_counts[organ_label]} supervoxels\")\n",
    "    else:\n",
    "        supervoxel_to_organ, statistics = None, None\n",
    "    \n",
    "    return final_supervoxel_map, supervoxel_to_organ, statistics\n",
    "\n",
    "\n",
    "def save_organ_supervoxel_results(supervoxel_map, organ_table, statistics, \n",
    "                                volume_img, output_prefix=\"organ_supervoxels\"):\n",
    "    \"\"\"\n",
    "    Save supervoxel results to files.\n",
    "    \n",
    "    Args:\n",
    "        supervoxel_map: 3D array with supervoxel IDs\n",
    "        organ_table: dict mapping supervoxel_id -> organ_label  \n",
    "        statistics: dict with supervoxel statistics\n",
    "        volume_img: original volume image (for affine/header)\n",
    "        output_prefix: prefix for output files\n",
    "    \"\"\"\n",
    "    # Save supervoxel map as NIfTI\n",
    "    supervoxel_img = nib.Nifti1Image(supervoxel_map.astype(np.int32), \n",
    "                                   volume_img.affine, volume_img.header)\n",
    "    supervoxel_img.header.set_data_dtype(np.int32)\n",
    "    nib.save(supervoxel_img, f\"{output_prefix}_map.nii.gz\")\n",
    "    print(f\"Saved supervoxel map: {output_prefix}_map.nii.gz\")\n",
    "    \n",
    "    # Save organ table as CSV\n",
    "    import csv\n",
    "    with open(f\"{output_prefix}_table.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['supervoxel_id', 'organ_label'])\n",
    "        writer.writerow([0, 0])  # Background\n",
    "        for sv_id in sorted(organ_table.keys()):\n",
    "            writer.writerow([sv_id, organ_table[sv_id]])\n",
    "    print(f\"Saved organ table: {output_prefix}_table.csv\")\n",
    "    \n",
    "    # Save statistics as CSV\n",
    "    with open(f\"{output_prefix}_stats.csv\", 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = ['supervoxel_id', 'organ_label', 'voxel_count', 'volume_mm3',\n",
    "                 'mean_intensity', 'std_intensity', 'min_intensity', 'max_intensity',\n",
    "                 'centroid_x', 'centroid_y', 'centroid_z']\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        for sv_id in sorted(statistics.keys()):\n",
    "            stats = statistics[sv_id]\n",
    "            row = [sv_id, stats['organ_label'], stats['voxel_count'], stats['volume_mm3'],\n",
    "                  stats['mean_intensity'], stats['std_intensity'], \n",
    "                  stats['min_intensity'], stats['max_intensity']] + stats['centroid_voxel']\n",
    "            writer.writerow(row)\n",
    "    print(f\"Saved statistics: {output_prefix}_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c80b34b",
   "metadata": {},
   "source": [
    "## Try SLIC for visual context aware supervoxels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c40f92",
   "metadata": {},
   "source": [
    "### define a preprocessing routine to enhance SLIC results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "203adfef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:58.382168Z",
     "iopub.status.busy": "2025-08-08T14:06:58.381723Z",
     "iopub.status.idle": "2025-08-08T14:06:58.395847Z",
     "shell.execute_reply": "2025-08-08T14:06:58.395288Z",
     "shell.execute_reply.started": "2025-08-08T14:06:58.382134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.exposure import equalize_adapthist\n",
    "\n",
    "def preprocessing_for_SLIC(data: np.ndarray):\n",
    "    # 2ï¸âƒ£ Clip extreme intensities (e.g. 0.5%â€“99.5% quantiles) for contrast enhancement\n",
    "    vmin, vmax = np.quantile(data, (0.005, 0.995))\n",
    "    data = np.clip(data, vmin, vmax)\n",
    "    data = exposure.rescale_intensity(data, in_range=(vmin, vmax), out_range=(0, 1))  #  [oai_citation:0â€¡scikit-image.org](https://scikit-image.org/docs/0.25.x/api/skimage.segmentation.html?utm_source=chatgpt.com) [oai_citation:1â€¡researchgate.net](https://www.researchgate.net/publication/330691413_A_novel_technique_for_analysing_histogram_equalized_medical_images_using_superpixels?utm_source=chatgpt.com) [oai_citation:2â€¡arxiv.org](https://arxiv.org/abs/2204.05278?utm_source=chatgpt.com) [oai_citation:3â€¡scikit-image.org](https://scikit-image.org/skimage-tutorials/lectures/three_dimensional_image_processing.html?utm_source=chatgpt.com)\n",
    "\n",
    "    sigma_vox = np.array([1.0 / s for s in spacing])  # blur by 1 mm across axes\n",
    "    data = gaussian_filter(data, sigma=sigma_vox)\n",
    "\n",
    "    data = exposure.equalize_hist(data)\n",
    "\n",
    "    # Apply slice-wise CLAHE for 3D volume\n",
    "    data = np.stack([equalize_adapthist(slice_, clip_limit=0.03)\n",
    "                       for slice_ in data], axis=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "65dba8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:58.396789Z",
     "iopub.status.busy": "2025-08-08T14:06:58.396552Z",
     "iopub.status.idle": "2025-08-08T14:06:58.409851Z",
     "shell.execute_reply": "2025-08-08T14:06:58.409187Z",
     "shell.execute_reply.started": "2025-08-08T14:06:58.396767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "96f953b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:58.410812Z",
     "iopub.status.busy": "2025-08-08T14:06:58.410574Z",
     "iopub.status.idle": "2025-08-08T14:06:58.419671Z",
     "shell.execute_reply": "2025-08-08T14:06:58.419053Z",
     "shell.execute_reply.started": "2025-08-08T14:06:58.410792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parametri SLIC: n_segments definisce quanti supervoxels circa si voglion\n",
    "apply_SLIC = lambda data, spacing, n_supervoxels: slic(\n",
    "                preprocessing_for_SLIC(data), \n",
    "                n_segments=n_supervoxels, \n",
    "                compactness=0.2,\n",
    "                spacing=spacing,\n",
    "                start_label=0,\n",
    "                max_num_iter=10, \n",
    "                channel_axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c5189",
   "metadata": {},
   "source": [
    "### Initialize predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "43579e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T15:57:25.257544Z",
     "iopub.status.busy": "2025-08-08T15:57:25.257244Z",
     "iopub.status.idle": "2025-08-08T15:57:26.152965Z",
     "shell.execute_reply": "2025-08-08T15:57:26.152331Z",
     "shell.execute_reply.started": "2025-08-08T15:57:25.257499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n",
      "  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n"
     ]
    }
   ],
   "source": [
    "# 2) Initialise predictors ------------------------\n",
    "morf_predictor = CustomNNUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=False, # == test time augmentation\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda', 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=False #it interfere with SHAP loading bar\n",
    ")\n",
    "\n",
    "lerf_predictor = CustomNNUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=False, # == test time augmentation\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda', 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=False #it interfere with SHAP loading bar\n",
    ")\n",
    "# initializes the network architecture, loads the checkpoint\n",
    "morf_predictor.initialize_from_trained_model_folder(\n",
    "    model_dir,\n",
    "    use_folds=(0,),\n",
    "    checkpoint_name='checkpoint_final.pth',\n",
    ")\n",
    "\n",
    "lerf_predictor.initialize_from_trained_model_folder(\n",
    "    model_dir,\n",
    "    use_folds=(0,),\n",
    "    checkpoint_name='checkpoint_final.pth',\n",
    ")\n",
    "\n",
    "# generic predictor for some general operations (baseline computation etc)\n",
    "predictor = morf_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea278ef9",
   "metadata": {},
   "source": [
    "### some utils for Nifti geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d0ee2218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.429096Z",
     "iopub.status.busy": "2025-08-08T14:06:59.428857Z",
     "iopub.status.idle": "2025-08-08T14:06:59.433029Z",
     "shell.execute_reply": "2025-08-08T14:06:59.432177Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.429078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_spacing(ct_img_data: nib.nifti1.Nifti1Image):\n",
    "    \"\"\"affine = ct_img_data.affine\n",
    "    spacing = np.sqrt(np.sum(affine[:3, :3] ** 2, axis=0))\"\"\"\n",
    "    # this is equivalent to\n",
    "    spacing = ct_img_data.header.get_zooms() # get_zooms returns (x, y, z) spacing\n",
    "    return spacing\n",
    "\n",
    "def get_origin(ct_img_data: nib.nifti1.Nifti1Image):\n",
    "    affine = ct_img_data.affine\n",
    "    origin = affine[:3, 3]\n",
    "    return origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089768e",
   "metadata": {},
   "source": [
    "# Define a ROI to explain segmentation in. \n",
    "Maybe this will provide a more useful attribution map, highlighting nearby organs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7a0a43ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.434892Z",
     "iopub.status.busy": "2025-08-08T14:06:59.434458Z",
     "iopub.status.idle": "2025-08-08T14:06:59.446476Z",
     "shell.execute_reply": "2025-08-08T14:06:59.445890Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.434870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# if true, we will use a json file containing the bounding box for the Region of Interest (ROI), otherwise we will use a masked segmentation\\nROI_TYPE = \"BoundingBox\"  # \"BoundingBox\" or \"MaskedSegmentation'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# if true, we will use a json file containing the bounding box for the Region of Interest (ROI), otherwise we will use a masked segmentation\n",
    "ROI_TYPE = \"BoundingBox\"  # \"BoundingBox\" or \"MaskedSegmentation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "3f927bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.447437Z",
     "iopub.status.busy": "2025-08-08T14:06:59.447200Z",
     "iopub.status.idle": "2025-08-08T14:06:59.456107Z",
     "shell.execute_reply": "2025-08-08T14:06:59.455496Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.447415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#BB_ROI = json.load(open(BB_ROI_paths[volume_codes[1]], \"r\")) if ROI_TYPE == \"BoundingBox\" else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c67e3b",
   "metadata": {},
   "source": [
    "### ROI mask is a binary mask highlighting the lymphnodes of interest. We need a bounding box to crop the volume accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4b53e224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.457007Z",
     "iopub.status.busy": "2025-08-08T14:06:59.456784Z",
     "iopub.status.idle": "2025-08-08T14:06:59.468089Z",
     "shell.execute_reply": "2025-08-08T14:06:59.467467Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.456992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def get_mask_bbox_slices(mask_nii_path):\n",
    "    \"\"\"\n",
    "    Load a binary ROI mask NIfTI and compute the minimal 3D bounding\n",
    "    box slices containing all positive voxels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask_nii_path : str or Path\n",
    "        Path to the input binary ROI mask NIfTI (.nii or .nii.gz).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3-tuple of Python slice objects (x_slice, y_slice, z_slice)\n",
    "        defining the minimal bounding box.\n",
    "    \"\"\"\n",
    "    # 1) Load mask\n",
    "    nii = nib.load(str(mask_nii_path))\n",
    "    data = nii.get_fdata()\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input NIfTI must be a 3D volume\")\n",
    "    \n",
    "    # 2) Find indices of positive voxels\n",
    "    pos_voxels = np.argwhere(data > 0)\n",
    "    if pos_voxels.size == 0:\n",
    "        raise ValueError(\"No positive voxels found in mask\")\n",
    "    \n",
    "    # 3) Compute min/max per axis\n",
    "    x_min, y_min, z_min = pos_voxels.min(axis=0)\n",
    "    x_max, y_max, z_max = pos_voxels.max(axis=0)\n",
    "    \n",
    "    # 4) Build slice objects (end is exclusive, hence +1)\n",
    "    bbox_slices = (\n",
    "        slice(int(x_min), int(x_max) + 1),\n",
    "        slice(int(y_min), int(y_max) + 1),\n",
    "        slice(int(z_min), int(z_max) + 1),\n",
    "    )\n",
    "    \n",
    "    return bbox_slices\n",
    "\n",
    "def get_slices_from_BB_ROI(BB_ROI: dict) -> Tuple[slice, slice, slice]:\n",
    "    \"\"\"\n",
    "    Extracts the bounding box slices from the BB_ROI dictionary.\n",
    "    Parameters\n",
    "    ----------\n",
    "    BB_ROI : dict\n",
    "        Dictionary containing the bounding box coordinates with keys:\n",
    "        example:\n",
    "        {'FileFormat': array('MITK ROI', dtype='<U8'), 'Version': array(2), 'Geometry': array({'Size': [512.0, 512.0, 221.0], 'Transform': [1.3671875, -0.0, -0.0, 0, -0.0, 1.3671875, -0.0, 0, 0.0, 0.0, 5.0, 0, -350.0, -278.6000061035156, -432.239990234375, 1]},\n",
    "      dtype=object), 'ROIs': array([{'ID': 0, 'Max': [370.5, 291.5, 114.5], 'Min': [276.5, 193.49999999999994, 92.5], 'Properties': {'ColorProperty': {'color': [1.0, 0.0, 0.0]}, 'StringProperty': {'name': 'AUTOMI_00004_0000 Bounding Box'}}}],\n",
    "      dtype=object)}\n",
    "    \"\"\"\n",
    "    # take the ceil of both min and max (ROI in MITK apprently is shifted by 0.5 voxels)\n",
    "    x_slice = slice(int(np.ceil(BB_ROI['ROIs'][0]['Min'][0])), int(np.ceil(BB_ROI['ROIs'][0]['Max'][0])))\n",
    "    y_slice = slice(int(np.ceil(BB_ROI['ROIs'][0]['Min'][1])), int(np.ceil(BB_ROI['ROIs'][0]['Max'][1])))\n",
    "    z_slice = slice(int(np.ceil(BB_ROI['ROIs'][0]['Min'][2])), int(np.ceil(BB_ROI['ROIs'][0]['Max'][2])))\n",
    "    return x_slice, y_slice, z_slice\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "555b2a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.469227Z",
     "iopub.status.busy": "2025-08-08T14:06:59.468980Z",
     "iopub.status.idle": "2025-08-08T14:06:59.484437Z",
     "shell.execute_reply": "2025-08-08T14:06:59.483849Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.469202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "def load_roi_slices(\n",
    "    ROI_type: str,\n",
    "    ROI_BB_path: Optional[str] = None,\n",
    "    ROI_segmentation_mask_path: Optional[str] = None\n",
    ") -> Tuple[slice, slice, slice]:\n",
    "    \"\"\"\n",
    "    Load ROI definition and return bounding-box slices in x, y, z order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ROI_type : {\"BoundingBox\", \"MaskedSegmentation\"}\n",
    "        Type of ROI to load.\n",
    "    ROI_BB_path : str, optional\n",
    "        Path to JSON file defining the bounding box ROI.\n",
    "    ROI_segmentation_mask_path : str, optional\n",
    "        Path to NIfTI file defining the ROI segmentation mask.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_slice, y_slice, z_slice : slice\n",
    "        Slices along each axis for cropping operations.\n",
    "    \"\"\"\n",
    "    if ROI_type == \"BoundingBox\":\n",
    "        if ROI_BB_path is None:\n",
    "            raise ValueError(\"ROI_BB_path must be provided when ROI_type is 'BoundingBox'.\")\n",
    "        BB_ROI = json.load(open(ROI_BB_path, \"r\"))\n",
    "        x_slice, y_slice, z_slice = get_slices_from_BB_ROI(BB_ROI)\n",
    "\n",
    "    elif ROI_type == \"MaskedSegmentation\":\n",
    "        if ROI_segmentation_mask_path is None:\n",
    "            raise ValueError(\"ROI_segmentation_mask_path must be provided when ROI_type is 'MaskedSegmentation'.\")\n",
    "        # Optionally inspect mask details if needed:\n",
    "        mask_img = nib.load(ROI_segmentation_mask_path)\n",
    "        print(f\"Loaded mask shape: {mask_img.shape}, affine: {mask_img.affine}\")\n",
    "        x_slice, y_slice, z_slice = get_mask_bbox_slices(ROI_segmentation_mask_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported ROI_type: {ROI_type}\")\n",
    "\n",
    "    return x_slice, y_slice, z_slice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bde5f",
   "metadata": {},
   "source": [
    "### the identified region is our ROI bounding box in case we use the masked segmentation as manually derived ROI\n",
    "### otherwise we just use the ROI bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "55c5b2b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.485370Z",
     "iopub.status.busy": "2025-08-08T14:06:59.485153Z",
     "iopub.status.idle": "2025-08-08T14:06:59.497887Z",
     "shell.execute_reply": "2025-08-08T14:06:59.497202Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.485354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def slices_to_binary_mask(volume_shape, bbox_slices, dtype=np.uint8):\n",
    "    \"\"\"\n",
    "    Create a binary mask of given shape where voxels inside the provided\n",
    "    3D boundingâ€box slices are set to 1, and all others to 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume_shape : tuple of int\n",
    "        The full 3D volume dimensions, e.g. (X, Y, Z).\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3â€tuple of slice objects (x_slice, y_slice, z_slice) defining\n",
    "        the region to mask.\n",
    "    dtype : dataâ€type, optional\n",
    "        The desired dataâ€type of the output mask (default: np.uint8).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : np.ndarray\n",
    "        A binary mask array of shape `volume_shape`, with ones in the\n",
    "        region defined by `bbox_slices` and zeros elsewhere.\n",
    "    \"\"\"\n",
    "    if len(volume_shape) != len(bbox_slices):\n",
    "        raise ValueError(f\"volume_shape has {len(volume_shape)} dimensions, \"\n",
    "                         f\"but bbox_slices has {len(bbox_slices)} slices\")\n",
    "\n",
    "    # Initialize mask to zeros\n",
    "    mask = np.zeros(volume_shape, dtype=dtype)\n",
    "    # Set the boundingâ€box region to 1\n",
    "    mask[bbox_slices] = 1\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ef654af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.499076Z",
     "iopub.status.busy": "2025-08-08T14:06:59.498901Z",
     "iopub.status.idle": "2025-08-08T14:06:59.513331Z",
     "shell.execute_reply": "2025-08-08T14:06:59.512687Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.499057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_roi_mask(\n",
    "    volume_path: str,\n",
    "    bbox_slices: Tuple[slice, slice, slice],\n",
    "    output_path: Optional[str] = \"ROI_binary_mask.nii.gz\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a binary ROI mask based on bounding-box slices and save as NIfTI.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume_path : str\n",
    "        Path to the input volume NIfTI file.\n",
    "    bbox_slices : tuple of slice\n",
    "        Slices (x_slice, y_slice, z_slice) defining the ROI bounding box.\n",
    "    output_path : str, optional\n",
    "        Path where the binary mask NIfTI will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_path : str\n",
    "        Path to the saved binary mask file.\n",
    "    \"\"\"\n",
    "    img = nib.load(volume_path)\n",
    "    volume_shape = img.get_fdata().shape\n",
    "    affine = img.affine\n",
    "\n",
    "    # Generate binary mask array\n",
    "    mask_array = slices_to_binary_mask(\n",
    "        volume_shape=volume_shape,\n",
    "        bbox_slices=bbox_slices\n",
    "    )\n",
    "\n",
    "    # Create and save NIfTI mask\n",
    "    save_nifty_binary(mask_array, affine, output_path)\n",
    "    print(f\"Saved binary ROI mask to {output_path}\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d507c7c",
   "metadata": {},
   "source": [
    "### to **crop** correctly the volume around the *ROI*, we need to derive the **receptive field** of the sliding window inference, that depends on the *patch size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d27c65d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.514239Z",
     "iopub.status.busy": "2025-08-08T14:06:59.514001Z",
     "iopub.status.idle": "2025-08-08T14:06:59.527259Z",
     "shell.execute_reply": "2025-08-08T14:06:59.526736Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.514217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patch_size = np.array(predictor.configuration_manager.patch_size)\\nprint(\"Patch size: \", patch_size)\\n\\n# Receptive field is twice the patch size-1\\nRF = 2*(patch_size-1)\\nprint(\"Receptive field: \", RF)'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"patch_size = np.array(predictor.configuration_manager.patch_size)\n",
    "print(\"Patch size: \", patch_size)\n",
    "\n",
    "# Receptive field is twice the patch size-1\n",
    "RF = 2*(patch_size-1)\n",
    "print(\"Receptive field: \", RF)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee54246",
   "metadata": {},
   "source": [
    "### Consider the receptive field to compute the final slices for cropping\n",
    "Remember that model metadata are related to transposed volume (nnunetv2 takes (D, H, W) shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "1b2d2ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.528069Z",
     "iopub.status.busy": "2025-08-08T14:06:59.527877Z",
     "iopub.status.idle": "2025-08-08T14:06:59.538339Z",
     "shell.execute_reply": "2025-08-08T14:06:59.537691Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.528048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_rf_slices(\n",
    "    bbox_slices: Tuple[slice, slice, slice],\n",
    "    patch_size: np.ndarray,\n",
    "    volume_shape: Tuple[int, int, int]\n",
    ") -> Tuple[slice, slice, slice]:\n",
    "    \"\"\"\n",
    "    Compute bounding-box slices expanded by the model's receptive field.\n",
    "\n",
    "    RF is defined as 2*(patch_size - 1), and applied symmetrically.\n",
    "    \"\"\"\n",
    "    x_slice, y_slice, z_slice = bbox_slices\n",
    "    W, H, D = volume_shape\n",
    "\n",
    "    # receptive field along each axis (model input axes reversed)\n",
    "    RF = 2 * (patch_size - 1)\n",
    "    RF_z, RF_y, RF_x = RF  # expect patch_size as [D,H,W]\n",
    "\n",
    "    x_start = max(x_slice.start - RF_x // 2, 0)\n",
    "    x_stop = min(x_slice.stop + RF_x // 2, W)\n",
    "    y_start = max(y_slice.start - RF_y // 2, 0)\n",
    "    y_stop = min(y_slice.stop + RF_y // 2, H)\n",
    "    z_start = max(z_slice.start - RF_z // 2, 0)\n",
    "    z_stop = min(z_slice.stop + RF_z // 2, D)\n",
    "\n",
    "    return (slice(x_start, x_stop), slice(y_start, y_stop), slice(z_start, z_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "bdaa31ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.539293Z",
     "iopub.status.busy": "2025-08-08T14:06:59.539056Z",
     "iopub.status.idle": "2025-08-08T14:06:59.552327Z",
     "shell.execute_reply": "2025-08-08T14:06:59.551838Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.539272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def crop_volume_and_affine(nii_path, bbox_slices, save_cropped_nii_path=None):\n",
    "    \"\"\"\n",
    "    Crop a 3D NIfTI volume using the given bounding-box slices and\n",
    "    recompute the affine so the cropped volume retains correct world coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nii_path : str or Path\n",
    "        Path to the input NIfTI volume (.nii or .nii.gz).\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3-tuple (x_slice, y_slice, z_slice) as returned by get_mask_bbox_slices().\n",
    "    save_cropped_nii_path : str or Path, optional\n",
    "        If provided, the cropped volume will be saved here as a new NIfTI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_data : np.ndarray\n",
    "        The volume data cropped to the bounding box.\n",
    "    new_affine : np.ndarray\n",
    "        The updated 4Ã—4 affine transform for the cropped volume.\n",
    "    \"\"\"\n",
    "    # 1) Load the original image\n",
    "    img = nib.load(str(nii_path))\n",
    "    data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "\n",
    "    # 2) Crop the data array\n",
    "    cropped_data = data[bbox_slices]\n",
    "\n",
    "    # 3) Extract the voxelâ€offsets for x, y, z from the slice starts\n",
    "    x_slice, y_slice, z_slice = bbox_slices\n",
    "    z0, y0, x0 = z_slice.start, y_slice.start, x_slice.start\n",
    "\n",
    "    # 4) Compute the new affine translation: shift the origin by the voxel offsets\n",
    "    # Note voxel coordinates are (i, j, k) = (x, y, z)\n",
    "    offset_vox = np.array([x0, y0, z0])\n",
    "    new_affine = affine.copy()\n",
    "    new_affine[:3, 3] += affine[:3, :3].dot(offset_vox)\n",
    "\n",
    "    # 5) Optionally save the cropped volume\n",
    "    if save_cropped_nii_path is not None:\n",
    "        cropped_img = nib.Nifti1Image(cropped_data, new_affine)\n",
    "        nib.save(cropped_img, str(save_cropped_nii_path))\n",
    "\n",
    "    return cropped_data, new_affine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc8614",
   "metadata": {},
   "source": [
    "### We need a way to check original mask overlapping in the new cropped volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90728d9e",
   "metadata": {},
   "source": [
    "### We also need a mask to correctly ignoring out-of ROI context in our aggregation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "50bb83d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.553371Z",
     "iopub.status.busy": "2025-08-08T14:06:59.553111Z",
     "iopub.status.idle": "2025-08-08T14:06:59.566367Z",
     "shell.execute_reply": "2025-08-08T14:06:59.565817Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.553350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "def crop_volume_with_rf(\n",
    "    volume_path: str,\n",
    "    bbox_slices: Tuple[slice, slice, slice],\n",
    "    patch_size: np.ndarray,\n",
    "    output_dir: Optional[str] = \".\"\n",
    ") -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Crop the volume and binary ROI mask to the ROI padded by receptive field.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "      - 'cropped_volume'\n",
    "      - 'affine_cropped_volume'\n",
    "      - 'cropped_roi_mask'\n",
    "      - 'affine_cropped_mask'\n",
    "    \"\"\"\n",
    "    volume_img = nib.load(volume_path)\n",
    "    volume_shape = volume_img.get_fdata().shape\n",
    "\n",
    "    # compute RF-expanded slices\n",
    "    padded_slices = compute_rf_slices(bbox_slices, patch_size, volume_shape)\n",
    "\n",
    "    # ensure output directory exists\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # crop volume\n",
    "    cropped_vol_path = out_dir / \"cropped_volume.nii.gz\"\n",
    "    cropped_volume, affine_cropped_volume = crop_volume_and_affine(\n",
    "        nii_path=volume_path,\n",
    "        bbox_slices=padded_slices,\n",
    "        save_cropped_nii_path=cropped_vol_path\n",
    "    )\n",
    "\n",
    "    # crop ROI mask\n",
    "    roi_mask_path = out_dir / \"ROI_binary_mask.nii.gz\"\n",
    "    cropped_mask, affine_cropped_mask = crop_volume_and_affine(\n",
    "        nii_path=str(roi_mask_path),\n",
    "        bbox_slices=padded_slices,\n",
    "        save_cropped_nii_path=out_dir / \"cropped_mask_with_RF.nii.gz\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"cropped_volume\": (cropped_volume, affine_cropped_volume),\n",
    "        \"cropped_roi_mask\": (cropped_mask, affine_cropped_mask)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5bdb8",
   "metadata": {},
   "source": [
    "## We execute SHAP on this cropped image, and we only consider our ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b05a1ca",
   "metadata": {},
   "source": [
    "### set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "e7fbaabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.567255Z",
     "iopub.status.busy": "2025-08-08T14:06:59.567048Z",
     "iopub.status.idle": "2025-08-08T14:06:59.582211Z",
     "shell.execute_reply": "2025-08-08T14:06:59.581435Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.567233Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6e74e",
   "metadata": {},
   "source": [
    "### Preprocessing (skipped for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "459dce8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.583211Z",
     "iopub.status.busy": "2025-08-08T14:06:59.582975Z",
     "iopub.status.idle": "2025-08-08T14:06:59.593369Z",
     "shell.execute_reply": "2025-08-08T14:06:59.592738Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.583192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#NNUNET_PREPROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fced347e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.594293Z",
     "iopub.status.busy": "2025-08-08T14:06:59.594110Z",
     "iopub.status.idle": "2025-08-08T14:06:59.604788Z",
     "shell.execute_reply": "2025-08-08T14:06:59.604007Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.594279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_volume(\n",
    "    cropped_volume_path: str,\n",
    "    predictor,\n",
    "    dataset_json_path: str,\n",
    "    use_nnunet: bool = True\n",
    ") -> Tuple[np.ndarray, nib.Nifti1Image]:\n",
    "    \"\"\"\n",
    "    Preprocess the cropped volume for nnU-Net inference.\n",
    "\n",
    "    If `use_nnunet` is True, applies nnU-Net's default preprocessing.\n",
    "    Otherwise, loads the volume directly and reformats axes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    volume_np : np.ndarray\n",
    "        Array of shape (1, C, D, H, W) ready for torch conversion.\n",
    "    saved_volume_nii : nib.Nifti1Image\n",
    "        NIfTI image object of the preprocessed volume (for debugging or saving).\n",
    "    \"\"\"\n",
    "    if use_nnunet:\n",
    "        # nnU-Net preprocessing utility, assumed imported\n",
    "        volume_np = nnunetv2_default_preprocessing(\n",
    "            cropped_volume_path,\n",
    "            predictor,\n",
    "            dataset_json_path\n",
    "        )  # expects (C, D, H, W)\n",
    "        # Save NIfTI for debugging\n",
    "        volume_transposed = volume_np.squeeze().transpose(2, 1, 0) # back to (W,H,D)\n",
    "        affine = nib.load(cropped_volume_path).affine\n",
    "        path = Path(cropped_volume_path).replace(suffix=\"_preproc.nii.gz\")\n",
    "        save_nifty(volume_transposed, affine, path)\n",
    "\n",
    "    else:\n",
    "        cropped = nib.load(cropped_volume_path)\n",
    "        data = cropped.get_fdata()\n",
    "        data = np.transpose(data, (2, 1, 0))  # (D,H,W)\n",
    "        data = np.expand_dims(data, axis=0)   # (1, D,H,W)\n",
    "        volume_np = data\n",
    "        volume_nii = None\n",
    "\n",
    "    return volume_np, volume_nii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f894e7d",
   "metadata": {},
   "source": [
    "### Supervoxels subdivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3f194fa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.605592Z",
     "iopub.status.busy": "2025-08-08T14:06:59.605391Z",
     "iopub.status.idle": "2025-08-08T14:06:59.618703Z",
     "shell.execute_reply": "2025-08-08T14:06:59.617975Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.605577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_supervoxel_map(\n",
    "    cropped_volume_path: str,\n",
    "    supervoxel_type: str = \"FCC\",\n",
    "    fcc_cube_side: float = 100.0,\n",
    "    slic_n_supervoxels: int = 380,\n",
    "    organ_map_path: Optional[str] = None,\n",
    "    just_load: Optional[bool] = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a supervoxel map for the cropped volume.\n",
    "    Supports:\n",
    "      - \"full-organs\": 1 organ = 1 supervoxel, simple as that\n",
    "      - \"FCC\": standard face-centered cubic\n",
    "      - \"SLIC\": simple linear iterative clustering\n",
    "      - \"FCC-organs\": FCC constrained by organ presence\n",
    "    Returns an array of shape (D, H, W) with consecutive integer labels.\n",
    "    \"\"\"\n",
    "    if just_load:\n",
    "        sv_map = nib.load(\"supervoxel_map.nii.gz\").get_fdata()\n",
    "    else:\n",
    "        img = nib.load(cropped_volume_path)\n",
    "        data = img.get_fdata().astype(np.float32)\n",
    "\n",
    "        if supervoxel_type == \"full-organs\":\n",
    "            if organ_map_path is None:\n",
    "                raise ValueError(\"organ_map_path must be provided for 'full-organs' type\")\n",
    "            # load organ segmentation map\n",
    "            organ_img = nib.load(organ_map_path)\n",
    "            # the supervoxel map is just the organ map\n",
    "            sv_map = organ_img.get_fdata().astype(np.float32)\n",
    "        elif supervoxel_type == \"FCC\":\n",
    "            sv_map = generate_FCC_supervoxel_map(img, S=fcc_cube_side)\n",
    "        elif supervoxel_type == \"SLIC\":\n",
    "            spacing = get_spacing(data)\n",
    "            sv_map = apply_SLIC(data, spacing, slic_n_supervoxels)\n",
    "        elif supervoxel_type == \"FCC-organs\":\n",
    "            if organ_map_path is None:\n",
    "                raise ValueError(\"organ_map_path must be provided for 'FCC-organs' type\")\n",
    "            # load organ segmentation map\n",
    "            organ_img = nib.load(organ_map_path)\n",
    "            sv_map, organ_table, statistics = generate_FCC_organs_supervoxel_map(\n",
    "            volume_img=img, organ_img=organ_img, S=fcc_cube_side\n",
    "        )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported supervoxel_type: {supervoxel_type}\")\n",
    "\n",
    "        # remap labels to consecutive ints\n",
    "        _, inverse = np.unique(sv_map, return_inverse=True)\n",
    "        sv_map = inverse.reshape(sv_map.shape)\n",
    "        \n",
    "        # debug\n",
    "        image = nib.Nifti1Image(sv_map.astype(np.float32), img.affine)\n",
    "        nib.save(image, \"supervoxel_map.nii.gz\")\n",
    "        print(\"supervoxel map saved!\")\n",
    "\n",
    "    # reorder axes (W,H,D) -> (D,H,W)\n",
    "    sv_map = np.transpose(sv_map, (2, 1, 0))\n",
    "    return sv_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61673022",
   "metadata": {},
   "source": [
    "### derive baseline cached dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f96b4308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.619717Z",
     "iopub.status.busy": "2025-08-08T14:06:59.619460Z",
     "iopub.status.idle": "2025-08-08T14:06:59.632415Z",
     "shell.execute_reply": "2025-08-08T14:06:59.631860Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.619695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_cached_output_dictionary(volume_file: Path,\n",
    "                                 predictor: CustomNNUNetPredictor,\n",
    "                                 preprocess_before_run: bool = True,\n",
    "                                verbose: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Return a dictionary indexed by the slices for the sliding window, of the output of the inference for each patch\n",
    "        of the original volume\n",
    "        \"\"\"\n",
    "        rw = predictor.plans_manager.image_reader_writer_class()\n",
    "\n",
    "        # If nnU-Net returns a class instead of an instance, instantiate it\n",
    "        if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "            rw = rw()\n",
    "\n",
    "        orig_image, orig_props = rw.read_images(\n",
    "            [str(volume_file)]\n",
    "        )             # (C, Z, Y, X)\n",
    "\n",
    "        if preprocess_before_run:\n",
    "        \n",
    "            preprocessor = predictor.configuration_manager.preprocessor_class()\n",
    "            # the following cause the kernel death at first notebook run\n",
    "            data_pp, _, _ = preprocessor.run_case_npy(\n",
    "                    orig_image,\n",
    "                    seg=None,\n",
    "                    properties=orig_props,\n",
    "                    plans_manager=predictor.plans_manager,\n",
    "                    configuration_manager=predictor.configuration_manager,\n",
    "                    dataset_json=predictor.dataset_json\n",
    "                )\n",
    "        \n",
    "            # to torch, channel-first is already true\n",
    "            inp_tensor = torch.from_numpy(data_pp)\n",
    "        else:\n",
    "            inp_tensor = torch.from_numpy(orig_image)\n",
    "\n",
    "        slicers = predictor._internal_get_sliding_window_slicers(inp_tensor.shape[1:])\n",
    "\n",
    "        dictionary = predictor.get_output_dictionary_sliding_window(inp_tensor, slicers)\n",
    "\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "87ac185d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.633717Z",
     "iopub.status.busy": "2025-08-08T14:06:59.633211Z",
     "iopub.status.idle": "2025-08-08T14:06:59.646175Z",
     "shell.execute_reply": "2025-08-08T14:06:59.645534Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.633694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#USE_STORED_DICTIONARY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "36795cdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.647095Z",
     "iopub.status.busy": "2025-08-08T14:06:59.646899Z",
     "iopub.status.idle": "2025-08-08T14:06:59.659173Z",
     "shell.execute_reply": "2025-08-08T14:06:59.658480Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.647081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_baseline_prediction(\n",
    "    volume_tensor: \"torch.Tensor\",\n",
    "    predictor,\n",
    "    cropped_volume_path: str,\n",
    "    use_nnunet: bool = True\n",
    ") -> Tuple[\"torch.Tensor\", Dict]:\n",
    "    \"\"\"\n",
    "    Compute baseline segmentation mask and cache full prediction outputs.\n",
    "\n",
    "    Returns the segmentation mask tensor and a cache dictionary.\n",
    "    \"\"\"\n",
    "    # Predict logits\n",
    "    logits = predictor.predict_sliding_window_return_logits(volume_tensor[0])\n",
    "    # Convert to binary mask (assuming class 1 is positive)\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)\n",
    "\n",
    "    # Cache prediction outputs for SHAP\n",
    "    cache_dict = get_cached_output_dictionary(\n",
    "        volume_file=cropped_volume_path,\n",
    "        predictor=predictor,\n",
    "        preprocess_before_run=use_nnunet,\n",
    "        verbose=True\n",
    "    )\n",
    "    return seg_mask, cache_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487311a",
   "metadata": {},
   "source": [
    "### Include masking in the forward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eb94f",
   "metadata": {},
   "source": [
    "### **Chrabaszcz aggregation** Â \n",
    "\n",
    "Let\n",
    "\n",
    "* $z_1^{(i)}(x)$ â€“ class-1 logit at voxel $x$ after perturbation *i*\n",
    "* $P_i(x)=\\mathbf 1\\!\\left[\\arg\\max_c z_c^{(i)}(x)=1\\right]$ â€“ binary mask of voxels currently predicted as lymph-node\n",
    "* no ROI, total volume considered\n",
    "\n",
    "$$\n",
    "S_{\\text{Chr}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x} P_i(x)\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the `scaling_factor`.\n",
    "\n",
    "* **Counts evidence only from voxels the model *currently* labels as class 1.**\n",
    "* *False positives (FP):* contribute **positively** (they are in $P_i$).\n",
    "* *False negatives (FN):* contribute **zero** (their logit is absent).\n",
    "\n",
    "Source: Chrabaszcz et al., *Aggregated Attributions for Explanatory Analysis of 3-D Segmentation Models*, 2024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "f2b3555c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.660219Z",
     "iopub.status.busy": "2025-08-08T14:06:59.659950Z",
     "iopub.status.idle": "2025-08-08T14:06:59.673908Z",
     "shell.execute_reply": "2025-08-08T14:06:59.673170Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.660171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def chrabaszcz_aggregation(logits: torch.Tensor,\n",
    "                           scaling_factor: float = 1.0,\n",
    "                          ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n",
    "    Explanatory Analysis of 3D Segmentation Models\"\n",
    "    \"\"\"\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)\n",
    "    aggregate = torch.sum(logits[1].double() * seg_mask)\n",
    "\n",
    "    return aggregate / scaling_factor  # normalize to avoid overflows in SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ddf1c",
   "metadata": {},
   "source": [
    "\n",
    "### **True positive aggregation** (Chrabaszcz aggregation + baseline-mask filtering)\n",
    "\n",
    "Introduce the unperturbed prediction $P_0$, and, optionally, the ROI mask $R(x)$.\n",
    "Keep only voxels that are **still** class 1 *and* were class 1 before:\n",
    "\n",
    "$$\n",
    "S_{\\text{Chr\\,keep}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x} \\bigl[P_i(x)\\land P_0(x)\\land R(x)\\bigr]\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "* **True positives preserved** (TP core) add positive evidence.\n",
    "* **FP created by the perturbation** are **ignored** (masked out).\n",
    "* **FN** lower the score indirectly because their logits disappear from the sum.\n",
    "\n",
    "Conceptually this is the **positive part** of a signed logit-difference metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8c45bfdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.675738Z",
     "iopub.status.busy": "2025-08-08T14:06:59.674739Z",
     "iopub.status.idle": "2025-08-08T14:06:59.684613Z",
     "shell.execute_reply": "2025-08-08T14:06:59.683922Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.675721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def true_positive_aggregation(logits: torch.Tensor,\n",
    "                          unperturbed_binary_mask: torch.Tensor,\n",
    "                          ROI_mask: torch.Tensor,\n",
    "                           scaling_factor: float = 1.0,\n",
    "                          ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n",
    "    Explanatory Analysis of 3D Segmentation Models\", with the  addition of filtering by the unperturbed segmentation.\n",
    "    We can use this to ignore \"false positive\" voxels -> only account for true positive contribution;\n",
    "    so this corresponds conceptually to the positive part of a logits difference metric\n",
    "    \"\"\"\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)          # (D,H,W)\n",
    "    seg_mask = seg_mask.bool() & ROI_mask.bool() & unperturbed_binary_mask.bool()  # prefer boolean indexing for reletively sparse tensors\n",
    "    aggregate = torch.sum(logits[1].double()[seg_mask])\n",
    "\n",
    "    return aggregate / scaling_factor  # normalize to avoid overflows in SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48f967",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **False-positive aggregation**\n",
    "\n",
    "Directly sum class-1 evidence from **new** positives inside ROI:\n",
    "\n",
    "$$\n",
    "S_{\\text{FP}}^{(i)} \\;=\\;\n",
    "-\\frac{1}{\\alpha}\\sum_{x}\n",
    "\\bigl[P_i(x)\\land\\neg P_0(x)\\land R(x)\\bigr]\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "* Measures **only** the spurious lymph-node evidence a perturbation introduces.\n",
    "* Higher value â‡’ stronger tendency to hallucinate extra nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "b6284667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.685610Z",
     "iopub.status.busy": "2025-08-08T14:06:59.685388Z",
     "iopub.status.idle": "2025-08-08T14:06:59.698121Z",
     "shell.execute_reply": "2025-08-08T14:06:59.697478Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.685596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def false_positive_aggregation(logits: torch.Tensor,\n",
    "                              unperturbed_binary_mask: torch.Tensor,\n",
    "                              ROI_mask: torch.Tensor,\n",
    "                              scaling_factor: float = 1.0,\n",
    "                              ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Negative part of signed logit-difference objective, returned with positive sign;\n",
    "    Only accounts for false positive voxels in segmentation (spurious lymph nodes)\n",
    "    \"\"\"\n",
    "    # current segmentation (prevailing class)\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)     # (D,H,W) âˆˆ {0,1}\n",
    "\n",
    "    fp_mask  = seg_mask * ROI_mask * torch.logical_not(unperturbed_binary_mask.bool()).float()   # prefer float multiplication for dense tensors\n",
    "    # change sign to get negative scores for false positives\n",
    "    aggregate = - torch.sum(logits[1].double() * fp_mask) \n",
    "\n",
    "    return aggregate / scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54518c56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Dice aggregation (prediction-vs-baseline, ROI-restricted)**\n",
    "\n",
    "Let $R(x)$ be the ROI mask.\n",
    "\n",
    "$$\n",
    "P_i' = P_i \\odot R, \\qquad\n",
    "P_0' = P_0 \\odot R\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{\\text{Dice}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\;\n",
    "\\frac{2\\,\\langle P_i',\\,P_0'\\rangle}{\\lVert P_i'\\rVert_1 + \\lVert P_0'\\rVert_1 + \\varepsilon}\n",
    "$$\n",
    "\n",
    "* Drops when either **FP** ($P_i'=1,\\,P_0'=0$) or **FN** ($P_i'=0,\\,P_0'=1$) appear â†’ penalises both error types symmetrically.\n",
    "\n",
    "Based on the â€œself-consistency Diceâ€ used in MiSuRe (Hasany et al., 2024).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "694f3049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.699042Z",
     "iopub.status.busy": "2025-08-08T14:06:59.698813Z",
     "iopub.status.idle": "2025-08-08T14:06:59.709492Z",
     "shell.execute_reply": "2025-08-08T14:06:59.708943Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.699028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dice_aggregation(logits: torch.Tensor,\n",
    "                    unperturbed_binary_mask: torch.Tensor,\n",
    "                    ROI_mask: torch.Tensor,\n",
    "                    scaling_factor: float = 1.0,\n",
    "                    eps: float = 1e-9,  # small value to avoid division by zero\n",
    "                    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Use Dice score, the same aggregation measure from \"Hasany et al. - 2024 - MiSuRe is all you need to explain your image segmentation\"\n",
    "    Dice score provides a single aggregation metric that accounts for both false negatives and false positives penalization.\n",
    "    Specificly, we instead score each perturbation supervoxels by that Dice => supervoxels that contribute the most in reproducing\n",
    "    the baseline segmentation, will get an higher score\n",
    "    \"\"\"\n",
    "    # 1. Boolean masks restricted to ROI\n",
    "    pred = (logits.argmax(dim=0) == 1).float() * ROI_mask.float()\n",
    "    base = unperturbed_binary_mask.float()      * ROI_mask.float()\n",
    "\n",
    "    # 2. Intersection and denominator\n",
    "    inter = (pred * base).sum()\n",
    "    denom = pred.sum() + base.sum() + eps       # |P| + |B|\n",
    "\n",
    "    # 3. Dice coefficient\n",
    "    dice = (2.0 * inter) / denom\n",
    "\n",
    "    return dice / scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be36dd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Signed logit-difference (masked)**\n",
    "\n",
    "Define a signed weight\n",
    "\n",
    "$$\n",
    "w(x)=\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } P_0(x)=1\\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases},\n",
    "\\qquad w(x)\\leftarrow w(x)\\,R(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_{\\text{LD}}^{(i)} \\;=\\;\n",
    "\\frac{1}{\\alpha}\\sum_{x} P_i(x)\\;w(x)\\;z_1^{(i)}(x)\n",
    "$$\n",
    "\n",
    "* **Positive attribution:** voxels that *keep* the baseline TP (support segmentation).\n",
    "* **Negative attribution:** voxels that become class 1 **only** after perturbation (generate FP inside ROI).\n",
    "* FN reduce the positive term (logits disappear) but do **not** add negative mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "551ec010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.710464Z",
     "iopub.status.busy": "2025-08-08T14:06:59.710279Z",
     "iopub.status.idle": "2025-08-08T14:06:59.723662Z",
     "shell.execute_reply": "2025-08-08T14:06:59.723037Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.710450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def logit_difference_aggregation(\n",
    "        logits: torch.Tensor,\n",
    "        unperturbed_binary_mask: torch.Tensor,\n",
    "        ROI_mask: torch.Tensor,\n",
    "        scaling_factor: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Signed logit-difference objective *masked by the prevailing class*.\n",
    "    \"\"\"\n",
    "    # current segmentation (prevailing class)\n",
    "    seg_mask = (torch.argmax(logits, dim=0) == 1)     # (D,H,W) âˆˆ {0,1}\n",
    "\n",
    "    # +1 inside baseline positives, âˆ’1 elsewhere...\n",
    "    signed_weight = torch.where(unperturbed_binary_mask.bool(),\n",
    "                                torch.tensor(1.0, device=logits.device),\n",
    "                                torch.tensor(-1.0, device=logits.device))\n",
    "\n",
    "    # ... but we only care of false positives inside the ROI (we don't even have the segmentation mask outside the ROI)\n",
    "    signed_weight = signed_weight * ROI_mask\n",
    "\n",
    "    # aggregate signed class-1 evidence, restricted to voxels\n",
    "    # that are *currently* predicted as class-1 (seg_mask)\n",
    "    aggregate = torch.sum(logits[1] * seg_mask * signed_weight)\n",
    "    return aggregate / scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcd5cf",
   "metadata": {},
   "source": [
    "### Define a function to prepare all the steps for SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "96fccb97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T14:06:59.749597Z",
     "iopub.status.busy": "2025-08-08T14:06:59.749354Z",
     "iopub.status.idle": "2025-08-08T14:06:59.761353Z",
     "shell.execute_reply": "2025-08-08T14:06:59.760816Z",
     "shell.execute_reply.started": "2025-08-08T14:06:59.749574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "def prepare_data_for_shap(\n",
    "    volume_path: str,\n",
    "    patch_size: np.ndarray,\n",
    "    predictor,\n",
    "    dataset_json_path: str,\n",
    "    ROI_BB_path: Optional[str] = None,\n",
    "    ROI_segmentation_mask_path: Optional[str] = None,\n",
    "    ROI_type: str = \"BoundingBox\",\n",
    "    nnunet_preprocessing: bool = True,\n",
    "    supervoxel_type: str = \"FCC\",\n",
    "    fcc_supervoxel_size: Optional[float] = None,\n",
    "    slic_n_supervoxels: Optional[int] = None,\n",
    "    organ_map_path: Optional[str] = None,\n",
    "    load_stored_sv_map: Optional[bool] = False, # Forbidden for multiple (different) volumes, only for debugging single volume\n",
    "    device: str = 'cuda:0'\n",
    ") -> Tuple[torch.Tensor, np.ndarray, torch.Tensor, torch.Tensor, torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Orchestrate data preparation by invoking modular utilities.\n",
    "    Returns:\n",
    "      - volume_tensor (1,C,D,H,W)\n",
    "      - affine of cropped volume\n",
    "      - supervoxel_map_tensor (D,H,W)\n",
    "      - segmentation_mask_tensor (D,H,W)\n",
    "      - ROI_mask_tensor (D,H,W)\n",
    "      - cache_dict\n",
    "    \"\"\"\n",
    "    # 1. Load ROI slices\n",
    "    x_slice, y_slice, z_slice = load_roi_slices(\n",
    "        ROI_type,\n",
    "        ROI_BB_path,\n",
    "        ROI_segmentation_mask_path\n",
    "    )\n",
    "\n",
    "    # 2. Create binary ROI mask\n",
    "    roi_mask_path = create_roi_mask(\n",
    "        volume_path,\n",
    "        (x_slice, y_slice, z_slice)\n",
    "    )\n",
    "\n",
    "    # 3. Compute receptive-fieldâ€“padded slices\n",
    "    vol_img = nib.load(volume_path)\n",
    "    padded_slices = compute_rf_slices(\n",
    "        (x_slice, y_slice, z_slice),\n",
    "        patch_size,\n",
    "        vol_img.get_fdata().shape\n",
    "    )\n",
    "\n",
    "    # 4. Crop volume and ROI mask using RF slices\n",
    "    cropped = crop_volume_with_rf(\n",
    "        volume_path,\n",
    "        (x_slice, y_slice, z_slice),\n",
    "        patch_size\n",
    "    )\n",
    "    cropped_volume, affine_cropped = cropped['cropped_volume']\n",
    "    cropped_mask, _ = cropped['cropped_roi_mask']\n",
    "\n",
    "    cropped_vol_path = Path('cropped_volume.nii.gz')\n",
    "    cropped_mask_path = Path('cropped_mask_with_RF.nii.gz')\n",
    "\n",
    "    # 5. If provided, also crop the organs map with the same RF slices\n",
    "    cropped_organs_path = None\n",
    "    if organ_map_path is not None:\n",
    "        cropped_organs_path = 'cropped_organ_map_with_RF.nii.gz'\n",
    "        crop_volume_and_affine(\n",
    "            organ_map_path,\n",
    "            padded_slices,\n",
    "            Path(cropped_organs_path)\n",
    "        )\n",
    "\n",
    "    # 6. Preprocess volume\n",
    "    volume_np, _ = preprocess_volume(\n",
    "        str(cropped_vol_path),\n",
    "        predictor,\n",
    "        dataset_json_path,\n",
    "        use_nnunet=nnunet_preprocessing\n",
    "    )\n",
    "\n",
    "    # 7. Convert to torch tensor\n",
    "    volume_tensor = torch.from_numpy(\n",
    "        volume_np.astype(np.float32)\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "    # 8. Generate supervoxel map, passing cropped_organs_path for FCC-organs\n",
    "    sv_array = generate_supervoxel_map(\n",
    "        str(cropped_vol_path),\n",
    "        supervoxel_type=supervoxel_type,\n",
    "        fcc_cube_side = fcc_supervoxel_size,\n",
    "        slic_n_supervoxels = slic_n_supervoxels,\n",
    "        organ_map_path=cropped_organs_path,\n",
    "        just_load=load_stored_sv_map\n",
    "    )\n",
    "    sv_tensor = torch.from_numpy(sv_array).long().to(device)\n",
    "\n",
    "    # 9. Compute baseline segmentation and cache\n",
    "    seg_mask, cache_dict = compute_baseline_prediction(\n",
    "        volume_tensor,\n",
    "        predictor,\n",
    "        str(cropped_vol_path),\n",
    "        use_nnunet=nnunet_preprocessing\n",
    "    )\n",
    "\n",
    "    # 10. Prepare ROI mask tensor\n",
    "    roi_mask_tensor = torch.from_numpy(\n",
    "        np.transpose(cropped_mask, (2, 1, 0))\n",
    "    ).to(device)\n",
    "\n",
    "    return (\n",
    "        volume_tensor,\n",
    "        affine_cropped,\n",
    "        sv_tensor,\n",
    "        seg_mask.to(device),\n",
    "        roi_mask_tensor,\n",
    "        cache_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5f78e",
   "metadata": {},
   "source": [
    "## Iterate over volumes and run SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "65d5356b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T16:03:50.978382Z",
     "iopub.status.busy": "2025-08-08T16:03:50.977812Z",
     "iopub.status.idle": "2025-08-08T16:03:50.982623Z",
     "shell.execute_reply": "2025-08-08T16:03:50.981798Z",
     "shell.execute_reply.started": "2025-08-08T16:03:50.978359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "d971961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "# evita spam in console\n",
    "os.environ.setdefault(\"WANDB_SILENT\", \"true\")\n",
    "\n",
    "def wandb_login(project=None, entity=None):\n",
    "    # 1) prova da variabile dâ€™ambiente / .env\n",
    "    api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "    # 2) fallback a kaggle_secrets se non trovato\n",
    "    if not api_key:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            api_key = UserSecretsClient().get_secret(\"wandb_key\")\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"WANDB_API_KEY non trovata nÃ© in .env nÃ© in Kaggle secrets\")\n",
    "\n",
    "    # login\n",
    "    wandb.login(key=api_key, relogin=True)\n",
    "\n",
    "    # opzionale: avvia run\n",
    "    if project is not None and entity is not None: \n",
    "        return wandb.init(project=project, entity=entity, settings=wandb.Settings(silent=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "3da06bbe-af09-4529-b5c2-1d4af0702af1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T16:03:50.984159Z",
     "iopub.status.busy": "2025-08-08T16:03:50.983973Z",
     "iopub.status.idle": "2025-08-08T16:03:51.201586Z",
     "shell.execute_reply": "2025-08-08T16:03:51.201039Z",
     "shell.execute_reply.started": "2025-08-08T16:03:50.984143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if WANDB:\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "    settings=wandb.Settings(silent=True)  # no console spam\n",
    "    \n",
    "    wandb_login()\n",
    "\n",
    "    morf_predictor.set_wandb_logging(wandb_logging=WANDB, wandb_label=\"MoRF_\", wandb_commit=False)\n",
    "    lerf_predictor.set_wandb_logging(wandb_logging=WANDB, wandb_label=\"LeRF_\", wandb_commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3b467494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T16:03:51.202281Z",
     "iopub.status.busy": "2025-08-08T16:03:51.202099Z",
     "iopub.status.idle": "2025-08-08T16:03:51.205599Z",
     "shell.execute_reply": "2025-08-08T16:03:51.204817Z",
     "shell.execute_reply.started": "2025-08-08T16:03:51.202267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_json_path = Path(model_dir) / \"dataset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "6781c620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T16:03:51.224329Z",
     "iopub.status.busy": "2025-08-08T16:03:51.223878Z",
     "iopub.status.idle": "2025-08-08T16:03:51.232256Z",
     "shell.execute_reply": "2025-08-08T16:03:51.231739Z",
     "shell.execute_reply.started": "2025-08-08T16:03:51.224313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "patch_size = np.array(morf_predictor.configuration_manager.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "734131bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-08T16:03:51.233365Z",
     "iopub.status.busy": "2025-08-08T16:03:51.233091Z",
     "iopub.status.idle": "2025-08-08T16:03:51.243652Z",
     "shell.execute_reply": "2025-08-08T16:03:51.243128Z",
     "shell.execute_reply.started": "2025-08-08T16:03:51.233350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "@torch.inference_mode()\n",
    "def forward_segmentation_output_to_explain(\n",
    "        input_image:         torch.Tensor,\n",
    "        perturbation_mask:   torch.BoolTensor | None,\n",
    "        segmentation_mask:      torch.Tensor,   # remember that must be cropped to the same size of the other tensors\n",
    "        ROI_bounding_box_mask:      torch.Tensor,\n",
    "        baseline_prediction_dict: dict,\n",
    "        predictor: CustomNNUNetPredictor,\n",
    "        aggregation_fn: Callable = true_positive_aggregation,\n",
    ") -> torch.Tensor:           # returns a scalar per sample\n",
    "    \"\"\"\n",
    "    Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n",
    "    by the network â€“ adapt to your real metric as needed.\n",
    "    \"\"\"\n",
    "    logits = predictor.predict_sliding_window_return_logits_with_caching(\n",
    "        input_image, perturbation_mask, baseline_prediction_dict,\n",
    "    )                              # (C, D, H, W)\n",
    "    # we now mask both by the segmentation prevalent class, and by ROI\n",
    "    D,H,W = logits.shape[1:]\n",
    "    aggregate = aggregation_fn(\n",
    "        logits = logits,\n",
    "        unperturbed_binary_mask = segmentation_mask,\n",
    "        ROI_mask = ROI_bounding_box_mask,\n",
    "        scaling_factor = ((D*H*W) if aggregation_fn != dice_aggregation else 1.0)\n",
    "    )\n",
    "\n",
    "    return aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cff41b",
   "metadata": {},
   "source": [
    "## Evaluate attribution maps using ABPC (Area Between Perturbation Curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "7bfe1ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_voxel_volume_mm3(nifti_volume) -> float:\n",
    "    \"\"\"\n",
    "    Compute the volume of a single voxel in mm^3 from a NIfTI image.\n",
    "\n",
    "    Parameters:\n",
    "    - nifti_volume: nib.Nifti1Image, the input NIfTI image\n",
    "\n",
    "    Returns:\n",
    "    - voxel_volume_mm3: float, volume of a single voxel in mm^3\n",
    "    \"\"\"\n",
    "    spacing = nifti_volume.header.get_zooms()  # get_zooms returns (x, y, z) spacing in mm\n",
    "    unit = nifti_volume.header.get_xyzt_units()[0]  # spatial unit\n",
    "    if unit == 'mm':\n",
    "        pass  # already in mm\n",
    "    elif unit == 'cm':\n",
    "        spacing = tuple(s * 10.0 for s in spacing)\n",
    "    elif unit == 'm':\n",
    "        spacing = tuple(s * 1000.0 for s in spacing)\n",
    "    elif unit == 'um':\n",
    "        spacing = tuple(s * 1e-3 for s in spacing)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported spatial unit: {unit}\")\n",
    "    voxel_volume_mm3 = spacing[0] * spacing[1] * spacing[2]\n",
    "    return voxel_volume_mm3\n",
    "\n",
    "\n",
    "def get_total_volume(feature_map: np.ndarray, voxel_volume_mm3: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the total volume of a tensor in voxels and in mm^3.\n",
    "    Parameters:\n",
    "    - feature_map: np.ndarray with shape (Z, Y, X), integer-labeled regions\n",
    "    - voxel_volume_mm3: float, volume of a single voxel in mm^3\n",
    "\n",
    "    Returns:\n",
    "    - volume_voxels: int, total volume in voxels\n",
    "    - total_volume_mm: float, total volume in mm^3\n",
    "    \"\"\"\n",
    "    volume_voxels = np.prod(feature_map.shape)\n",
    "    total_volume_mm = volume_voxels * voxel_volume_mm3\n",
    "    return volume_voxels, total_volume_mm\n",
    "\n",
    "def get_supervoxel_volumes(feature_map: np.ndarray, voxel_volume_mm3: float, include_background: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the volume (voxel count) for each supervoxel in the feature map.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_map: np.ndarray with shape (Z, Y, X), integer-labeled regions\n",
    "    - voxel_volume_mm3: float, volume of a single voxel in mm^3\n",
    "    - include_background: bool, if False, excludes region ID 0 from the result\n",
    "\n",
    "    Returns:\n",
    "    - supervoxel_volumes: np.ndarrays with shape (2, num_features,), volume (voxel count) per region\n",
    "    the first element of the first axis contains the number of voxels, the second the volume in mm^3\n",
    "    0th element corresponds to region ID 0, 1st to ID 1, etc.\n",
    "    \"\"\"\n",
    "    features = np.unique(feature_map)\n",
    "    if not include_background:\n",
    "        features = features[features != 0]\n",
    "\n",
    "    # Compute the volume (voxel count) for each feature region\n",
    "    supervoxel_volumes = np.array([\n",
    "        np.sum(feature_map == feat_id) for feat_id in features\n",
    "    ])\n",
    "    supervoxel_volumes = (supervoxel_volumes, supervoxel_volumes * voxel_volume_mm3)\n",
    "\n",
    "    return supervoxel_volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "e7302164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to retrieve the interpretable feature attribution vector from feature map + attribution map\n",
    "def get_attribution_vector(feature_map: np.ndarray, attribution_map: np.ndarray, include_background: bool = True):\n",
    "    \"\"\"\n",
    "    Recover the original interpretable feature attribution vector from a dense attribution map,\n",
    "    assuming the attribution is constant within each labeled region of the feature map.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_map: np.ndarray with shape (Z, Y, X), integer-labeled regions\n",
    "    - attribution_map: np.ndarray with same shape, values are repeated per region\n",
    "    - include_background: bool, if False, excludes region ID 0 from the result\n",
    "\n",
    "    Returns:\n",
    "    - attribution_vector: np.ndarray of shape (num_features,), one attribution per region\n",
    "    \"\"\"\n",
    "    features = np.unique(feature_map)\n",
    "    if not include_background:\n",
    "        features = features[features != 0]\n",
    "\n",
    "    # Select the first voxel index of each feature region\n",
    "    attribution_vector = np.array([\n",
    "        attribution_map[feature_map == f][0]  # safe because values are constant per region\n",
    "        for f in features\n",
    "    ])\n",
    "\n",
    "    return attribution_vector\n",
    "\n",
    "\n",
    "# define the method that given an attribution map in the interpretable space, return\n",
    "# two generators:\n",
    "# the MoRF (Most Relevant First) generator, returning the interpretable feature vectors sorted by relevance order\n",
    "# the LeRF (Least Relevant First) generator, returning the interpretable feature vectors sorted by inverse relevance order\n",
    "\n",
    "def generate_perturbations_ABPC(attribution_vector):\n",
    "    sorted_features = sorted(range(len(attribution_vector)), key=lambda i: attribution_vector[i], reverse=True)\n",
    "    # Create the MoRF generator\n",
    "    # each item is a binary vector containing the first n features on, and the remaining off\n",
    "    def morf_generator():\n",
    "        vector = np.ones_like(attribution_vector)\n",
    "        yield torch.from_numpy(vector).float()\n",
    "        for i in sorted_features:\n",
    "            vector[i] = 0\n",
    "            yield torch.from_numpy(vector).float()\n",
    "\n",
    "    # Create the LeRF generator\n",
    "    # each item is a binary vector containing the last n features on, and the remaining off\n",
    "    def lerf_generator():\n",
    "        vector = np.ones_like(attribution_vector)\n",
    "        yield torch.from_numpy(vector).float()\n",
    "        for i in reversed(sorted_features):\n",
    "            vector[i] = 0\n",
    "            yield torch.from_numpy(vector).float()\n",
    "\n",
    "    return morf_generator(), lerf_generator()\n",
    "\n",
    "\n",
    "def generate_perturbations_ABPC_with_volumes(attribution_vector, volume_vector):\n",
    "    \"\"\"\n",
    "    Generate perturbation sequences with volume tracking.\n",
    "    \n",
    "    Returns:\n",
    "    - morf_generator: generator yielding (binary_vector, cumulative_volume_removed)\n",
    "    - lerf_generator: generator yielding (binary_vector, cumulative_volume_removed)\n",
    "    \"\"\"\n",
    "    sorted_indices = sorted(range(len(attribution_vector)), key=lambda i: attribution_vector[i], reverse=True)\n",
    "    \n",
    "    def morf_generator():\n",
    "        vector = np.ones_like(attribution_vector)\n",
    "        cumulative_volume = np.zeros((2,))  # (voxel count, mm^3)\n",
    "        yield torch.from_numpy(vector).float(), cumulative_volume\n",
    "        \n",
    "        for i in sorted_indices:\n",
    "            vector[i] = 0\n",
    "            cumulative_volume[0] += volume_vector[0][i]\n",
    "            cumulative_volume[1] += volume_vector[1][i]\n",
    "            yield torch.from_numpy(vector).float(), cumulative_volume\n",
    "\n",
    "    def lerf_generator():\n",
    "        vector = np.ones_like(attribution_vector)\n",
    "        cumulative_volume = np.zeros((2,))  # (voxel count, mm^3)\n",
    "        yield torch.from_numpy(vector).float(), cumulative_volume\n",
    "        \n",
    "        for i in reversed(sorted_indices):\n",
    "            vector[i] = 0\n",
    "            cumulative_volume[0] += volume_vector[0][i]\n",
    "            cumulative_volume[1] += volume_vector[1][i]\n",
    "            yield torch.from_numpy(vector).float(), cumulative_volume\n",
    "\n",
    "    return morf_generator(), lerf_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "928b3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the perturbator function that given the current binary input, the original input volume and the feature map\n",
    "# returns the perturbed volume and the perturbation map\n",
    "\n",
    "# (from Custom Captum Lime/KernelSHAP)\n",
    "def default_from_interp_rep_transform(curr_sample, original_input, feature_map, baseline, attribute_background=True):\n",
    "\n",
    "    def _build_keep_mask(labels: torch.Tensor, sample_vec: torch.Tensor, attribute_background: bool) -> torch.BoolTensor:\n",
    "        \"\"\"\n",
    "        Returns a boolean mask 'keep':\n",
    "            True  -> take from original_inputs\n",
    "            False -> take from baselines\n",
    "        \"\"\"\n",
    "        M = sample_vec.shape[0]\n",
    "        sample_vec = sample_vec.bool()\n",
    "\n",
    "        if attribute_background:\n",
    "            # interpretable idx = input label directly\n",
    "            valid = (labels >= 0) & (labels < M)\n",
    "            idx   = torch.clamp(labels, 0, M-1)\n",
    "            keep  = torch.ones_like(labels, dtype=torch.bool)\n",
    "            keep[valid] = sample_vec[idx[valid]]\n",
    "        else:\n",
    "            # interpretable idx i -> input label i+1\n",
    "            # background (0) is never perturbed\n",
    "            valid = (labels >= 1) & (labels <= M)\n",
    "            idx   = (labels - 1).clamp(min=0, max=M-1)\n",
    "            keep  = torch.ones_like(labels, dtype=torch.bool)\n",
    "            keep[valid] = sample_vec[idx[valid]]\n",
    "            # all labels == 0 (background) remain True\n",
    "        return keep\n",
    "\n",
    "    \n",
    "    keep_mask = _build_keep_mask(feature_map, curr_sample.to(device), attribute_background)\n",
    "    \n",
    "    return keep_mask.to(original_input.dtype) * original_input + (~keep_mask).to(original_input.dtype) * baseline, (~keep_mask).to(original_input.dtype).unsqueeze(0)  # perturbation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "1c39987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_attribution_vector = np.random.normal(size=(5,))\\nprint(\"attribution vector: \", test_attribution_vector)\\ntest_feature_map = np.array([[0,2,3],[2,1,0], [4,4,4]])\\nprint(\"feature map: \", test_feature_map)\\n# place each attribution in the corresponding place in the map\\ntest_attribution_map = np.zeros_like(test_feature_map, dtype=float)\\nfor i in range(test_feature_map.shape[0]):\\n    for j in range(test_feature_map.shape[1]):\\n        test_attribution_map[i, j] = test_attribution_vector[test_feature_map[i, j]]\\nprint(\"attribution map: \", test_attribution_map)\\n\\nresulting_attribution_vector = get_attribution_vector(test_feature_map, test_attribution_map, False)\\nprint(\"attribution vector: \", resulting_attribution_vector)'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_attribution_vector = np.random.normal(size=(5,))\n",
    "print(\"attribution vector: \", test_attribution_vector)\n",
    "test_feature_map = np.array([[0,2,3],[2,1,0], [4,4,4]])\n",
    "print(\"feature map: \", test_feature_map)\n",
    "# place each attribution in the corresponding place in the map\n",
    "test_attribution_map = np.zeros_like(test_feature_map, dtype=float)\n",
    "for i in range(test_feature_map.shape[0]):\n",
    "    for j in range(test_feature_map.shape[1]):\n",
    "        test_attribution_map[i, j] = test_attribution_vector[test_feature_map[i, j]]\n",
    "print(\"attribution map: \", test_attribution_map)\n",
    "\n",
    "resulting_attribution_vector = get_attribution_vector(test_feature_map, test_attribution_map, False)\n",
    "print(\"attribution vector: \", resulting_attribution_vector)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5e3fbcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_original_input = np.random.normal(size=(3,3))\\ntest_original_input = torch.from_numpy(test_original_input).float().unsqueeze(0).to(device)\\ntest_original_input'"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_original_input = np.random.normal(size=(3,3))\n",
    "test_original_input = torch.from_numpy(test_original_input).float().unsqueeze(0).to(device)\n",
    "test_original_input\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7ebab01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\"sorted attribution vector: \", np.sort(resulting_attribution_vector)[::-1])\\nprint(\"sorted indices of attribution vector: \", np.argsort(resulting_attribution_vector)[::-1])\\ntest_morf_generator, test_lerf_generator = generate_perturbations_ABPC(resulting_attribution_vector)\\nprint(\"MoRF samples - perturbed input:\")\\nfor sample in test_morf_generator:\\n    print(sample)\\n    perturbed_input, perturbation_map = default_from_interp_rep_transform(\\n        sample, test_original_input, torch.from_numpy(test_feature_map), baseline=0.0, attribute_background=False)\\n    print(\"perturbed input: \", perturbed_input)\\n    print(\"perturbation map: \", perturbation_map)\\n\\nprint(\"LeRF samples - perturbed input:\")\\nfor sample in test_lerf_generator:\\n    print(sample)\\n    perturbed_input, perturbation_map = default_from_interp_rep_transform(\\n        sample, test_original_input, torch.from_numpy(test_feature_map), baseline=0.0, attribute_background=False)\\n    print(\"perturbed input: \", perturbed_input)\\n    print(\"perturbation map: \", perturbation_map)'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(\"sorted attribution vector: \", np.sort(resulting_attribution_vector)[::-1])\n",
    "print(\"sorted indices of attribution vector: \", np.argsort(resulting_attribution_vector)[::-1])\n",
    "test_morf_generator, test_lerf_generator = generate_perturbations_ABPC(resulting_attribution_vector)\n",
    "print(\"MoRF samples - perturbed input:\")\n",
    "for sample in test_morf_generator:\n",
    "    print(sample)\n",
    "    perturbed_input, perturbation_map = default_from_interp_rep_transform(\n",
    "        sample, test_original_input, torch.from_numpy(test_feature_map), baseline=0.0, attribute_background=False)\n",
    "    print(\"perturbed input: \", perturbed_input)\n",
    "    print(\"perturbation map: \", perturbation_map)\n",
    "\n",
    "print(\"LeRF samples - perturbed input:\")\n",
    "for sample in test_lerf_generator:\n",
    "    print(sample)\n",
    "    perturbed_input, perturbation_map = default_from_interp_rep_transform(\n",
    "        sample, test_original_input, torch.from_numpy(test_feature_map), baseline=0.0, attribute_background=False)\n",
    "    print(\"perturbed input: \", perturbed_input)\n",
    "    print(\"perturbation map: \", perturbation_map)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "cf662e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_area_between_curves(morf_curve, lerf_curve):\n",
    "    \"\"\"\n",
    "    Compute the area between the LeRF and MoRF curves using the trapezoidal rule. In the literature, we divide\n",
    "    the area by the number of steps to normalize it.\n",
    "    \"\"\"\n",
    "    # Ensure the curves are of the same length\n",
    "    assert len(morf_curve) == len(lerf_curve), \"Curves must be of the same length\"\n",
    "\n",
    "    # Compute the area using the trapezoidal rule\n",
    "    area = 0.0\n",
    "    for i in range(1, len(morf_curve)):\n",
    "        area += 0.5 * ((lerf_curve[i] - morf_curve[i]) + (lerf_curve[i-1] - morf_curve[i-1]))\n",
    "    return area / len(morf_curve)\n",
    "\n",
    "\n",
    "def compute_aopc(morf_curve):\n",
    "    # Compute the Area Over the Perturbation Curve (AOPC)\n",
    "    reference = morf_curve[0]\n",
    "\n",
    "    area = 0.0\n",
    "    for i in range(1, len(morf_curve)):\n",
    "        area += 0.5 * ((reference - morf_curve[i]) + (reference - morf_curve[i-1]))\n",
    "    return area / len(morf_curve)\n",
    "\n",
    "def normalized_abpc(morf_curve, lerf_curve):\n",
    "    abpc = compute_area_between_curves(morf_curve, lerf_curve)\n",
    "    range = max(lerf_curve) - min(morf_curve)\n",
    "    return abpc / range\n",
    "\n",
    "def normalized_aopc(morf_curve):\n",
    "    aopc = compute_aopc(morf_curve)\n",
    "    range = max(morf_curve) - min(morf_curve)\n",
    "    return aopc / range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3a466cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(morf_curve, lerf_curve):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(morf_curve, label='MORF Curve', color='blue')\n",
    "    plt.plot(lerf_curve, label='LERF Curve', color='red')\n",
    "    plt.fill_between(range(len(morf_curve)), morf_curve, lerf_curve, where=(morf_curve > lerf_curve), color='lightblue', alpha=0.5)\n",
    "    plt.title('MORF vs LERF Curves')\n",
    "    plt.xlabel('Perturbation Steps')\n",
    "    plt.ylabel('Attribution Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "92c20671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# let's define the main block to compute one evaluation step, given the\n",
    "# original volume, the interpretable input vector, and the forward function\n",
    "def evaluate_step(original_volume: torch.Tensor,\n",
    "                  interpretable_input: torch.Tensor,\n",
    "                  feature_map: torch.Tensor,\n",
    "                  forward_function: Callable,\n",
    "                  predictor: CustomNNUNetPredictor,\n",
    "                  attribute_background: bool):\n",
    "    # obtain perturbed input\n",
    "    perturbed_input, perturbation_map = default_from_interp_rep_transform( \n",
    "                                                 interpretable_input, \n",
    "                                                 original_volume,\n",
    "                                                 feature_map,\n",
    "                                                 baseline=0.0,\n",
    "                                                 attribute_background=attribute_background\n",
    "                                                )\n",
    "\n",
    "    # compute the forward pass with the perturbed input\n",
    "    output = forward_function(perturbed_input[0], perturbation_map, predictor)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_ABPC_curves(original_volume: torch.Tensor,\n",
    "                       feature_map: torch.Tensor,\n",
    "                       attribution_map: np.ndarray,\n",
    "                       forward_function: Callable,\n",
    "                       morf_predictor: CustomNNUNetPredictor,\n",
    "                       lerf_predictor: CustomNNUNetPredictor,\n",
    "                       attribute_background: bool):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the Area Between Perturbation Curves (ABPC) using MoRF and LeRF strategies.\n",
    "    Parameters:\n",
    "    - original_volume: torch.Tensor of shape (1, C, D, H, W)\n",
    "    - feature_map: torch.Tensor of shape (D, H, W) with integer-labeled regions\n",
    "    - attribution_map: np.ndarray of shape (D, H, W) with dense attribution values\n",
    "    - forward_function: Callable that takes (input_volume, perturbation_mask, predictor) and returns a scalar output\n",
    "    - morf_predictor: CustomNNUNetPredictor for MoRF evaluation\n",
    "    - lerf_predictor: CustomNNUNetPredictor for LeRF evaluation\n",
    "    - attribute_background: bool, whether to include background region in attribution vector\n",
    "    Returns:\n",
    "    - morf_curve: np.ndarray of MoRF evaluation outputs\n",
    "    - lerf_curve: np.ndarray of LeRF evaluation outputs\n",
    "    - ABPC_area: float, area between the MoRF and LeRF curves\n",
    "    \"\"\"\n",
    "    # 1) get the attribution vector\n",
    "    attribution_vector = get_attribution_vector(feature_map.cpu().numpy(), attribution_map.cpu().numpy(), attribute_background)\n",
    "    print(\"Number of steps for the ABPC computation: \", len(attribution_vector))\n",
    "    # 2) get the two generators of binary inputs\n",
    "    morf_generator, lerf_generator = generate_perturbations_ABPC(attribution_vector)\n",
    "    morf_curve, lerf_curve = [], []\n",
    "\n",
    "    for morf_sample, lerf_sample in zip(morf_generator, lerf_generator):\n",
    "        #print(\"MoRF sample: \", morf_sample)\n",
    "        #print(\"LeRF sample: \", lerf_sample)\n",
    "        # 3) compute the evaluation step for both samples\n",
    "        morf_output = evaluate_step(original_volume, morf_sample, feature_map, forward_function, morf_predictor, attribute_background).cpu()\n",
    "        lerf_output = evaluate_step(original_volume, lerf_sample, feature_map, forward_function, lerf_predictor, attribute_background).cpu()\n",
    "\n",
    "        morf_curve.append(morf_output)\n",
    "        lerf_curve.append(lerf_output)\n",
    "        if WANDB:\n",
    "            wandb.log({\"MoRF\": morf_output.item(), \"LeRF\": lerf_output.item()})\n",
    "\n",
    "    morf_curve = np.array(morf_curve)\n",
    "    lerf_curve = np.array(lerf_curve)\n",
    "    # 4) compute the area between the two curves, and other metrics\n",
    "    ABPC_area = compute_area_between_curves(morf_curve, lerf_curve)\n",
    "    AOPC_area = compute_aopc(morf_curve)\n",
    "    norm_ABPC_area = normalized_abpc(morf_curve, lerf_curve)\n",
    "    norm_AOPC_area = normalized_aopc(morf_curve)\n",
    "\n",
    "    print(\"ABPC area: \", ABPC_area)\n",
    "    print(\"AOPC area: \", AOPC_area)\n",
    "    print(\"Normalized ABPC area: \", norm_ABPC_area)\n",
    "    print(\"Normalized AOPC area: \", norm_AOPC_area)\n",
    "    if WANDB:\n",
    "        wandb.log({\"ABPC_area\": ABPC_area.item(), \n",
    "                   \"AOPC\": AOPC_area.item(), \n",
    "                   \"norm_ABPC\": norm_ABPC_area.item(), \n",
    "                   \"norm_AOPC\": norm_AOPC_area.item()})\n",
    "\n",
    "    # 5) plot and store the curves\n",
    "    #plot_curves(morf_curve, lerf_curve)\n",
    "    #store_curves(morf_curve, lerf_curve)\n",
    "\n",
    "    # 6) return the computed values\n",
    "    return morf_curve, lerf_curve, ABPC_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "aaa9251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "def compute_ABPC_curves_with_volumes(original_volume: torch.Tensor,\n",
    "                       feature_map: torch.Tensor,\n",
    "                       attribution_map: np.ndarray,\n",
    "                       forward_function: Callable,\n",
    "                       morf_predictor: CustomNNUNetPredictor,\n",
    "                       lerf_predictor: CustomNNUNetPredictor,\n",
    "                       attribute_background: bool,\n",
    "                       voxel_volume_mm3: float = 1.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the Area Between Perturbation Curves (ABPC) using MoRF and LeRF strategies with volume tracking.\n",
    "    \n",
    "    Additional Parameters:\n",
    "    - voxel_volume_mm3: float, volume of a single voxel in mmÂ³ for proper volume calculation\n",
    "    \"\"\"\n",
    "    # 1) get supervoxel volumes\n",
    "    supervoxel_volumes = get_supervoxel_volumes(\n",
    "        feature_map.cpu().numpy(), \n",
    "        voxel_volume_mm3=voxel_volume_mm3,\n",
    "        include_background=attribute_background\n",
    "    )\n",
    "    #print(f\"Supervoxel volumes (voxels, mmÂ³): {supervoxel_volumes}\")\n",
    "\n",
    "    total_voxels, total_volume = get_total_volume(feature_map.cpu().numpy(), voxel_volume_mm3)\n",
    "    print(f\"Total volume: {total_volume:.2f} mmÂ³ ({total_voxels} voxels of {voxel_volume_mm3:.2f} mmÂ³ each)\")\n",
    "    \n",
    "    # 2) get the attribution vector with volumes\n",
    "    attribution_vector = get_attribution_vector(\n",
    "        feature_map.cpu().numpy(), \n",
    "        attribution_map.cpu().numpy(), \n",
    "        include_background=attribute_background\n",
    "    )\n",
    "    \n",
    "    print(\"Number of steps for the ABPC computation: \", len(attribution_vector))\n",
    "    print(f\"Total volume of supervoxels: {np.sum(supervoxel_volumes[1]):.2f} mmÂ³\")\n",
    "\n",
    "    # 3) get the two generators with volume tracking\n",
    "    morf_generator, lerf_generator = generate_perturbations_ABPC_with_volumes(\n",
    "        attribution_vector, supervoxel_volumes\n",
    "    )\n",
    "    \n",
    "    morf_curve, lerf_curve = [], []\n",
    "    step = 0\n",
    "\n",
    "    # Log initial total volume\n",
    "    if WANDB:\n",
    "        wandb.log({\n",
    "            \"total_volume_voxels\": total_voxels,\n",
    "            \"total_volume_mm3\": total_volume,\n",
    "            \"num_supervoxels\": len(attribution_vector)\n",
    "        }, commit=False)\n",
    "\n",
    "    for (morf_sample, morf_cumul_vol), (lerf_sample, lerf_cumul_vol) in zip(morf_generator, lerf_generator):\n",
    "        print(f\"Step {step}: MoRF cumul. volume removed: {morf_cumul_vol[1]:.2f} mmÂ³ ({morf_cumul_vol[0]} voxels), \"\n",
    "              f\"LeRF cumul. volume removed: {lerf_cumul_vol[1]:.2f} mmÂ³ ({lerf_cumul_vol[0]} voxels)\")\n",
    "        # 4) compute the evaluation step for both samples\n",
    "        morf_output = evaluate_step(original_volume, morf_sample, feature_map, forward_function, morf_predictor, attribute_background).cpu()\n",
    "        lerf_output = evaluate_step(original_volume, lerf_sample, feature_map, forward_function, lerf_predictor, attribute_background).cpu()\n",
    "\n",
    "        morf_curve.append(morf_output)\n",
    "        lerf_curve.append(lerf_output)\n",
    "        \n",
    "        # Calculate volume percentages\n",
    "        morf_vol_pct = (morf_cumul_vol[0] / total_voxels) * 100.0\n",
    "        lerf_vol_pct = (lerf_cumul_vol[0] / total_voxels) * 100.0\n",
    "\n",
    "        if WANDB:\n",
    "            wandb.log({\n",
    "                \"supervoxels_perturbed\": step,\n",
    "                \"MoRF\": morf_output.item(), \n",
    "                \"LeRF\": lerf_output.item(),\n",
    "                \"MoRF_volume_removed_voxels\": morf_cumul_vol[0],\n",
    "                \"LeRF_volume_removed_voxels\": lerf_cumul_vol[0],\n",
    "                \"MoRF_volume_removed_mm3\": morf_cumul_vol[1],\n",
    "                \"LeRF_volume_removed_mm3\": lerf_cumul_vol[1],\n",
    "                \"MoRF_volume_removed_pct\": morf_vol_pct,\n",
    "                \"LeRF_volume_removed_pct\": lerf_vol_pct\n",
    "            }, commit=True)\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "    morf_curve = np.array(morf_curve)\n",
    "    lerf_curve = np.array(lerf_curve)\n",
    "    \n",
    "    # 5) compute the area between the two curves, and other metrics\n",
    "    ABPC_area = compute_area_between_curves(morf_curve, lerf_curve)\n",
    "    AOPC_area = compute_aopc(morf_curve)\n",
    "    norm_ABPC_area = normalized_abpc(morf_curve, lerf_curve)\n",
    "    norm_AOPC_area = normalized_aopc(morf_curve)\n",
    "\n",
    "    \"\"\"print(\"ABPC area: \", ABPC_area)\n",
    "    print(\"AOPC area: \", AOPC_area)\n",
    "    print(\"Normalized ABPC area: \", norm_ABPC_area)\n",
    "    print(\"Normalized AOPC area: \", norm_AOPC_area)\"\"\"\n",
    "    \n",
    "    if WANDB:\n",
    "        wandb.log({\n",
    "            \"ABPC_area\": ABPC_area.item(), \n",
    "            \"AOPC\": AOPC_area.item(), \n",
    "            \"norm_ABPC\": norm_ABPC_area.item(), \n",
    "            \"norm_AOPC\": norm_AOPC_area.item()\n",
    "        }, commit=True)\n",
    "\n",
    "    # 6) return the computed values\n",
    "    return morf_curve, lerf_curve, ABPC_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "ee7d1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some parameters for the experiment\n",
    "# if true, we will use a json file containing the bounding box for the Region of Interest (ROI), otherwise we will use a masked segmentation\n",
    "ROI_TYPE = \"BoundingBox\"  # \"BoundingBox\" or \"MaskedSegmentation\"\n",
    "\n",
    "NNUNET_PREPROCESSING = False\n",
    "SUPERVOXEL_TYPE = \"full-organs\" \n",
    "SLIC_N_SUPERVOXELS = 380\n",
    "\n",
    "N_SAMPLES = 1000 if SUPERVOXEL_TYPE == \"full-organs\" else 2000\n",
    "FCC_SUPERVOXEL_SIZE = 100 if SUPERVOXEL_TYPE == \"FCC\" else 50  # in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "4515f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ROI_TYPE == \"BoundingBox\":\n",
    "    BB_ROI_paths = {}\n",
    "    for volume_code in volume_codes:\n",
    "        if IN_KAGGLE:\n",
    "            BB_ROI_paths[volume_code] = f\"{mount_dir}/BB-ROI/AUTOMI_{volume_code}.json\"\n",
    "        else:\n",
    "            BB_ROI_paths[volume_code] = nnUNet_results + \"/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/BB-ROI/\" + f\"AUTOMI_{volume_code}.json\"\n",
    "elif ROI_TYPE == \"MaskedSegmentation\":\n",
    "    # get the manually derived ROI mask from the dataset, where we manually added it\n",
    "    if IN_KAGGLE:\n",
    "        ROI_segmentation_mask_path = \"/kaggle/input/segmentation-masked-ROI.nii\"\n",
    "    else:\n",
    "        ROI_segmentation_mask_path = nnUNet_raw + \"/segmentation-masked-ROI.nii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "bcc398c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_codes = [\"00004\", \"00005\", \"00024\", \"00027\", \"00029\", \"00034\", \"00039\", \"00044\"]\n",
    "#volume_codes = [\"00004\"]\n",
    "#volume_codes = [\"00039\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f1ef5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = [true_positive_aggregation, false_positive_aggregation, dice_aggregation, logit_difference_aggregation]\n",
    "#aggregation_functions = [dice_aggregation, logit_difference_aggregation]\n",
    "#aggregation_functions = [dice_aggregation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "08798e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "attribution_map_paths = {}\n",
    "for volume_code in volume_codes:\n",
    "    for agg_type in aggregation_functions:\n",
    "        if agg_type.__name__ not in attribution_map_paths:\n",
    "            attribution_map_paths[agg_type.__name__] = {}\n",
    "        # for false positive, take the inverted signs for better interpretation of the metric (ORIGINAL DEFINITION OF FP AGGREGATION HAD POSITIVE SIGN FOR LOGITS -> NEGATIVE SIGN IN INFLUENTIAL SUPERVOXELS)\n",
    "        suffix = \"_signed\" if agg_type.__name__ == \"false_positive_aggregation\" else \"\"\n",
    "        attribution_map_paths[agg_type.__name__][volume_code] = Path(join(exp_results_path, SUPERVOXEL_TYPE, agg_type.__name__, volume_code, \"attribution_map\" + suffix + \".nii.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "3a58f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for volume_code in volume_codes:\n",
    "    for agg_type in aggregation_functions:\n",
    "        assert attribution_map_paths[agg_type.__name__][volume_code].exists(), f\"Attribution map not found: {attribution_map_paths[agg_type.__name__][volume_code]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "799e7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_exp = [] # already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on volume code 0: 00004\n",
      "  Shape: (512, 512, 221)\n",
      "Saved binary ROI mask to ROI_binary_mask.nii.gz\n",
      "supervoxel map saved!\n",
      "  Voxel volume: 9.3460 mmÂ³\n",
      "  Using aggregation function: true_positive_aggregation\n",
      "Total volume: 251223095.70 mmÂ³ (26880256 voxels of 9.35 mmÂ³ each)\n",
      "Number of steps for the ABPC computation:  9\n",
      "Total volume of supervoxels: 6718742.56 mmÂ³\n",
      "Step 0: MoRF cumul. volume removed: 0.00 mmÂ³ (0.0 voxels), LeRF cumul. volume removed: 0.00 mmÂ³ (0.0 voxels)\n",
      "Step 1: MoRF cumul. volume removed: 891945.65 mmÂ³ (95436.0 voxels), LeRF cumul. volume removed: 510058.40 mmÂ³ (54575.0 voxels)\n",
      "Step 2: MoRF cumul. volume removed: 1071519.85 mmÂ³ (114650.0 voxels), LeRF cumul. volume removed: 2460103.03 mmÂ³ (263225.0 voxels)\n",
      "Step 3: MoRF cumul. volume removed: 1145727.16 mmÂ³ (122590.0 voxels), LeRF cumul. volume removed: 4971711.92 mmÂ³ (531961.0 voxels)\n",
      "Step 4: MoRF cumul. volume removed: 1474678.61 mmÂ³ (157787.0 voxels), LeRF cumul. volume removed: 5166090.20 mmÂ³ (552759.0 voxels)\n",
      "Step 5: MoRF cumul. volume removed: 1552652.36 mmÂ³ (166130.0 voxels), LeRF cumul. volume removed: 5244063.95 mmÂ³ (561102.0 voxels)\n",
      "Step 6: MoRF cumul. volume removed: 1747030.64 mmÂ³ (186928.0 voxels), LeRF cumul. volume removed: 5573015.40 mmÂ³ (596299.0 voxels)\n",
      "Step 7: MoRF cumul. volume removed: 4258639.53 mmÂ³ (455664.0 voxels), LeRF cumul. volume removed: 5647222.71 mmÂ³ (604239.0 voxels)\n",
      "Step 8: MoRF cumul. volume removed: 6208684.16 mmÂ³ (664314.0 voxels), LeRF cumul. volume removed: 5826796.91 mmÂ³ (623453.0 voxels)\n",
      "Step 9: MoRF cumul. volume removed: 6718742.56 mmÂ³ (718889.0 voxels), LeRF cumul. volume removed: 6718742.56 mmÂ³ (718889.0 voxels)\n",
      "  Using aggregation function: false_positive_aggregation\n",
      "Total volume: 251223095.70 mmÂ³ (26880256 voxels of 9.35 mmÂ³ each)\n",
      "Number of steps for the ABPC computation:  9\n",
      "Total volume of supervoxels: 6718742.56 mmÂ³\n",
      "Step 0: MoRF cumul. volume removed: 0.00 mmÂ³ (0.0 voxels), LeRF cumul. volume removed: 0.00 mmÂ³ (0.0 voxels)\n",
      "Step 1: MoRF cumul. volume removed: 179574.20 mmÂ³ (19214.0 voxels), LeRF cumul. volume removed: 194378.28 mmÂ³ (20798.0 voxels)\n",
      "Step 2: MoRF cumul. volume removed: 1071519.85 mmÂ³ (114650.0 voxels), LeRF cumul. volume removed: 272352.03 mmÂ³ (29141.0 voxels)\n",
      "Step 3: MoRF cumul. volume removed: 3583128.74 mmÂ³ (383386.0 voxels), LeRF cumul. volume removed: 601303.48 mmÂ³ (64338.0 voxels)\n",
      "Step 4: MoRF cumul. volume removed: 5533173.37 mmÂ³ (592036.0 voxels), LeRF cumul. volume removed: 1111361.89 mmÂ³ (118913.0 voxels)\n",
      "Step 5: MoRF cumul. volume removed: 5607380.68 mmÂ³ (599976.0 voxels), LeRF cumul. volume removed: 1185569.19 mmÂ³ (126853.0 voxels)\n",
      "Step 6: MoRF cumul. volume removed: 6117439.08 mmÂ³ (654551.0 voxels), LeRF cumul. volume removed: 3135613.82 mmÂ³ (335503.0 voxels)\n",
      "Step 7: MoRF cumul. volume removed: 6446390.53 mmÂ³ (689748.0 voxels), LeRF cumul. volume removed: 5647222.71 mmÂ³ (604239.0 voxels)\n"
     ]
    }
   ],
   "source": [
    "for i, volume_code in enumerate(volume_codes):\n",
    "    print(f\"Working on volume code {i}: {volume_code}\")\n",
    "    print(f\"  Shape: {nib.load(ct_img_paths[volume_code]).shape}\")\n",
    "\n",
    "    os.mkdir(volume_code) if not os.path.exists(volume_code) else None\n",
    "\n",
    "    volume, affine, supervoxel_map, segmentation_mask, ROI_mask, cache_dict = prepare_data_for_shap(\n",
    "        volume_path=ct_img_paths[volume_code],\n",
    "        patch_size=patch_size,\n",
    "        predictor=predictor,\n",
    "        dataset_json_path=dataset_json_path,\n",
    "        ROI_BB_path=BB_ROI_paths[volume_code] if ROI_TYPE == \"BoundingBox\" else None,\n",
    "        ROI_segmentation_mask_path=ROI_segmentation_mask_path if ROI_TYPE == \"MaskedSegmentation\" else None,\n",
    "        ROI_type=ROI_TYPE,\n",
    "        nnunet_preprocessing=NNUNET_PREPROCESSING,\n",
    "        supervoxel_type=SUPERVOXEL_TYPE,\n",
    "        fcc_supervoxel_size=FCC_SUPERVOXEL_SIZE,\n",
    "        slic_n_supervoxels=SLIC_N_SUPERVOXELS,\n",
    "        organ_map_path=organ_map_paths[volume_code],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Compute voxel volume for this specific volume\n",
    "    voxel_volume_mm3 = compute_voxel_volume_mm3(nib.load(ct_img_paths[volume_code]))\n",
    "    print(f\"  Voxel volume: {voxel_volume_mm3:.4f} mmÂ³\")\n",
    "    \n",
    "    for agg_fn in aggregation_functions:\n",
    "        print(f\"  Using aggregation function: {agg_fn.__name__}\")\n",
    "        if (volume_code, agg_fn.__name__) in skip_exp:\n",
    "            print(\"  Skipping this experiment as already done\")\n",
    "            continue\n",
    "\n",
    "        if WANDB:\n",
    "            # Detect device name\n",
    "            if device.type == \"cuda\":\n",
    "                GPU_NAME = torch.cuda.get_device_name(device)\n",
    "            else:\n",
    "                GPU_NAME = \"CPU\"\n",
    "                \n",
    "            wandb_run = wandb.init(\n",
    "                project=\"automi\",  # your W&B project name\n",
    "                #name=\"run_01\",                 # optional, name for this run\n",
    "                config={\n",
    "                        \"device_name\": GPU_NAME,\n",
    "                        \"volume_code\": volume_code,\n",
    "                        \"n_samples\": N_SAMPLES,\n",
    "                        \"supervoxel_type\": SUPERVOXEL_TYPE,\n",
    "                        \"fcc_supervoxel_size\": FCC_SUPERVOXEL_SIZE,\n",
    "                        \"slic_n_supervoxels\": SLIC_N_SUPERVOXELS,\n",
    "                        \"nnunet_preprocessing\": NNUNET_PREPROCESSING,\n",
    "                        \"aggregation_function\": agg_fn.__name__,\n",
    "                        \"group\": \"ABPC-volumes\",\n",
    "                        \"voxel_volume_mm3\": voxel_volume_mm3,\n",
    "                    }\n",
    "            )\n",
    "\n",
    "        # 0) get the pre-computed attribution map\n",
    "        attribution_map = torch.from_numpy(nib.load(attribution_map_paths[agg_fn.__name__][volume_code]).get_fdata().transpose(2,1,0)).float().to(device)\n",
    "\n",
    "        \n",
    "        # a) wrap the aggregation function\n",
    "        forward_func = lambda vol, mask, predictor: forward_segmentation_output_to_explain(\n",
    "            input_image=vol,\n",
    "            perturbation_mask=mask,\n",
    "            segmentation_mask=segmentation_mask,\n",
    "            ROI_bounding_box_mask=ROI_mask,\n",
    "            baseline_prediction_dict=cache_dict,\n",
    "            predictor=predictor,\n",
    "            aggregation_fn=agg_fn\n",
    "        )\n",
    "\n",
    "        # get the two curves and the area between them\n",
    "        aopc_curve, abpc_curve, aopc_area = compute_ABPC_curves_with_volumes(\n",
    "            original_volume=volume,\n",
    "            feature_map=supervoxel_map,\n",
    "            attribution_map=attribution_map,\n",
    "            forward_function=forward_func,\n",
    "            morf_predictor=morf_predictor,\n",
    "            lerf_predictor=lerf_predictor,\n",
    "            attribute_background=(SUPERVOXEL_TYPE not in [\"FCC-organs\", \"full-organs\"]),\n",
    "            voxel_volume_mm3=voxel_volume_mm3\n",
    "        )\n",
    "\n",
    "        if WANDB:\n",
    "            wandb_run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7037513,
     "sourceId": 12710655,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
