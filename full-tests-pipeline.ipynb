{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa6542d",
   "metadata": {
    "id": "THifmOYu9Cip",
    "papermill": {
     "duration": 0.011347,
     "end_time": "2025-07-01T13:42:04.276700",
     "exception": false,
     "start_time": "2025-07-01T13:42:04.265353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Import Packages for the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89667483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:42:04.297794Z",
     "iopub.status.busy": "2025-07-01T13:42:04.297491Z",
     "iopub.status.idle": "2025-07-01T13:42:09.309903Z",
     "shell.execute_reply": "2025-07-01T13:42:09.309241Z"
    },
    "id": "k-hj38QV_raZ",
    "papermill": {
     "duration": 5.024391,
     "end_time": "2025-07-01T13:42:09.311244",
     "exception": false,
     "start_time": "2025-07-01T13:42:04.286853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import basic packages for later use\n",
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81698fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:42:09.335388Z",
     "iopub.status.busy": "2025-07-01T13:42:09.334993Z",
     "iopub.status.idle": "2025-07-01T13:42:09.425783Z",
     "shell.execute_reply": "2025-07-01T13:42:09.425166Z"
    },
    "id": "Ld3_-quILggy",
    "papermill": {
     "duration": 0.103844,
     "end_time": "2025-07-01T13:42:09.426905",
     "exception": false,
     "start_time": "2025-07-01T13:42:09.323061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check whether GPU accelerated computing is available\n",
    "assert torch.cuda.is_available() # if there is an error here, enable GPU in the Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03893534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:42:09.448487Z",
     "iopub.status.busy": "2025-07-01T13:42:09.448245Z",
     "iopub.status.idle": "2025-07-01T13:43:57.143436Z",
     "shell.execute_reply": "2025-07-01T13:43:57.142615Z"
    },
    "id": "c4KbMYcEWZjU",
    "outputId": "ba9a5149-e800-477a-9717-4f4796903bc1",
    "papermill": {
     "duration": 107.707152,
     "end_time": "2025-07-01T13:43:57.144981",
     "exception": false,
     "start_time": "2025-07-01T13:42:09.437829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnunetv2\r\n",
      "  Downloading nnunetv2-2.6.2.tar.gz (211 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.6.0+cu124)\r\n",
      "Collecting acvl-utils<0.3,>=0.2.3 (from nnunetv2)\r\n",
      "  Downloading acvl_utils-0.2.5.tar.gz (29 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting dynamic-network-architectures<0.5,>=0.4.1 (from nnunetv2)\r\n",
      "  Downloading dynamic_network_architectures-0.4.2.tar.gz (28 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (4.67.1)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.15.2)\r\n",
      "Collecting batchgenerators>=0.25.1 (from nnunetv2)\r\n",
      "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.2.2)\r\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.25.2)\r\n",
      "Requirement already satisfied: SimpleITK>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.5.0)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.2.3)\r\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.20.3)\r\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2025.3.30)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.32.3)\r\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (5.3.2)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (3.7.2)\r\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.12.2)\r\n",
      "Collecting imagecodecs (from nnunetv2)\r\n",
      "  Downloading imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Collecting yacs (from nnunetv2)\r\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\r\n",
      "Collecting batchgeneratorsv2>=0.3.0 (from nnunetv2)\r\n",
      "  Downloading batchgeneratorsv2-0.3.0.tar.gz (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.8.1)\r\n",
      "Requirement already satisfied: blosc2>=3.0.0b1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (3.2.1)\r\n",
      "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2.3->nnunetv2)\r\n",
      "  Downloading connected_components_3d-3.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (11.1.0)\r\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (1.0.0)\r\n",
      "Collecting unittest2 (from batchgenerators>=0.25.1->nnunetv2)\r\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (3.6.0)\r\n",
      "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.3.0->nnunetv2)\r\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Requirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (1.9.2)\r\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (1.1.0)\r\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (4.3.8)\r\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (2.10.2)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (9.0.0)\r\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.0.15)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2.4.1)\r\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\r\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (2.37.0)\r\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (25.0)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (4.13.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.2->nnunetv2)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.2->nnunetv2) (1.3.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (4.57.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (1.4.8)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (2.9.0.post0)\r\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->nnunetv2) (6.5.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunetv2) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunetv2) (2025.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (2025.4.26)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nnunetv2) (1.5.0)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs->nnunetv2) (6.0.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->nnunetv2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->nnunetv2) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->nnunetv2) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->nnunetv2) (2024.2.0)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.5.3)\r\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\r\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting traceback2 (from unittest2->batchgenerators>=0.25.1->nnunetv2)\r\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->nnunetv2) (2024.2.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.1.0)\r\n",
      "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2)\r\n",
      "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading imagecodecs-2025.3.30-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\r\n",
      "Downloading connected_components_3d-3.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\r\n",
      "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\r\n",
      "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\r\n",
      "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.6.2-py3-none-any.whl size=285890 sha256=f60e364e1f31e89ffe75c93b4a2a23dca130b1c9373650a7e3d1b543d6f66aad\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/ec/d2/0fb1be0015c40f2dc99535af585e41e876dd2b369039d9385b\r\n",
      "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for acvl-utils: filename=acvl_utils-0.2.5-py3-none-any.whl size=27213 sha256=ebe749b6f0024c69c8d56d3c44c72c8c32dfe409b646b57f5b744fb6f8887d16\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/8c/10/dcba79e0b2d1d463605233cec1fc6cfad47af5230b8985e464\r\n",
      "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=5ad03ef9dea4bd8dc3215f6b552a548f68d917540763ff77777ec42aa657b9a8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/11/c7/fadca30e054c602093ffe36ba8a2f0a87dd2f86ac75191d3ed\r\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.3.0-py3-none-any.whl size=65215 sha256=29dc029d0c34e57742d80da9f3ad3b7a55fafcace6f03163c92d5229f73d28f4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/c1/8f/94ca60255dbbadf27e1da4885002a6943c95b067b8e2dd39ea\r\n",
      "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.4.2-py3-none-any.whl size=39025 sha256=d58b0ed7fd7fea852e199b86046b80a899f9d136e24d13c7726a9ecfbfd76fe1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/07/a9/c3/fcdf69ef4481860db91d0dc2466f63df8c3933262424a23f54\r\n",
      "Successfully built nnunetv2 acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\r\n",
      "Installing collected packages: linecache2, argparse, yacs, traceback2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, unittest2, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fft-conv-pytorch, connected-components-3d, batchgenerators, imagecodecs, dynamic-network-architectures, batchgeneratorsv2, acvl-utils, nnunetv2\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "Successfully installed acvl-utils-0.2.5 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.3.0 connected-components-3d-3.24.0 dynamic-network-architectures-0.4.2 fft-conv-pytorch-1.2.0 imagecodecs-2025.3.30 linecache2-1.0.0 nnunetv2-2.6.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\r\n",
      "Collecting captum\r\n",
      "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.7.2)\r\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (25.0)\r\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.57.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->captum) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->captum) (2024.2.0)\r\n",
      "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: captum\r\n",
      "Successfully installed captum-0.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nnunetv2\n",
    "!pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd260d4f",
   "metadata": {
    "id": "tgssjiuNVvq5",
    "papermill": {
     "duration": 0.033448,
     "end_time": "2025-07-01T13:43:57.213117",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.179669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Mount the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dc70e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:43:57.284280Z",
     "iopub.status.busy": "2025-07-01T13:43:57.283515Z",
     "iopub.status.idle": "2025-07-01T13:43:57.542200Z",
     "shell.execute_reply": "2025-07-01T13:43:57.541345Z"
    },
    "id": "_WLi-mVRjbfb",
    "outputId": "e8440113-7302-4a08-9b64-d35179d4e3b4",
    "papermill": {
     "duration": 0.296493,
     "end_time": "2025-07-01T13:43:57.543505",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.247012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "code  nnunet_raw  preprocessed_files  results  segmentation-masked-ROI.nii\r\n",
      "/bin/bash: line 1: cd: /kaggle/input/automi-seg: No such file or directory\r\n",
      "__notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "\n",
    "# Google Colab\n",
    "\"\"\"# for colab users only - mounting the drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount = True)\n",
    "\n",
    "drive_dir = \"/content/drive/My Drive\"\n",
    "mount_dir = join(drive_dir, \"tesi\", \"automi\")\n",
    "base_dir = os.getcwd()\"\"\"\n",
    "\n",
    "# Kaggle\n",
    "mount_dir = \"/kaggle/input/automi-seg\"\n",
    "base_dir = os.getcwd()\n",
    "print(base_dir)\n",
    "!ls '/kaggle/input'\n",
    "!cd \"/kaggle/input/automi-seg\" ; ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9b255",
   "metadata": {
    "id": "d-CV4Jc4W77P",
    "papermill": {
     "duration": 0.034919,
     "end_time": "2025-07-01T13:43:57.658634",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.623715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Setting up nnU-Nets folder structure and environment variables\n",
    "nnUnet expects a certain folder structure and environment variables.\n",
    "\n",
    "Roughly they tell nnUnet:\n",
    "1. Where to look for stuff\n",
    "2. Where to put stuff\n",
    "\n",
    "For more information about this please check: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dba02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:43:57.727015Z",
     "iopub.status.busy": "2025-07-01T13:43:57.726200Z",
     "iopub.status.idle": "2025-07-01T13:43:57.731287Z",
     "shell.execute_reply": "2025-07-01T13:43:57.730749Z"
    },
    "id": "T9ifLrYhjfAT",
    "papermill": {
     "duration": 0.040491,
     "end_time": "2025-07-01T13:43:57.732352",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.691861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_if_dont_exist(folder_path,overwrite=False):\n",
    "    \"\"\"\n",
    "    creates a folder if it does not exists\n",
    "    input:\n",
    "    folder_path : relative path of the folder which needs to be created\n",
    "    over_write :(default: False) if True overwrite the existing folder\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder_path):\n",
    "\n",
    "        if not overwrite:\n",
    "            print(f\"{folder_path} exists.\")\n",
    "        else:\n",
    "            print(f\"{folder_path} overwritten\")\n",
    "            shutil.rmtree(folder_path)\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    else:\n",
    "      os.makedirs(folder_path)\n",
    "      print(f\"{folder_path} created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312d88d",
   "metadata": {
    "id": "JAvjVPF0_7t3",
    "papermill": {
     "duration": 0.032842,
     "end_time": "2025-07-01T13:43:57.799389",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.766547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Set environment Variables and creating folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8709b2fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:43:57.868981Z",
     "iopub.status.busy": "2025-07-01T13:43:57.868757Z",
     "iopub.status.idle": "2025-07-01T13:43:57.874204Z",
     "shell.execute_reply": "2025-07-01T13:43:57.873372Z"
    },
    "id": "3rlqq-V-CWh8",
    "outputId": "e16ea0fc-57b8-4478-e8df-14942c962c9c",
    "papermill": {
     "duration": 0.041157,
     "end_time": "2025-07-01T13:43:57.875239",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.834082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: /kaggle/input/nnunet_raw\n",
      "nnUNet_results: /kaggle/input/results\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 📦 SETUP nnUNet ENVIRONMENT\n",
    "# ===========================\n",
    "\n",
    "# Definisci i path da settare\n",
    "path_dict = {\n",
    "    \"nnUNet_raw\": join(mount_dir, \"nnunet_raw\"),\n",
    "    \"nnUNet_preprocessed\": join(mount_dir, \"preprocessed_files\"),#\"nnUNet_preprocessed\"),\n",
    "    \"nnUNet_results\": join(mount_dir, \"results\"),#\"nnUNet_results\"),\n",
    "    # \"RAW_DATA_PATH\": join(mount_dir, \"RawData\"),  # Facoltativo, se ti serve salvare zips\n",
    "}\n",
    "\n",
    "# Scrivi i path nelle variabili di ambiente, che vengono lette dal modulo paths di nnunetv2\n",
    "for env_var, path in path_dict.items():\n",
    "    os.environ[env_var] = path\n",
    "\n",
    "\"\"\"from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "\n",
    "print(\"nnUNet_raw:\", nnUNet_raw)\n",
    "print(\"nnUNet_results:\", nnUNet_results)\n",
    "if nnUNet_raw == None:\n",
    "    nnUNet_raw = \"/kaggle/input/automi-seg/nnunet_raw\"\n",
    "if nnUNet_results == None:\n",
    "    nnUNet_results = \"/kaggle/input/automi-seg/results\"\"\"\n",
    "# Kaggle has some very unconsistent behaviors in dataset mounting...\n",
    "nnUNet_raw = \"/kaggle/input/nnunet_raw\"\n",
    "nnUNet_results = \"/kaggle/input/results\"\n",
    "print(\"nnUNet_raw:\", nnUNet_raw)\n",
    "print(\"nnUNet_results:\", nnUNet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025a966",
   "metadata": {
    "papermill": {
     "duration": 0.034303,
     "end_time": "2025-07-01T13:43:57.944025",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.909722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9652b993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:43:58.010542Z",
     "iopub.status.busy": "2025-07-01T13:43:58.009901Z",
     "iopub.status.idle": "2025-07-01T13:43:58.040804Z",
     "shell.execute_reply": "2025-07-01T13:43:58.039935Z"
    },
    "id": "RO5hP5Ao79QG",
    "papermill": {
     "duration": 0.065607,
     "end_time": "2025-07-01T13:43:58.042104",
     "exception": false,
     "start_time": "2025-07-01T13:43:57.976497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ct_img_path = join(nnUNet_raw, \"imagesTr\", \"AUTOMI_00039_0000.nii\")\n",
    "organ_mask_path = join(nnUNet_raw, \"total_segmentator_structures\", \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\")\n",
    "ct_img = nib.load(ct_img_path)\n",
    "organ_mask = nib.load(organ_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3dfc554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:43:58.109747Z",
     "iopub.status.busy": "2025-07-01T13:43:58.109513Z",
     "iopub.status.idle": "2025-07-01T13:43:58.113979Z",
     "shell.execute_reply": "2025-07-01T13:43:58.113370Z"
    },
    "id": "0RGc0dNi9Wop",
    "outputId": "770675ee-5426-493d-a037-b7fdf58afe5d",
    "papermill": {
     "duration": 0.039316,
     "end_time": "2025-07-01T13:43:58.115177",
     "exception": false,
     "start_time": "2025-07-01T13:43:58.075861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT shape: (512, 512, 283)\n",
      "Organ shape: (512, 512, 283)\n",
      "Spacing: (1.171875, 1.171875, 5.0)\n",
      "Organ spacing: (1.171875, 1.171875, 5.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"CT shape:\", ct_img.shape)\n",
    "print(\"Organ shape:\", organ_mask.shape)\n",
    "print(\"Spacing:\", ct_img.header.get_zooms())\n",
    "print(\"Organ spacing:\", organ_mask.header.get_zooms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730de5d",
   "metadata": {
    "papermill": {
     "duration": 0.032896,
     "end_time": "2025-07-01T13:43:58.181544",
     "exception": false,
     "start_time": "2025-07-01T13:43:58.148648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Re-align CT scan with its own organ segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e215316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:43:58.248375Z",
     "iopub.status.busy": "2025-07-01T13:43:58.248096Z",
     "iopub.status.idle": "2025-07-01T13:44:00.946656Z",
     "shell.execute_reply": "2025-07-01T13:44:00.946021Z"
    },
    "id": "2UGYxNOXBhmN",
    "papermill": {
     "duration": 2.733076,
     "end_time": "2025-07-01T13:44:00.947943",
     "exception": false,
     "start_time": "2025-07-01T13:43:58.214867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "# Load CT and misaligned organ mask\n",
    "ct = sitk.ReadImage(ct_img_path, sitk.sitkFloat32)\n",
    "organ_mask = sitk.ReadImage(organ_mask_path, sitk.sitkUInt8)\n",
    "\n",
    "# Resample organ mask to match CT space\n",
    "resampler = sitk.ResampleImageFilter()\n",
    "resampler.SetReferenceImage(ct)\n",
    "resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "organ_resampled = resampler.Execute(organ_mask)\n",
    "\n",
    "# Save aligned output\n",
    "sitk.WriteImage(organ_resampled, \"organ_mask_resampled_to_ct.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1740c993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:01.015012Z",
     "iopub.status.busy": "2025-07-01T13:44:01.014778Z",
     "iopub.status.idle": "2025-07-01T13:44:01.033735Z",
     "shell.execute_reply": "2025-07-01T13:44:01.033012Z"
    },
    "id": "Fq-CUTS2Omeq",
    "outputId": "1ab8dd02-93d3-4430-9a08-ec72940d36a0",
    "papermill": {
     "duration": 0.053329,
     "end_time": "2025-07-01T13:44:01.034839",
     "exception": false,
     "start_time": "2025-07-01T13:44:00.981510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT shape: (512, 512, 283)\n",
      "Organ shape: (512, 512, 283)\n",
      "Spacing: (1.171875, 1.171875, 5.0)\n",
      "Organ spacing: (1.171875, 1.171875, 5.0)\n"
     ]
    }
   ],
   "source": [
    "#organ_mask_path = join(nnUNet_raw, \"organ_mask_resampled_to_ct.nii.gz\")\n",
    "organ_mask_path = \"organ_mask_resampled_to_ct.nii.gz\"\n",
    "ct_img = nib.load(ct_img_path)\n",
    "organ_mask = nib.load(organ_mask_path)\n",
    "print(\"CT shape:\", ct_img.shape)\n",
    "print(\"Organ shape:\", organ_mask.shape)\n",
    "print(\"Spacing:\", ct_img.header.get_zooms())\n",
    "print(\"Organ spacing:\", organ_mask.header.get_zooms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edfe1471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:01.102115Z",
     "iopub.status.busy": "2025-07-01T13:44:01.101876Z",
     "iopub.status.idle": "2025-07-01T13:44:01.104922Z",
     "shell.execute_reply": "2025-07-01T13:44:01.104438Z"
    },
    "papermill": {
     "duration": 0.037795,
     "end_time": "2025-07-01T13:44:01.105925",
     "exception": false,
     "start_time": "2025-07-01T13:44:01.068130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model directory; note that this is readonly in Kaggle environment\n",
    "model_dir = join(nnUNet_results, 'Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c5363",
   "metadata": {
    "papermill": {
     "duration": 0.032536,
     "end_time": "2025-07-01T13:44:01.172533",
     "exception": false,
     "start_time": "2025-07-01T13:44:01.139997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utility to export logits to a visualizable segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10aef67a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:01.238345Z",
     "iopub.status.busy": "2025-07-01T13:44:01.238098Z",
     "iopub.status.idle": "2025-07-01T13:44:03.266045Z",
     "shell.execute_reply": "2025-07-01T13:44:03.265248Z"
    },
    "papermill": {
     "duration": 2.062597,
     "end_time": "2025-07-01T13:44:03.267468",
     "exception": false,
     "start_time": "2025-07-01T13:44:01.204871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from nnunetv2.configuration import default_num_processes\n",
    "from nnunetv2.inference.export_prediction import export_prediction_from_logits\n",
    "\n",
    "def export_logits_to_nifty_segmentation(\n",
    "    predictor,\n",
    "    volume_file: Path,\n",
    "    model_dir: str,\n",
    "    logits: Union[str, np.ndarray, torch.Tensor],\n",
    "    npz_dir: str | None,\n",
    "    output_dir: str = \"\",\n",
    "    fold: int = 0,\n",
    "    save_probs: bool = False,\n",
    "    from_file: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts a saved .npz logits file into a native-space NIfTI segmentation using nnU-Net's helper.\n",
    "\n",
    "    Args:\n",
    "        predictor: An instantiated nnU-Net predictor object with loaded plans/configs.\n",
    "        volume_file: An Path object pointing to the raw image file.\n",
    "        model_dir (str): Path to the nnU-Net mode1Introductionl directory containing dataset.json.\n",
    "        logits (str or tensor): Base name of the .npz logits file (no extension), when from_file=True\n",
    "        npz_dir (str): Directory where the .npz file is stored.\n",
    "        output_dir (str): Directory where the .nii.gz segmentation will be saved.\n",
    "        fold (int): The fold number used for prediction (default is 0).\n",
    "        save_probs (bool): Whether to save softmax probabilities as a .npz file.\n",
    "        from_file (bool). Whether to convert from a file instead of from the logits (default true)\n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        npz_logits = Path(npz_dir) / f\"{logits}.npz\"\n",
    "        output_nii = Path(output_dir) / f\"{logits}_seg.nii.gz\"\n",
    "        logits = np.load(npz_logits)[\"logits\"]\n",
    "    else:\n",
    "        output_nii = Path(output_dir) / \"exported_seg.nii.gz\"\n",
    "\n",
    "    plans_manager = predictor.plans_manager\n",
    "    configuration_manager = predictor.configuration_manager\n",
    "    dataset_json = Path(model_dir) / \"dataset.json\"\n",
    "\n",
    "    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n",
    "    rw = plans_manager.image_reader_writer_class()\n",
    "    if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "        rw = rw()\n",
    "    img_np, img_props = rw.read_images([str(volume_file)])\n",
    "\n",
    "    _, _, data_props = preprocessor.run_case_npy(\n",
    "        img_np, seg=None, properties=img_props,\n",
    "        plans_manager=plans_manager,\n",
    "        configuration_manager=configuration_manager,\n",
    "        dataset_json=dataset_json\n",
    "    )\n",
    "\n",
    "\n",
    "    export_prediction_from_logits(\n",
    "        predicted_array_or_file=logits,\n",
    "        properties_dict=data_props,\n",
    "        configuration_manager=configuration_manager,\n",
    "        plans_manager=plans_manager,\n",
    "        dataset_json_dict_or_file=str(dataset_json),\n",
    "        output_file_truncated=os.path.splitext(str(output_nii))[0],\n",
    "        save_probabilities=save_probs,\n",
    "        num_threads_torch=default_num_processes\n",
    "    )\n",
    "\n",
    "    print(f\"✅  NIfTI segmentation written → {output_nii}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7289865e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:03.338828Z",
     "iopub.status.busy": "2025-07-01T13:44:03.337966Z",
     "iopub.status.idle": "2025-07-01T13:44:03.343486Z",
     "shell.execute_reply": "2025-07-01T13:44:03.342787Z"
    },
    "papermill": {
     "duration": 0.041632,
     "end_time": "2025-07-01T13:44:03.344625",
     "exception": false,
     "start_time": "2025-07-01T13:44:03.302993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'export_logits_to_nifty_segmentation(\\n    predictor=predictor,\\n    plan=plan,\\n    model_dir=Path(model_dir),\\n    logits_filename=\"pred_00007\",\\n    npz_dir=\"SHAP/shap_run\",\\n    output_dir=\"SHAP/shap_run\",\\n    fold=0,\\n    save_probs=False\\n)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"export_logits_to_nifty_segmentation(\n",
    "    predictor=predictor,\n",
    "    plan=plan,\n",
    "    model_dir=Path(model_dir),\n",
    "    logits_filename=\"pred_00007\",\n",
    "    npz_dir=\"SHAP/shap_run\",\n",
    "    output_dir=\"SHAP/shap_run\",\n",
    "    fold=0,\n",
    "    save_probs=False\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99f085",
   "metadata": {
    "papermill": {
     "duration": 0.032761,
     "end_time": "2025-07-01T13:44:03.410728",
     "exception": false,
     "start_time": "2025-07-01T13:44:03.377967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## We define a sliding window caching for faster multi-inference scenario, like SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c6fe9",
   "metadata": {
    "papermill": {
     "duration": 0.032618,
     "end_time": "2025-07-01T13:44:03.476087",
     "exception": false,
     "start_time": "2025-07-01T13:44:03.443469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Try to override the sliding_window_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ba8cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:03.544539Z",
     "iopub.status.busy": "2025-07-01T13:44:03.544248Z",
     "iopub.status.idle": "2025-07-01T13:44:09.895409Z",
     "shell.execute_reply": "2025-07-01T13:44:09.894594Z"
    },
    "papermill": {
     "duration": 6.387699,
     "end_time": "2025-07-01T13:44:09.896870",
     "exception": false,
     "start_time": "2025-07-01T13:44:03.509171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from acvl_utils.cropping_and_padding.padding import pad_nd_image\n",
    "from nnunetv2.utilities.helpers import empty_cache, dummy_context\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.inference.sliding_window_prediction import compute_gaussian, compute_steps_for_sliding_window\n",
    "\n",
    "class CustomNNUNetPredictor(nnUNetPredictor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_sliding_window_return_logits_with_caching(self, input_image: torch.Tensor,\n",
    "                                                          perturbation_mask: torch.BoolTensor,\n",
    "                                                          baseline_prediction_dict: dict) \\\n",
    "            -> Union[np.ndarray, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Method predict_sliding_window_return_logits taken from official nnunetv2 documentation:\n",
    "        https://github.com/MIC-DKFZ/nnUNet/blob/58a3b121a6d1846a978306f6c79a7c005b7d669b/nnunetv2/inference/predict_from_raw_data.py\n",
    "        We add a perturbation_mask parameter to check each patch for the actual presence of a perturbation\n",
    "        \"\"\"\n",
    "        assert isinstance(input_image, torch.Tensor)\n",
    "        self.network = self.network.to(self.device)\n",
    "        self.network.eval()\n",
    "\n",
    "        empty_cache(self.device)\n",
    "\n",
    "        # DEBUG --------------\n",
    "        \"\"\"voxels  = np.prod(input_image.shape[1:])          # (X*Y*Z)\n",
    "        bytes_per_voxel = 2                        # fp16\n",
    "        needed  = voxels * self.label_manager.num_segmentation_heads * bytes_per_voxel\n",
    "        print(f\"≈{needed/1e9:.1f} GB per predicted_logits\")\"\"\"\n",
    "\n",
    "        # Autocast can be annoying\n",
    "        # If the device_type is 'cpu' then it's slow as heck on some CPUs (no auto bfloat16 support detection)\n",
    "        # and needs to be disabled.\n",
    "        # If the device_type is 'mps' then it will complain that mps is not implemented, even if enabled=False\n",
    "        # is set. Whyyyyyyy. (this is why we don't make use of enabled=False)\n",
    "        # So autocast will only be active if we have a cuda device.\n",
    "        with torch.autocast(self.device.type, enabled=True) if self.device.type == 'cuda' else dummy_context():\n",
    "            assert input_image.ndim == 4, 'input_image must be a 4D np.ndarray or torch.Tensor (c, x, y, z)'\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'Input shape: {input_image.shape}')\n",
    "                print(\"step_size:\", self.tile_step_size)\n",
    "                print(\"mirror_axes:\", self.allowed_mirroring_axes if self.use_mirroring else None)\n",
    "                print(f'Perturbation mask shape: {perturbation_mask.shape}')\n",
    "\n",
    "\n",
    "            # if input_image is smaller than tile_size we need to pad it to tile_size.\n",
    "            data, slicer_revert_padding = pad_nd_image(input_image, self.configuration_manager.patch_size,\n",
    "                                                       'constant', {'value': 0}, True,\n",
    "                                                       None)\n",
    "\n",
    "            # slicers can be applied to both perturbed volume and \n",
    "            slicers = self._internal_get_sliding_window_slicers(data.shape[1:])\n",
    "\n",
    "            if self.perform_everything_on_device and self.device != 'cpu':\n",
    "                # behavior changed\n",
    "                try:\n",
    "                    predicted_logits = self._internal_predict_sliding_window_return_logits(\n",
    "                        data, slicers, True, perturbation_mask, baseline_prediction_dict, caching=True\n",
    "                    )\n",
    "                except RuntimeError as e:\n",
    "                    if \"CUDA out of memory\" in str(e):\n",
    "                        print(\"⚠️  CUDA OOM, cambiare batch size o patch size!\")\n",
    "                        raise\n",
    "                    else:\n",
    "                        # Mostra l'errore reale e aborta: niente CPU fallback\n",
    "                        raise\n",
    "            else:\n",
    "                predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n",
    "                                                                                       self.perform_everything_on_device)\n",
    "\n",
    "            empty_cache(self.device)\n",
    "            # revert padding\n",
    "            predicted_logits = predicted_logits[(slice(None), *slicer_revert_padding[1:])]\n",
    "        return predicted_logits\n",
    "                \n",
    "\n",
    "    def _slice_key(self, slicer_tuple):\n",
    "        # make slicer object hashable to use it for cache lookup\n",
    "        return tuple((s.start, s.stop, s.step) for s in slicer_tuple)\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def _internal_predict_sliding_window_return_logits(self,\n",
    "                                                       data: torch.Tensor,\n",
    "                                                       slicers,\n",
    "                                                       do_on_device: bool = True,\n",
    "                                                       perturbation_mask: torch.BoolTensor | None = None,\n",
    "                                                       baseline_prediction_dict: dict | None = None,\n",
    "                                                       caching: bool = False,\n",
    "                                                       ):\n",
    "        \"\"\"\n",
    "        Modified to manage the caching of patches\n",
    "        \"\"\"\n",
    "        predicted_logits = n_predictions = prediction = gaussian = workon = None\n",
    "        results_device = self.device if do_on_device else torch.device('cpu')\n",
    "        if next(self.network.parameters()).device != results_device:\n",
    "            self.network = self.network.to(results_device)\n",
    "\n",
    "        def producer(d, slh, q):\n",
    "            for s in slh:\n",
    "                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(results_device), s))\n",
    "            q.put('end')\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data to device\n",
    "            if self.verbose:\n",
    "                print(f'move image to device {results_device}')\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            # preallocate arrays\n",
    "            if self.verbose:\n",
    "                print(f'preallocating results arrays on device {results_device}')\n",
    "            predicted_logits = torch.zeros((self.label_manager.num_segmentation_heads, *data.shape[1:]),\n",
    "                                           dtype=torch.half,\n",
    "                                           device=results_device)\n",
    "            n_predictions = torch.zeros(data.shape[1:], dtype=torch.half, device=results_device)\n",
    "\n",
    "            if self.use_gaussian:\n",
    "                gaussian = compute_gaussian(tuple(self.configuration_manager.patch_size), sigma_scale=1. / 8,\n",
    "                                            value_scaling_factor=10,\n",
    "                                            device=results_device)\n",
    "            else:\n",
    "                gaussian = 1\n",
    "\n",
    "        \n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                cache_hits = 0\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    try:\n",
    "                        if caching and not self.check_overlapping(sl, perturbation_mask):\n",
    "                            prediction = baseline_prediction_dict[self._slice_key(sl)].to(results_device)\n",
    "                            cache_hits += 1\n",
    "                        else:\n",
    "                            prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "                    except Exception as e:\n",
    "                        raise RuntimeError(\"Errore nella predizione del patch\") from e\n",
    "\n",
    "                    # 2) sanity-check device\n",
    "                    assert prediction.device == predicted_logits.device\n",
    "\n",
    "                    if self.use_gaussian:\n",
    "                        prediction *= gaussian\n",
    "                    predicted_logits[sl] += prediction\n",
    "                    n_predictions[sl[1:]] += gaussian\n",
    "\n",
    "                    # free up gpu memory\n",
    "                    del prediction, workon\n",
    "                    \n",
    "                    queue.task_done()\n",
    "                    pbar.set_postfix(\n",
    "                        cache=f\"{cache_hits}\",\n",
    "                        mem=f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\"\n",
    "                    )\n",
    "                    pbar.update(1)\n",
    "            queue.join()\n",
    "            if self.verbose and not self.allow_tqdm:\n",
    "                print(f\"Cache hits: {cache_hits}\\\\{len(slicers)}\")\n",
    "            \n",
    "\n",
    "            # predicted_logits /= n_predictions\n",
    "            torch.div(predicted_logits, n_predictions, out=predicted_logits)\n",
    "            # check for infs\n",
    "            if torch.any(torch.isinf(predicted_logits)):\n",
    "                raise RuntimeError('Encountered inf in predicted array. Aborting... If this problem persists, '\n",
    "                                   'reduce value_scaling_factor in compute_gaussian or increase the dtype of '\n",
    "                                   'predicted_logits to fp32')\n",
    "        except Exception as e:\n",
    "            del predicted_logits, n_predictions, prediction, gaussian, workon\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return predicted_logits\n",
    "  \n",
    "\n",
    "\n",
    "    def get_output_dictionary_sliding_window(self, data: torch.Tensor, slicers,\n",
    "                                            do_on_device: bool = True,\n",
    "                                            ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        # create a dictionary that associates the output of the inference, to each slicer of the sliding window module\n",
    "        # this way we can set ready for cache the output for the untouched patches.\n",
    "        \"\"\"\n",
    "        \n",
    "        dictionary = dict()\n",
    "        prediction = workon = None\n",
    "        results_device = self.device if do_on_device else torch.device('cpu')\n",
    "        if next(self.network.parameters()).device != results_device:\n",
    "            self.network = self.network.to(results_device)\n",
    "\n",
    "        def producer(d, slh, q):\n",
    "            for s in slh:\n",
    "                #tqdm.write(f\"put patch {s} on queue\")    # dentro producer\n",
    "                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(self.device), s))\n",
    "            q.put('end')\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data and network to device\n",
    "            if self.verbose:\n",
    "                print(f'move image and model to device {results_device}')\n",
    "\n",
    "            self.network = self.network.to(results_device)\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "\n",
    "                    pred_cpu = pred_gpu.cpu()\n",
    "                    # save prediction in the dictionary\n",
    "                    dictionary[self._slice_key(sl)] = pred_cpu\n",
    "                    # immediately free gpu memory\n",
    "                    del pred_gpu\n",
    "                    \n",
    "                    queue.task_done()\n",
    "                    pbar.update()\n",
    "            queue.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            del workon#, prediction\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return dictionary\n",
    "\n",
    "        try:\n",
    "            empty_cache(self.device)\n",
    "\n",
    "            # move data and network to device\n",
    "            if self.verbose:\n",
    "                print(f'move image and model to device {results_device}')\n",
    "\n",
    "            self.network = self.network.to(results_device)\n",
    "            data = data.to(results_device)\n",
    "            queue = Queue(maxsize=2)\n",
    "            t = Thread(target=producer, args=(data, slicers, queue))\n",
    "            t.start()\n",
    "\n",
    "            if not self.allow_tqdm and self.verbose:\n",
    "                print(f'running prediction: {len(slicers)} steps')\n",
    "\n",
    "            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n",
    "                while True:\n",
    "                    item = queue.get()\n",
    "                    if item == 'end':\n",
    "                        queue.task_done()\n",
    "                        break\n",
    "                    workon, sl = item\n",
    "                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n",
    "\n",
    "                    pred_cpu = pred_gpu.cpu()\n",
    "                    # save prediction in the dictionary\n",
    "                    dictionary[self._slice_key(sl)] = pred_cpu\n",
    "                    # immediately free gpu memory\n",
    "                    del pred_gpu\n",
    "                    \n",
    "                    queue.task_done()\n",
    "                    pbar.update()\n",
    "            queue.join()\n",
    "\n",
    "        except Exception as e:\n",
    "            del workon#, prediction\n",
    "            empty_cache(self.device)\n",
    "            empty_cache(results_device)\n",
    "            raise e\n",
    "        return dictionary\n",
    "\n",
    "    def check_overlapping(self, slicer, perturbation_mask: torch.BoolTensor) -> bool:\n",
    "        \"\"\"\n",
    "        Restituisce True se la patch definita da `slicer`\n",
    "        contiene almeno un voxel perturbato.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        slicer : tuple\n",
    "            Quello prodotto da `_internal_get_sliding_window_slicers`,\n",
    "            cioè (slice(None), slice(x0,x1), slice(y0,y1), slice(z0,z1)).\n",
    "        perturbation_mask : torch.BoolTensor\n",
    "            Maschera (C, X, Y, Z) con True nei voxel da perturbare\n",
    "            (di solito C==1 o replicata sui canali).\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True ↔ almeno un voxel True nella patch.\n",
    "        \"\"\"\n",
    "        # NB: il primo elemento del tuple è sempre slice(None) (canali).\n",
    "        #     Lo manteniamo: non ha overhead e semplifica.\n",
    "        return perturbation_mask[slicer].any().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89877caf",
   "metadata": {
    "papermill": {
     "duration": 0.033884,
     "end_time": "2025-07-01T13:44:09.967248",
     "exception": false,
     "start_time": "2025-07-01T13:44:09.933364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Try Captum's kernel SHAP on the organ mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3034e8d",
   "metadata": {
    "papermill": {
     "duration": 0.033178,
     "end_time": "2025-07-01T13:44:10.035011",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.001833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### first derive a customized class from Captum library, to use sliding window caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7746723",
   "metadata": {
    "papermill": {
     "duration": 0.032876,
     "end_time": "2025-07-01T13:44:10.100702",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.067826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Try to customize KernelShap as a \"sibling\", so let's inherit the parent, LimeBase\n",
    "that's because we need to override (to-and-from)/interpret_rep_transform methods used to map the (1,M) binary mask vector with the perturbed volume AND the perturbation mask we need for caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b11320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:10.169177Z",
     "iopub.status.busy": "2025-07-01T13:44:10.168719Z",
     "iopub.status.idle": "2025-07-01T13:44:10.393444Z",
     "shell.execute_reply": "2025-07-01T13:44:10.392692Z"
    },
    "papermill": {
     "duration": 0.261096,
     "end_time": "2025-07-01T13:44:10.394905",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.133809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# pyre-strict\n",
    "import inspect\n",
    "import math\n",
    "import typing\n",
    "import warnings\n",
    "from collections.abc import Iterator\n",
    "from typing import Any, Callable, cast, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from captum._utils.common import (\n",
    "    _expand_additional_forward_args,\n",
    "    _expand_target,\n",
    "    _flatten_tensor_or_tuple,\n",
    "    _format_output,\n",
    "    _format_tensor_into_tuples,\n",
    "    _get_max_feature_index,\n",
    "    _is_tuple,\n",
    "    _reduce_list,\n",
    "    _run_forward,\n",
    ")\n",
    "from captum._utils.models.linear_model import SkLearnLasso\n",
    "from captum._utils.models.model import Model\n",
    "from captum._utils.progress import progress\n",
    "from captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\n",
    "from captum.attr._utils.attribution import PerturbationAttribution\n",
    "from captum.attr._utils.batching import _batch_example_iterator\n",
    "from captum.attr._utils.common import (\n",
    "    _construct_default_feature_mask,\n",
    "    _format_input_baseline,\n",
    ")\n",
    "from captum.log import log_usage\n",
    "from torch import Tensor, BoolTensor\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "class LimeBaseWithCustomArgumentToForwardFunc(PerturbationAttribution):\n",
    "    r\"\"\"\n",
    "    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        forward_func: Callable[..., Tensor],\n",
    "        interpretable_model: Model,\n",
    "        similarity_func: Callable[\n",
    "            ...,\n",
    "            Union[float, Tensor],\n",
    "        ],\n",
    "        perturb_func: Callable[..., object],\n",
    "        perturb_interpretable_space: bool,\n",
    "        from_interp_rep_transform: Optional[\n",
    "            Callable[..., Union[Tensor, Tuple[Tensor, ...]]]\n",
    "        ],\n",
    "        to_interp_rep_transform: Optional[Callable[..., Tensor]],\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "\n",
    "            forward_func (Callable): The forward function of the model or any\n",
    "                    modification of it. If a batch is provided as input for\n",
    "                    attribution, it is expected that forward_func returns a scalar\n",
    "                    representing the entire batch.\n",
    "            interpretable_model (Model): Model object to train interpretable model.\n",
    "                    A Model object provides a `fit` method to train the model,\n",
    "                    given a dataloader, with batches containing three tensors:\n",
    "\n",
    "                    - interpretable_inputs: Tensor\n",
    "                      [2D num_samples x num_interp_features],\n",
    "                    - expected_outputs: Tensor [1D num_samples],\n",
    "                    - weights: Tensor [1D num_samples]\n",
    "\n",
    "                    The model object must also provide a `representation` method to\n",
    "                    access the appropriate coefficients or representation of the\n",
    "                    interpretable model after fitting.\n",
    "                    Some predefined interpretable linear models are provided in\n",
    "                    captum._utils.models.linear_model including wrappers around\n",
    "                    SkLearn linear models as well as SGD-based PyTorch linear\n",
    "                    models.\n",
    "\n",
    "                    Note that calling fit multiple times should retrain the\n",
    "                    interpretable model, each attribution call reuses\n",
    "                    the same given interpretable model object.\n",
    "            similarity_func (Callable): Function which takes a single sample\n",
    "                    along with its corresponding interpretable representation\n",
    "                    and returns the weight of the interpretable sample for\n",
    "                    training interpretable model. Weight is generally\n",
    "                    determined based on similarity to the original input.\n",
    "                    The original paper refers to this as a similarity kernel.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> similarity_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_interpretable_input:\n",
    "                    >>>        Tensor [2D 1 x num_interp_features],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> float or Tensor containing float scalar\n",
    "\n",
    "                    perturbed_input and original_input will be the same type and\n",
    "                    contain tensors of the same shape (regardless of whether or not\n",
    "                    the sampling function returns inputs in the interpretable\n",
    "                    space). original_input is the same as the input provided\n",
    "                    when calling attribute.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "            perturb_func (Callable): Function which returns a single\n",
    "                    sampled input, generally a perturbation of the original\n",
    "                    input, which is used to train the interpretable surrogate\n",
    "                    model. Function can return samples in either\n",
    "                    the original input space (matching type and tensor shapes\n",
    "                    of original input) or in the interpretable input space,\n",
    "                    which is a vector containing the intepretable features.\n",
    "                    Alternatively, this function can return a generator\n",
    "                    yielding samples to train the interpretable surrogate\n",
    "                    model, and n_samples perturbations will be sampled\n",
    "                    from this generator.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> perturb_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor, tuple[Tensor, ...], or\n",
    "                    >>>    generator yielding tensor or tuple[Tensor, ...]\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "\n",
    "                    Returned sampled input should match the input type (Tensor\n",
    "                    or Tuple of Tensor and corresponding shapes) if\n",
    "                    perturb_interpretable_space = False. If\n",
    "                    perturb_interpretable_space = True, the return type should\n",
    "                    be a single tensor of shape 1 x num_interp_features,\n",
    "                    corresponding to the representation of the\n",
    "                    sample to train the interpretable model.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "            perturb_interpretable_space (bool): Indicates whether\n",
    "                    perturb_func returns a sample in the interpretable space\n",
    "                    (tensor of shape 1 x num_interp_features) or a sample\n",
    "                    in the original space, matching the format of the original\n",
    "                    input. Once sampled, inputs can be converted to / from\n",
    "                    the interpretable representation with either\n",
    "                    to_interp_rep_transform or from_interp_rep_transform.\n",
    "            from_interp_rep_transform (Callable): Function which takes a\n",
    "                    single sampled interpretable representation (tensor\n",
    "                    of shape 1 x num_interp_features) and returns\n",
    "                    the corresponding representation in the input space\n",
    "                    (matching shapes of original input to attribute).\n",
    "\n",
    "                    This argument is necessary if perturb_interpretable_space\n",
    "                    is True, otherwise None can be provided for this argument.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> from_interp_rep_transform(\n",
    "                    >>>    curr_sample: Tensor [2D 1 x num_interp_features]\n",
    "                    >>>    original_input: Tensor or Tuple of Tensors,\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor or tuple[Tensor, ...]\n",
    "\n",
    "                    Returned sampled input should match the type of original_input\n",
    "                    and corresponding tensor shapes.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "\n",
    "            to_interp_rep_transform (Callable): Function which takes a\n",
    "                    sample in the original input space and converts to\n",
    "                    its interpretable representation (tensor\n",
    "                    of shape 1 x num_interp_features).\n",
    "\n",
    "                    This argument is necessary if perturb_interpretable_space\n",
    "                    is False, otherwise None can be provided for this argument.\n",
    "\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> to_interp_rep_transform(\n",
    "                    >>>    curr_sample: Tensor or Tuple of Tensors,\n",
    "                    >>>    original_input: Tensor or Tuple of Tensors,\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor [2D 1 x num_interp_features]\n",
    "\n",
    "                    curr_sample will match the type of original_input\n",
    "                    and corresponding tensor shapes.\n",
    "\n",
    "                    All kwargs passed to the attribute method are\n",
    "                    provided as keyword arguments (kwargs) to this callable.\n",
    "        \"\"\"\n",
    "        PerturbationAttribution.__init__(self, forward_func)\n",
    "        self.interpretable_model = interpretable_model\n",
    "        self.similarity_func = similarity_func\n",
    "        self.perturb_func = perturb_func\n",
    "        self.perturb_interpretable_space = perturb_interpretable_space\n",
    "        self.from_interp_rep_transform = from_interp_rep_transform\n",
    "        self.to_interp_rep_transform = to_interp_rep_transform\n",
    "\n",
    "        if self.perturb_interpretable_space:\n",
    "            assert (\n",
    "                self.from_interp_rep_transform is not None\n",
    "            ), \"Must provide transform from interpretable space to original input space\"\n",
    "            \" when sampling from interpretable space.\"\n",
    "        else:\n",
    "            assert (\n",
    "                self.to_interp_rep_transform is not None\n",
    "            ), \"Must provide transform from original input space to interpretable space\"\n",
    "\n",
    "    @log_usage(part_of_slo=True)\n",
    "    @torch.no_grad()\n",
    "    def attribute(\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[Tuple[object, ...]] = None,\n",
    "        n_samples: int = 50,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: object,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to the inputs of the model using the approach described above.\n",
    "        It trains an interpretable model and returns a representation of the\n",
    "        interpretable model.\n",
    "\n",
    "        It is recommended to only provide a single example as input (tensors\n",
    "        with first dimension or batch size = 1). This is because LIME is generally\n",
    "        used for sample-based interpretability, training a separate interpretable\n",
    "        model to explain a model's prediction on each individual example.\n",
    "\n",
    "        A batch of inputs can be provided as inputs only if forward_func\n",
    "        returns a single value per batch (e.g. loss).\n",
    "        The interpretable feature representation should still have shape\n",
    "        1 x num_interp_features, corresponding to the interpretable\n",
    "        representation for the full batch, and perturbations_per_eval\n",
    "        must be set to 1.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n",
    "                        is computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            target (int, tuple, Tensor, or list, optional): Output indices for\n",
    "                        which surrogate model is trained\n",
    "                        (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs w            except --------ith > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (Any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. For all other types,\n",
    "                        the given argument is used for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            n_samples (int, optional): The number of samples of the original\n",
    "                        model used to train the surrogate interpretable model.\n",
    "                        Default: `50` if `n_samples` is not provided.\n",
    "            perturbations_per_eval (int, optional): Allows multiple samples\n",
    "                        to be processed simultaneously in one call to forward_fn.\n",
    "                        Each forward pass will contain a maximum of\n",
    "                        perturbations_per_eval * #examples samples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain at most\n",
    "                        (perturbations_per_eval * #examples) / num_devices\n",
    "                        samples.\n",
    "                        If the forward function returns a single scalar per batch,\n",
    "                        perturbations_per_eval must be set to 1.\n",
    "                        Default: 1\n",
    "            show_progress (bool, optional): Displays the progress of computation.\n",
    "                        It will try to use tqdm if available for advanced features\n",
    "                        (e.g. time estimation). Otherwise, it will fallback to\n",
    "                        a simple output of progress.\n",
    "                        Default: False\n",
    "            **kwargs (Any, optional): Any additional arguments necessary for\n",
    "                        sampling and transformation functions (provided to\n",
    "                        constructor).\n",
    "                        Default: None\n",
    "\n",
    "        Returns:\n",
    "            **interpretable model representation**:\n",
    "            - **interpretable model representation** (*Any*):\n",
    "                    A representation of the interpretable model trained. The return\n",
    "                    type matches the return type of train_interpretable_model_func.\n",
    "                    For example, this could contain coefficients of a\n",
    "                    linear surrogate model.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            >>> # SimpleClassifier takes a single input tensor of\n",
    "            >>> # float features with size N x 5,\n",
    "            >>> # and returns an Nx3 tensor of class probabilities.\n",
    "            >>> net = SimpleClassifier()\n",
    "            >>>\n",
    "            >>> # We will train an interpretable model with the same\n",
    "            >>> # features by simply sampling with added Gaussian noise\n",
    "            >>> # to the inputs and training a model to predict the\n",
    "            >>> # score of the target class.\n",
    "            >>>\n",
    "            >>> # For interpretable model training, we will use sklearn\n",
    "            >>> # linear model in this example. We have provided wrappers\n",
    "            >>> # around sklearn linear models to fit the Model interface.\n",
    "            >>> # Any arguments provided to the sklearn constructor can also\n",
    "            >>> # be provided to the wrapper, e.g.:\n",
    "            >>> # SkLearnLinearModel(\"linear_model.Ridge\", alpha=2.0)\n",
    "            >>> from captum._utils.models.linear_model import SkLearnLinearModel\n",
    "            >>>\n",
    "            >>>\n",
    "            >>> # Define similarity kernel (exponential kernel based on L2 norm)\n",
    "            >>> def similarity_kernel(\n",
    "            >>>     original_input: Tensor,\n",
    "            >>>     perturbed_input: Tensor,\n",
    "            >>>     perturbed_interpretable_input: Tensor,\n",
    "            >>>     **kwargs)->Tensor:\n",
    "            >>>         # kernel_width will be provided to attribute as a kwarg\n",
    "            >>>         kernel_width = kwargs[\"kernel_width\"]\n",
    "            >>>         l2_dist = torch.norm(original_input - perturbed_input)\n",
    "            >>>         return torch.exp(- (l2_dist**2) / (kernel_width**2))\n",
    "            >>>\n",
    "            >>>\n",
    "            >>> # Define sampling function\n",
    "            >>> # This function samples in original input space\n",
    "            >>> def perturb_func(\n",
    "            >>>     original_input: Tensor,\n",
    "            >>>     **kwargs)->Tensor:\n",
    "            >>>         return original_input + torch.randn_like(original_input)\n",
    "            >>>\n",
    "            >>> # For this example, we are setting the interpretable input to\n",
    "            >>> # match the model input, so the to_interp_rep_transform\n",
    "            >>> # function simply returns the input. In most cases, the interpretable\n",
    "            >>> # input will be different and may have a smaller feature set, so\n",
    "            >>> # an appropriate transformation function should be provided.\n",
    "            >>>\n",
    "            >>> def to_interp_transform(curr_sample, original_inp,\n",
    "            >>>                                      **kwargs):\n",
    "            >>>     return curr_sample\n",
    "            >>>\n",
    "            >>> # Generating random input with size 1 x 5\n",
    "            >>> input = torch.randn(1, 5)\n",
    "            >>> # Defining LimeBase interpreter\n",
    "            >>> lime_attr = LimeBase(net,\n",
    "                                     SkLearnLinearModel(\"linear_model.Ridge\"),\n",
    "                                     similarity_func=similarity_kernel,\n",
    "                                     perturb_func=perturb_func,\n",
    "                                     perturb_interpretable_space=False,\n",
    "                                     from_interp_rep_transform=None,\n",
    "                                     to_interp_rep_transform=to_interp_transform)\n",
    "            >>> # Computes interpretable model, returning coefficients of linear\n",
    "            >>> # model.\n",
    "            >>> attr_coefs = lime_attr.attribute(input, target=1, kernel_width=1.1)\n",
    "        \"\"\"\n",
    "        inp_tensor = cast(Tensor, inputs) if isinstance(inputs, Tensor) else inputs[0]\n",
    "        device = inp_tensor.device\n",
    "\n",
    "        interpretable_inps = []\n",
    "        similarities = []\n",
    "        outputs = []\n",
    "\n",
    "        curr_model_inputs = []\n",
    "        expanded_additional_args = None\n",
    "        expanded_target = None\n",
    "        gen_perturb_func = self._get_perturb_generator_func(inputs, **kwargs)\n",
    "\n",
    "        if show_progress:\n",
    "            attr_progress = progress(\n",
    "                total=math.ceil(n_samples / perturbations_per_eval),\n",
    "                desc=f\"{self.get_name()} attribution\",\n",
    "            )\n",
    "            attr_progress.update(0)\n",
    "\n",
    "        # LOOP FUNCTION -> HERE WE NEED TO GET THE PERTURBED INPUT, BUILD OUR PERTURBATION_MASK AND PASS IT TO THE FORWARD FUNCTION\n",
    "        # one convoluted thing is feature mask scope, it is passed to this method from Lime via super().attribute(), but it's not in the \n",
    "        # method declaration\n",
    "        feature_mask = kwargs[\"feature_mask\"]\n",
    "        batch_count = 0\n",
    "        for _ in range(n_samples):\n",
    "            try:\n",
    "                interpretable_inp, curr_model_input = gen_perturb_func()\n",
    "                perturbation_mask = self._get_perturbation_mask(interpretable_inp, curr_model_input, feature_mask)\n",
    "            except StopIteration:\n",
    "                warnings.warn(\n",
    "                    \"Generator completed prior to given n_samples iterations!\",\n",
    "                    stacklevel=1,\n",
    "                )\n",
    "                break\n",
    "            except:\n",
    "                print(\"error in the perturbation mask generation\")\n",
    "                raise\n",
    "               \n",
    "            # add the perturbation mask as an additional parameter for the forward function\n",
    "            if additional_forward_args is None:\n",
    "                additional_forward_args_with_mask = (perturbation_mask,)\n",
    "            elif isinstance(additional_forward_args, tuple):\n",
    "                additional_forward_args_with_mask = additional_forward_args + (perturbation_mask,)\n",
    "            else:\n",
    "                additional_forward_args_with_mask = (additional_forward_args, perturbation_mask)\n",
    "             #---------------------------------------------\n",
    "            batch_count += 1\n",
    "            interpretable_inps.append(interpretable_inp)\n",
    "            curr_model_inputs.append(curr_model_input)\n",
    "\n",
    "            curr_sim = self.similarity_func(\n",
    "                inputs, curr_model_input, interpretable_inp, **kwargs\n",
    "            )\n",
    "            similarities.append(\n",
    "                curr_sim.flatten()\n",
    "                if isinstance(curr_sim, Tensor)\n",
    "                else torch.tensor([curr_sim], device=device)\n",
    "            )\n",
    "\n",
    "            if len(curr_model_inputs) == perturbations_per_eval:\n",
    "                # change: removed if condition, to rebuild final expanded_additional_forward_args at each iteration\n",
    "                #if expanded_additional_args is None:\n",
    "                expanded_additional_args = _expand_additional_forward_args(\n",
    "                    additional_forward_args_with_mask, len(curr_model_inputs)\n",
    "                )\n",
    "                if expanded_target is None:\n",
    "                    expanded_target = _expand_target(target, len(curr_model_inputs))\n",
    "            \n",
    "                \n",
    "                model_out = self._evaluate_batch(\n",
    "                    curr_model_inputs,\n",
    "                    expanded_target,\n",
    "                    expanded_additional_args,\n",
    "                    device,\n",
    "                )\n",
    "\n",
    "                if show_progress:\n",
    "                    attr_progress.update()\n",
    "\n",
    "                outputs.append(model_out)\n",
    "\n",
    "                curr_model_inputs = []\n",
    "\n",
    "        if len(curr_model_inputs) > 0:\n",
    "            expanded_additional_args = _expand_additional_forward_args(\n",
    "                additional_forward_args_with_mask, len(curr_model_inputs)\n",
    "            )\n",
    "            expanded_target = _expand_target(target, len(curr_model_inputs))\n",
    "\n",
    "            model_out = self._evaluate_batch(\n",
    "                curr_model_inputs,\n",
    "                expanded_target,\n",
    "                expanded_additional_args,\n",
    "                device,\n",
    "            )\n",
    "            if show_progress:\n",
    "                attr_progress.update()\n",
    "            outputs.append(model_out)\n",
    "\n",
    "        if show_progress:\n",
    "            attr_progress.close()\n",
    "\n",
    "        # Argument 1 to \"cat\" has incompatible type\n",
    "        # \"list[Tensor | tuple[Tensor, ...]]\";\n",
    "        # expected \"tuple[Tensor, ...] | list[Tensor]\"  [arg-type]\n",
    "        combined_interp_inps = torch.cat(interpretable_inps).float()  # type: ignore\n",
    "        combined_outputs = (\n",
    "            torch.cat(outputs) if len(outputs[0].shape) > 0 else torch.stack(outputs)\n",
    "        ).float()\n",
    "        combined_sim = (\n",
    "            torch.cat(similarities)\n",
    "            if len(similarities[0].shape) > 0\n",
    "            else torch.stack(similarities)\n",
    "        ).float()\n",
    "        self.dataset = TensorDataset(combined_interp_inps, combined_outputs, combined_sim)\n",
    "        self.interpretable_model.fit(DataLoader(self.dataset, batch_size=batch_count))\n",
    "        return self.interpretable_model.representation()\n",
    "\n",
    "\n",
    "    def _get_perturbation_mask(\n",
    "        self,\n",
    "        interpretable_input: torch.Tensor,       # shape = (B, M)\n",
    "        original_inputs: TensorOrTupleOfTensorsGeneric, # shape = (B, C, D, W, H) or tuple thereof\n",
    "        feature_mask,\n",
    "    ) -> Union[torch.BoolTensor, Tuple[torch.BoolTensor, ...]]:\n",
    "        \"\"\"\n",
    "        Build a Boolean mask of shape (B, *input_dims) indicating which\n",
    "        elements should be perturbed (True) vs. left untouched (False).\n",
    "        \"\"\"\n",
    "    \n",
    "        # Case 1: single‐Tensor input\n",
    "        if isinstance(feature_mask, torch.Tensor):\n",
    "            # advanced indexing over the batch dimension\n",
    "            # result has shape (B, *feature_mask.shape)\n",
    "            mask = interpretable_input[:, feature_mask]\n",
    "            mask = ~mask.bool()\n",
    "\n",
    "            return mask\n",
    "    \n",
    "        # Case 2: multi‐input (tuple) model\n",
    "        else:\n",
    "            masks = []\n",
    "            for fm_i in feature_mask:\n",
    "                mask_i = interpretable_input[:, fm_i]  # → (B, *fm_i.shape)\n",
    "                masks.append(~mask_i.bool())\n",
    "            return tuple(masks)\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_perturb_generator_func(\n",
    "        self, inputs: TensorOrTupleOfTensorsGeneric, **kwargs: Any\n",
    "    ) -> Callable[\n",
    "        [], Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n",
    "    ]:\n",
    "        perturb_generator: Optional[Iterator[TensorOrTupleOfTensorsGeneric]]\n",
    "        perturb_generator = None\n",
    "        if inspect.isgeneratorfunction(self.perturb_func):\n",
    "            perturb_generator = self.perturb_func(inputs, **kwargs)\n",
    "\n",
    "        def generate_perturbation() -> (\n",
    "            Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n",
    "        ):\n",
    "            if perturb_generator:\n",
    "                curr_sample = next(perturb_generator)\n",
    "            else:\n",
    "                curr_sample = self.perturb_func(inputs, **kwargs)\n",
    "\n",
    "            if self.perturb_interpretable_space:\n",
    "                interpretable_inp = curr_sample\n",
    "                curr_model_input = self.from_interp_rep_transform(  # type: ignore\n",
    "                    curr_sample, inputs, **kwargs\n",
    "                )\n",
    "            else:\n",
    "                curr_model_input = curr_sample\n",
    "                interpretable_inp = self.to_interp_rep_transform(  # type: ignore\n",
    "                    curr_sample, inputs, **kwargs\n",
    "                )\n",
    "\n",
    "            return interpretable_inp, curr_model_input  # type: ignore\n",
    "\n",
    "        return generate_perturbation\n",
    "\n",
    "    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.)\n",
    "    def attribute_future(self) -> Callable:\n",
    "        r\"\"\"\n",
    "        This method is not implemented for LimeBase.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"LimeBase does not support attribution of future samples.\"\n",
    "        )\n",
    "\n",
    "    def _evaluate_batch(\n",
    "        self,\n",
    "        curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n",
    "        expanded_target: TargetType,\n",
    "        expanded_additional_args: object,\n",
    "        device: torch.device,\n",
    "    ) -> Tensor:\n",
    "        model_out = _run_forward(\n",
    "            self.forward_func,\n",
    "            #MOMENTANEAOUS---> sliding_window forward function only works with single items (no batch) --> take first\n",
    "            #_reduce_list(curr_model_inputs),\n",
    "            _reduce_list(curr_model_inputs)[0],\n",
    "            expanded_target,\n",
    "            expanded_additional_args,\n",
    "        )\n",
    "        if isinstance(model_out, Tensor):\n",
    "            assert model_out.numel() == len(curr_model_inputs), (\n",
    "                \"Number of outputs is not appropriate, must return \"\n",
    "                \"one output per perturbed input\"\n",
    "            )\n",
    "        if isinstance(model_out, Tensor):\n",
    "            return model_out.flatten()\n",
    "        return torch.tensor([model_out], device=device)\n",
    "\n",
    "    def has_convergence_delta(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def multiplies_by_inputs(self) -> bool:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Default transformations and methods\n",
    "# for Lime child implementation.\n",
    "\n",
    "\n",
    "# pyre-fixme[3]: Return type must be annotated.\n",
    "# pyre-fixme[2]: Parameter must be annotated.\n",
    "def default_from_interp_rep_transform(curr_sample, original_inputs, **kwargs):\n",
    "    assert (\n",
    "        \"feature_mask\" in kwargs\n",
    "    ), \"Must provide feature_mask to use default interpretable representation transform\"\n",
    "    assert (\n",
    "        \"baselines\" in kwargs\n",
    "    ), \"Must provide baselines to use default interpretable representation transform\"\n",
    "    feature_mask = kwargs[\"feature_mask\"]\n",
    "    if isinstance(feature_mask, Tensor):\n",
    "        binary_mask = curr_sample[0][feature_mask].bool()\n",
    "        input_space_transformed = (\n",
    "            binary_mask.to(original_inputs.dtype) * original_inputs\n",
    "            + (~binary_mask).to(original_inputs.dtype) * kwargs[\"baselines\"]\n",
    "        )\n",
    "        \n",
    "        return input_space_transformed\n",
    "    else:\n",
    "        binary_mask = tuple(\n",
    "            curr_sample[0][feature_mask[j]].bool() for j in range(len(feature_mask))\n",
    "        )\n",
    "        input_space_transformed = tuple(\n",
    "            binary_mask[j].to(original_inputs[j].dtype) * original_inputs[j]\n",
    "            + (~binary_mask[j]).to(original_inputs[j].dtype) * kwargs[\"baselines\"][j]\n",
    "            for j in range(len(feature_mask))\n",
    "        )\n",
    "        return input_space_transformed\n",
    "\n",
    "\n",
    "def get_exp_kernel_similarity_function(\n",
    "    distance_mode: str = \"cosine\",\n",
    "    kernel_width: float = 1.0,\n",
    ") -> Callable[..., float]:\n",
    "    r\"\"\"\n",
    "    This method constructs an appropriate similarity function to compute\n",
    "    weights for perturbed sample in LIME. Distance between the original\n",
    "    and perturbed inputs is computed based on the provided distance mode,\n",
    "    and the distance is passed through an exponential kernel with given\n",
    "    kernel width to convert to a range between 0 and 1.\n",
    "\n",
    "    The callable returned can be provided as the similarity_fn for\n",
    "    Lime or LimeBase.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        distance_mode (str, optional): Distance mode can be either \"cosine\" or\n",
    "                    \"euclidean\" corresponding to either cosine distance\n",
    "                    or Euclidean distance respectively. Distance is computed\n",
    "                    by flattening the original inputs and perturbed inputs\n",
    "                    (concatenating tuples of inputs if necessary) and computing\n",
    "                    distances between the resulting vectors.\n",
    "                    Default: \"cosine\"\n",
    "        kernel_width (float, optional):\n",
    "                    Kernel width for exponential kernel applied to distance.\n",
    "                    Default: 1.0\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        *Callable*:\n",
    "        - **similarity_fn** (*Callable*):\n",
    "            Similarity function. This callable can be provided as the\n",
    "            similarity_fn for Lime or LimeBase.\n",
    "    \"\"\"\n",
    "\n",
    "    # pyre-fixme[3]: Return type must be annotated.\n",
    "    # pyre-fixme[2]: Parameter must be annotated.\n",
    "    def default_exp_kernel(original_inp, perturbed_inp, __, **kwargs):\n",
    "        flattened_original_inp = _flatten_tensor_or_tuple(original_inp).float()\n",
    "        flattened_perturbed_inp = _flatten_tensor_or_tuple(perturbed_inp).float()\n",
    "        if distance_mode == \"cosine\":\n",
    "            cos_sim = CosineSimilarity(dim=0)\n",
    "            distance = 1 - cos_sim(flattened_original_inp, flattened_perturbed_inp)\n",
    "        elif distance_mode == \"euclidean\":\n",
    "            distance = torch.norm(flattened_original_inp - flattened_perturbed_inp)\n",
    "        else:\n",
    "            raise ValueError(\"distance_mode must be either cosine or euclidean.\")\n",
    "        return math.exp(-1 * (distance**2) / (2 * (kernel_width**2)))\n",
    "\n",
    "    return default_exp_kernel\n",
    "\n",
    "\n",
    "def default_perturb_func(\n",
    "    original_inp: TensorOrTupleOfTensorsGeneric, **kwargs: object\n",
    ") -> Tensor:\n",
    "    assert (\n",
    "        \"num_interp_features\" in kwargs\n",
    "    ), \"Must provide num_interp_features to use default interpretable sampling function\"\n",
    "    if isinstance(original_inp, Tensor):\n",
    "        device = original_inp.device\n",
    "    else:\n",
    "        device = original_inp[0].device\n",
    "\n",
    "    probs = torch.ones(1, cast(int, kwargs[\"num_interp_features\"])) * 0.5\n",
    "    return torch.bernoulli(probs).to(device=device).long()\n",
    "\n",
    "\n",
    "def construct_feature_mask(\n",
    "    feature_mask: Union[None, Tensor, Tuple[Tensor, ...]],\n",
    "    formatted_inputs: Tuple[Tensor, ...],\n",
    ") -> Tuple[Tuple[Tensor, ...], int]:\n",
    "    feature_mask_tuple: Tuple[Tensor, ...]\n",
    "    if feature_mask is None:\n",
    "        feature_mask_tuple, num_interp_features = _construct_default_feature_mask(\n",
    "            formatted_inputs\n",
    "        )\n",
    "    else:\n",
    "        feature_mask_tuple = _format_tensor_into_tuples(feature_mask)\n",
    "        min_interp_features = int(\n",
    "            min(\n",
    "                torch.min(single_mask).item()\n",
    "                for single_mask in feature_mask_tuple\n",
    "                if single_mask.numel()\n",
    "            )\n",
    "        )\n",
    "        if min_interp_features != 0:\n",
    "            warnings.warn(\n",
    "                \"Minimum element in feature mask is not 0, shifting indices to\"\n",
    "                \" start at 0.\",\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            feature_mask_tuple = tuple(\n",
    "                single_mask - min_interp_features for single_mask in feature_mask_tuple\n",
    "            )\n",
    "\n",
    "        num_interp_features = _get_max_feature_index(feature_mask_tuple) + 1\n",
    "    return feature_mask_tuple, num_interp_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e501f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:10.465796Z",
     "iopub.status.busy": "2025-07-01T13:44:10.465559Z",
     "iopub.status.idle": "2025-07-01T13:44:10.490658Z",
     "shell.execute_reply": "2025-07-01T13:44:10.490068Z"
    },
    "papermill": {
     "duration": 0.061583,
     "end_time": "2025-07-01T13:44:10.491806",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.430223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LimeWithCustomArgumentToForwardFunc(LimeBaseWithCustomArgumentToForwardFunc):\n",
    "    r\"\"\"\n",
    "    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n",
    "    This will just inherit our modified LimeBase class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        forward_func: Callable[..., Tensor],\n",
    "        interpretable_model: Optional[Model] = None,\n",
    "        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n",
    "        similarity_func: Optional[Callable] = None,\n",
    "        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n",
    "        perturb_func: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "\n",
    "        Args:\n",
    "\n",
    "\n",
    "            forward_func (Callable): The forward function of the model or any\n",
    "                    modification of it\n",
    "            interpretable_model (Model, optional): Model object to train\n",
    "                    interpretable model.\n",
    "\n",
    "                    This argument is optional and defaults to SkLearnLasso(alpha=0.01),\n",
    "                    which is a wrapper around the Lasso linear model in SkLearn.\n",
    "                    This requires having sklearn version >= 0.23 available.\n",
    "\n",
    "                    Other predefined interpretable linear models are provided in\n",
    "                    captum._utils.models.linear_model.\n",
    "\n",
    "                    Alternatively, a custom model object must provide a `fit` method to\n",
    "                    train the model, given a dataloader, with batches containing\n",
    "                    three tensors:\n",
    "\n",
    "                    - interpretable_inputs: Tensor\n",
    "                      [2D num_samples x num_interp_features],\n",
    "                    - expected_outputs: Tensor [1D num_samples],\n",
    "                    - weights: Tensor [1D num_samples]\n",
    "\n",
    "                    The model object must also provide a `representation` method to\n",
    "                    access the appropriate coefficients or representation of the\n",
    "                    interpretable model after fitting.\n",
    "\n",
    "                    Note that calling fit multiple times should retrain the\n",
    "                    interpretable model, each attribution call reuses\n",
    "                    the same given interpretable model object.\n",
    "            similarity_func (Callable, optional): Function which takes a single sample\n",
    "                    along with its corresponding interpretable representation\n",
    "                    and returns the weight of the interpretable sample for\n",
    "                    training the interpretable model.\n",
    "                    This is often referred to as a similarity kernel.\n",
    "\n",
    "                    This argument is optional and defaults to a function which\n",
    "                    applies an exponential kernel to the cosine distance between\n",
    "                    the original input and perturbed input, with a kernel width\n",
    "                    of 1.0.\n",
    "\n",
    "                    A similarity function applying an exponential\n",
    "                    kernel to cosine / euclidean distances can be constructed\n",
    "                    using the provided get_exp_kernel_similarity_function in\n",
    "                    captum.attr._core.lime.\n",
    "\n",
    "                    Alternately, a custom callable can also be provided.\n",
    "                    The expected signature of this callable is:\n",
    "\n",
    "                    >>> def similarity_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    perturbed_interpretable_input:\n",
    "                    >>>        Tensor [2D 1 x num_interp_features],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> float or Tensor containing float scalar\n",
    "\n",
    "                    perturbed_input and original_input will be the same type and\n",
    "                    contain tensors of the same shape, with original_input\n",
    "                    being the same as the input provided when calling attribute.\n",
    "\n",
    "                    kwargs includes baselines, feature_mask, num_interp_features\n",
    "                    (integer, determined from feature mask).\n",
    "            perturb_func (Callable, optional): Function which returns a single\n",
    "                    sampled input, which is a binary vector of length\n",
    "                    num_interp_features, or a generator of such tensors.\n",
    "\n",
    "                    This function is optional, the default function returns\n",
    "                    a binary vector where each element is selected\n",
    "                    independently and uniformly at LimeWithCustomArgumentToForwardFuncrandom. Custom\n",
    "                    logic for selecting sampled binary vectors can\n",
    "                    be implemented by providing a function with the\n",
    "                    following expected signature:\n",
    "\n",
    "                    >>> perturb_func(\n",
    "                    >>>    original_input: Tensor or tuple[Tensor, ...],\n",
    "                    >>>    **kwargs: Any\n",
    "                    >>> ) -> Tensor [Binary 2D Tensor 1 x num_interp_features]\n",
    "                    >>>  or generator yielding such tensors\n",
    "\n",
    "                    kwargs includes baselines, feature_mask, num_interp_features\n",
    "                    (integer, determined from feature mask).\n",
    "\n",
    "        \"\"\"\n",
    "        if interpretable_model is None:\n",
    "            interpretable_model = SkLearnLasso(alpha=0.01)\n",
    "\n",
    "        if similarity_func is None:\n",
    "            similarity_func = get_exp_kernel_similarity_function()\n",
    "\n",
    "        if perturb_func is None:\n",
    "            perturb_func = default_perturb_func\n",
    "\n",
    "        LimeBase.__init__(\n",
    "            self,\n",
    "            forward_func,\n",
    "            interpretable_model,\n",
    "            similarity_func,\n",
    "            perturb_func,\n",
    "            True,\n",
    "            default_from_interp_rep_transform,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    @log_usage(part_of_slo=True)\n",
    "    def attribute(  # type: ignore\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[object] = None,\n",
    "        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n",
    "        n_samples: int = 25,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        return_input_shape: bool = True,\n",
    "        show_progress: bool = False,\n",
    "    ) -> TensorOrTupleOfTensorsGeneric:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to the inputs of the model using the approach described above,\n",
    "        training an interpretable model and returning a representation of the\n",
    "        interpretable model.\n",
    "\n",
    "        It is recommended to only provide a single example as input (tensors\n",
    "        with first dimension or batch size = 1). This is because LIME is generally\n",
    "        used for sample-based interpretability, training a separate interpretable\n",
    "        model to explain a model's prediction on each individual example.\n",
    "\n",
    "        A batch of inputs can also be provided as inputs, similar to\n",
    "        other perturbation-based attribution methods. In this case, if forward_fn\n",
    "        returns a scalar per example, attributions will be computed for each\n",
    "        example independently, with a separate interpretable model trained for each\n",
    "        example. Note that provided similarity and pertforward_funcurbation functions will be\n",
    "        provided each example separately (first dimension = 1) in this case.\n",
    "        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n",
    "        still be computed using a single interpretable model for the full batch.\n",
    "        In this case, similarity and perturbation functions will be provided the\n",
    "        same original input containing the full batch.\n",
    "\n",
    "        The number of interpretable features is determined from the provided\n",
    "        feature mask, or if none is provided, from the default feature mask,\n",
    "        which considers each scalar input as a separate feature. It is\n",
    "        generally recommended to provide a feature mask which groups features\n",
    "        into a small number of interpretable features / components (e.g.\n",
    "        superpixels in images).\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n",
    "                        is computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n",
    "                        Baselines define reference value which replaces each\n",
    "                        feature when the corresponding interpretable feature\n",
    "                        is set to 0.\n",
    "                        Baselines can be provided as:\n",
    "\n",
    "                        - a single tensor, if inputs is a single tensor, with\n",
    "                          exactly the same dimensions as inputs or the first\n",
    "                          dimension is one and the remaining dimensions match\n",
    "                          with inputs.\n",
    "\n",
    "                        - a single scalar, if inputs is a single tensor, which will\n",
    "                          be broadcasted for each input value in input tensor.\n",
    "\n",
    "                        - a tuple of tensors or scalars, the baseline corresponding\n",
    "                          to each tensor in the inputs' tuple can be:\n",
    "\n",
    "                          - either a tensor with matching dimensions to\n",
    "                            corresponding tensor in the inputs' tuple\n",
    "                            or the first dimension is one and the remaining\n",
    "                            dimensions match with the corresponding\n",
    "                            input tensor.\n",
    "\n",
    "                          - or a scalar, corresponding to a tensor in the\n",
    "                            inputs' tuple. This scalar value is broadcasted\n",
    "                            for corresponding input tensor.\n",
    "\n",
    "                        In the cases when `baselines` iforward_funcs not provided, we internally\n",
    "                        use zero scalar corresponding to each input tensor.\n",
    "                        Default: None\n",
    "            target (int, tuple, Tensor, or list, optional): Output indices for\n",
    "                        which surrogate model is trained\n",
    "                        (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs with > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (Any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. It will be\n",
    "                        repeated for each of `n_steps` along the integrated\n",
    "                        path. For all other types, the given argument is used\n",
    "                        for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            feature_mask (Tensor or tuple[Tensor, ...], optional):\n",
    "                        feature_mask defines a mask for the input, grouping\n",
    "                        features which correspond to the same\n",
    "                        interpretable feature. feature_mask\n",
    "                        should contain the same number of tensors as inputs.\n",
    "                        Each tensor should\n",
    "                        be the same size as the corresponding input or\n",
    "                        broadcastable to match the inpuforward_funct tensor. Values across\n",
    "                        all tensors should be integers in the range 0 to\n",
    "                        num_interp_features - 1, and indices corresponding to the\n",
    "                        same feature should have the same value.\n",
    "                        Note that features are grouped across tensors\n",
    "                        (unlike feature ablation and occlusion), so\n",
    "                        if the same index is used in different tensors, those\n",
    "                        features are still grouped and added simultaneously.\n",
    "                        If None, then a feature mask is constructed which assigns\n",
    "                        each scalar within a tensor as a separate feature.\n",
    "                        Default: None\n",
    "            n_samples (int, optional): The number of samples of the original\n",
    "                        model used to train the surrogate interpretable model.\n",
    "                        Default: `50` if `n_samples` is not provided.\n",
    "            perturbations_per_eval (int, optional): Allows multiple samples\n",
    "                        to be processed simultaneously in one call to forward_fn.\n",
    "                        Each forward pass will contain a maximum of\n",
    "                        perturbations_per_eval * #examples samples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain at most\n",
    "                        (perturbations_per_eval * #examples) / num_devices\n",
    "                        samples.\n",
    "                        If the forward function returns a single scalar per batch,\n",
    "                        perturbations_per_eval must be set to 1.\n",
    "                        Default: 1\n",
    "            return_input_shape (bool, optional): Determines whether the returned\n",
    "                        tensor(s) only contain the coefficients for each interp-\n",
    "                        retable feature from the trained surrogate model, or\n",
    "                        whether the returned attributions match the input shape.\n",
    "                        When return_input_shape is True, the return type of attribute\n",
    "                        matches the input shape, with each element containing the\n",
    "                        coefficient of the corresponding interpretale feature.\n",
    "                        All elements with the same value in the feature mask\n",
    "                        will contain the same coefficient in the returned\n",
    "                        attributions.\n",
    "                        If forward_func returns a single element per batch, then the\n",
    "                        first dimension of each tensor will be 1, and the remaining\n",
    "                        dimensions will have the same shape as the original input\n",
    "                        tensor.\n",
    "                        If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpreatable models, with length\n",
    "                        num_interp_features.\n",
    "            show_progress (bool, optional): Displays the progress of computation.\n",
    "                        It will try to use tqdm if available for advanced features\n",
    "                        (e.g. time estimation). Otherwise, it will fallback to\n",
    "                        a simple output of progress.\n",
    "                        Default: False\n",
    "\n",
    "        Returns:\n",
    "            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n",
    "            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n",
    "                        The attributions with respect to each input feature.\n",
    "                        If return_input_shape = True, attributions will be\n",
    "                        the same size as the provided inputs, with each value\n",
    "                        providing the coefficient of the corresponding\n",
    "                        interpretale feature.\n",
    "                        If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpreatable models, with length\n",
    "                        num_interp_features.\n",
    "        Examples::\n",
    "\n",
    "            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n",
    "            >>> # and returns an Nx3 tensor of class probabilities.\n",
    "            >>> net = SimpleClassifier()\n",
    "\n",
    "            >>> # Generating random input with size 1 x 4 x 4\n",
    "            >>> input = torch.randn(1, 4, 4)\n",
    "\n",
    "            >>> # Defining Lime interpreter\n",
    "            >>> lime = Lime(net)\n",
    "            >>> # Computes attribution, with each of the 4 x 4 = 16\n",
    "            >>> # features as a separate interpretable feature\n",
    "            >>> attr = lime.attribute(input, target=1, n_samples=200)\n",
    "\n",
    "            >>> # Alternatively, we can group each 2x2 square of the inputs\n",
    "            >>> # as one 'interpretable' feature and perturb them together.\n",
    "            >>> # This can be done by creating a feature mask as follows, which\n",
    "            >>> # defines the feature groups, e.g.:\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # With this mask, all inputs with the same value are set to their\n",
    "            >>> # baseline value, when the corresponding binary interpretable\n",
    "            >>> # feature is set to 0.\n",
    "            >>> # The attributions can be calculated as follows:\n",
    "            >>> # feature mask has dimensions 1 x 4 x 4\n",
    "            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n",
    "            >>>                             [2,2,3,3],[2,2,3,3]]])\n",
    "\n",
    "            >>> # Computes interpretable model and returning attributions\n",
    "            >>> # matching input shape.\n",
    "            >>> attr = lime.attribute(input, target=1, feature_mask=feature_mask)\n",
    "        \"\"\"\n",
    "        return self._attribute_kwargs(\n",
    "            inputs=inputs,\n",
    "            baselines=baselines,\n",
    "            target=target,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            feature_mask=feature_mask,\n",
    "            n_samples=n_samples,\n",
    "            perturbations_per_eval=perturbations_per_eval,\n",
    "            return_input_shape=return_input_shape,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n",
    "    def attribute_future(self) -> Callable:\n",
    "        return super().attribute_future()\n",
    "\n",
    "    def _attribute_kwargs(  # type: ignore\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[object] = None,\n",
    "        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n",
    "        n_samples: int = 25,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        return_input_shape: bool = True,\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: object,\n",
    "    ) -> TensorOrTupleOfTensorsGeneric:\n",
    "        is_inputs_tuple = _is_tuple(inputs)\n",
    "        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n",
    "        bsz = formatted_inputs[0].shape[0]\n",
    "\n",
    "        feature_mask, num_interp_features = construct_feature_mask(\n",
    "            feature_mask, formatted_inputs\n",
    "        )\n",
    "\n",
    "        if num_interp_features > 10000:\n",
    "            warnings.warn(\n",
    "                \"Attempting to construct interpretable model with > 10000 features.\"\n",
    "                \"This can be very slow or lead to OOM issues. Please provide a feature\"\n",
    "                \"mask which groups input features to reduce the number of interpretable\"\n",
    "                \"features. \",\n",
    "                stacklevel=1,\n",
    "            )\n",
    "\n",
    "        coefs: Tensor\n",
    "        if bsz > 1:\n",
    "            test_output = _run_forward(\n",
    "                self.forward_func, inputs, target, additional_forward_args\n",
    "            )\n",
    "            if isinstance(test_output, Tensor) and torch.numel(test_output) > 1:\n",
    "                if torch.numel(test_output) == bsz:\n",
    "                    warnings.warn(\n",
    "                        \"You are providing multiple inputs for Lime / Kernel SHAP \"\n",
    "                        \"attributions. This trains a separate interpretable model \"\n",
    "                        \"for each example, which can be time consuming. It is \"\n",
    "                        \"recommended to compute attributions for one example at a \"\n",
    "                        \"time.\",\n",
    "                        stacklevel=1,\n",
    "                    )\n",
    "                    output_list = []\n",
    "                    for (\n",
    "                        curr_inps,\n",
    "                        curr_target,\n",
    "                        curr_additional_args,\n",
    "                        curr_baselines,\n",
    "                        curr_feature_mask,\n",
    "                    ) in _batch_example_iterator(\n",
    "                        bsz,\n",
    "                        formatted_inputs,\n",
    "                        target,\n",
    "                        additional_forward_args,# -----> CAN BE ALSO BATCHED AUTOMATICALLY BY THE LIBRARY ITERATOR\n",
    "                        baselines,\n",
    "                        feature_mask,\n",
    "                    ):\n",
    "                        coefs = super().attribute.__wrapped__(\n",
    "                            self,\n",
    "                            inputs=curr_inps if is_inputs_tuple else curr_inps[0],\n",
    "                            target=curr_target,\n",
    "                            additional_forward_args=curr_additional_args,\n",
    "                            n_samples=n_samples,\n",
    "                            perturbations_per_eval=perturbations_per_eval,\n",
    "                            baselines=(\n",
    "                                curr_baselines if is_inputs_tuple else curr_baselines[0]\n",
    "                            ),\n",
    "                            feature_mask=(\n",
    "                                curr_feature_mask\n",
    "                                if is_inputs_tuple\n",
    "                                else curr_feature_mask[0]\n",
    "                            ),\n",
    "                            num_interp_features=num_interp_features,\n",
    "                            show_progress=show_progress,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                        if return_input_shape:\n",
    "                            output_list.append(\n",
    "                                self._convert_output_shape(\n",
    "                                    curr_inps,\n",
    "                                    curr_feature_mask,\n",
    "                                    coefs,\n",
    "                                    num_interp_features,\n",
    "                                    is_inputs_tuple,\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            output_list.append(coefs.reshape(1, -1))  # type: ignore\n",
    "\n",
    "                    return _reduce_list(output_list)\n",
    "                else:\n",
    "                    raise AssertionError(\n",
    "                        \"Invalid number of outputs, forward function should return a\"\n",
    "                        \"scalar per example or a scalar per input batch.\"\n",
    "                    )\n",
    "            else:\n",
    "                assert perturbations_per_eval == 1, (\n",
    "                    \"Perturbations per eval must be 1 when forward function\"\n",
    "                    \"returns single value per batch!\"\n",
    "                )\n",
    "\n",
    "        coefs = super().attribute.__wrapped__(\n",
    "            self,\n",
    "            inputs=inputs,\n",
    "            target=target,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            n_samples=n_samples,\n",
    "            perturbations_per_eval=perturbations_per_eval,\n",
    "            baselines=baselines if is_inputs_tuple else baselines[0],\n",
    "            feature_mask=feature_mask if is_inputs_tuple else feature_mask[0],\n",
    "            num_interp_features=num_interp_features,\n",
    "            show_progress=show_progress,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if return_input_shape:\n",
    "            # pyre-fixme[7]: Expected `TensorOrTupleOfTensorsGeneric` but got\n",
    "            #  `Tuple[Tensor, ...]`.\n",
    "            return self._convert_output_shape(\n",
    "                formatted_inputs,\n",
    "                feature_mask,\n",
    "                coefs,\n",
    "                num_interp_features,\n",
    "                is_inputs_tuple,\n",
    "    \n",
    "            leading_dim_one=(bsz > 1),\n",
    "            )\n",
    "        else:\n",
    "            return coefs\n",
    "\n",
    "    @typing.overload\n",
    "    def _convert_output_shape(\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: Literal[True],\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Tuple[Tensor, ...]: ...\n",
    "\n",
    "    @typing.overload\n",
    "    def _convert_output_shape(  # type: ignore\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: Literal[False],\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Tensor: ...\n",
    "\n",
    "    @typing.overload\n",
    "    def _convert_output_shape(\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: bool,\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Union[Tensor, Tuple[Tensor, ...]]: ...\n",
    "\n",
    "    def _convert_output_shape(\n",
    "        self,\n",
    "        formatted_inp: Tuple[Tensor, ...],\n",
    "        feature_mask: Tuple[Tensor, ...],\n",
    "        coefs: Tensor,\n",
    "        num_interp_features: int,\n",
    "        is_inputs_tuple: bool,\n",
    "        leading_dim_one: bool = False,\n",
    "    ) -> Union[Tensor, Tuple[Tensor, ...]]:\n",
    "        coefs = coefs.flatten()\n",
    "        attr = [\n",
    "            torch.zeros_like(single_inp, dtype=torch.float)\n",
    "            for single_inp in formatted_inp\n",
    "        ]\n",
    "        for tensor_ind in range(len(formatted_inp)):\n",
    "            for single_feature in range(num_interp_features):\n",
    "                attr[tensor_ind] += (\n",
    "                    coefs[single_feature].item()\n",
    "                    * (feature_mask[tensor_ind] == single_feature).float()\n",
    "                )\n",
    "        if leading_dim_one:\n",
    "            for i in range(len(attr)):\n",
    "                attr[i] = attr[i][0:1]\n",
    "        return _format_output(is_inputs_tuple, tuple(attr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e39c00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:10.562497Z",
     "iopub.status.busy": "2025-07-01T13:44:10.562227Z",
     "iopub.status.idle": "2025-07-01T13:44:10.579579Z",
     "shell.execute_reply": "2025-07-01T13:44:10.579037Z"
    },
    "papermill": {
     "duration": 0.053845,
     "end_time": "2025-07-01T13:44:10.580648",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.526803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# pyre-strict\n",
    "\n",
    "from typing import Callable, cast, Generator, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from captum._utils.models.linear_model import SkLearnLinearRegression\n",
    "from captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\n",
    "from captum.attr._core.lime import construct_feature_mask, Lime\n",
    "from captum.attr._utils.common import _format_input_baseline\n",
    "from captum.log import log_usage\n",
    "from torch import Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "class KernelShapWithMask(LimeWithCustomArgumentToForwardFunc):\n",
    "    r\"\"\"\n",
    "    Kernel SHAP is a method that uses the LIME framework to compute\n",
    "    Shapley Values. Setting the loss function, weighting kernel and\n",
    "    regularization terms appropriately in the LIME framework allows\n",
    "    theoretically obtaining Shapley Values more efficiently than\n",
    "    directly computing Shapley Values.\n",
    "\n",
    "    More information regarding this method and proof of equivalence\n",
    "    can be found in the original paper here:\n",
    "    https://arxiv.org/abs/1705.07874\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, forward_func: Callable[..., Tensor]) -> None:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "\n",
    "            forward_func (Callable): The forward function of the model or\n",
    "                        any modification of it.\n",
    "        \"\"\"\n",
    "        Lime.__init__(\n",
    "            self,\n",
    "            forward_func,\n",
    "            interpretable_model=SkLearnLinearRegression(),\n",
    "            similarity_func=self.kernel_shap_similarity_kernel,\n",
    "            perturb_func=self.kernel_shap_perturb_generator,\n",
    "        )\n",
    "        self.inf_weight = 1000000.0\n",
    "\n",
    "    @log_usage(part_of_slo=True)\n",
    "    def attribute(  # type: ignore\n",
    "        self,\n",
    "        inputs: TensorOrTupleOfTensorsGeneric,\n",
    "        baselines: BaselineType = None,\n",
    "        target: TargetType = None,\n",
    "        additional_forward_args: Optional[object] = None,\n",
    "        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n",
    "        n_samples: int = 25,\n",
    "        perturbations_per_eval: int = 1,\n",
    "        return_input_shape: bool = True,\n",
    "        show_progress: bool = False,\n",
    "    ) -> TensorOrTupleOfTensorsGeneric:\n",
    "        r\"\"\"\n",
    "        This method attributes the output of the model with given target index\n",
    "        (in case it is provided, otherwise it assumes that output is a\n",
    "        scalar) to the inputs of the model using the approach described above,\n",
    "        training an interpretable model based on KernelSHAP and returning a\n",
    "        representation of the interpretable model.\n",
    "\n",
    "        It is recommended to only provide a single example as input (tensors\n",
    "        with first dimension or batch size = 1). This is because LIME / KernelShap\n",
    "        is generally used for sample-based interpretability, training a separate\n",
    "        interpretable model to explain a model's prediction on each individual example.\n",
    "\n",
    "        A batch of inputs can also be provided as inputs, similar to\n",
    "        other perturbation-based attribution methods. In this case, if forward_fn\n",
    "        returns a scalar per example, attributions will be computed for each\n",
    "        example independently, with a separate interpretable model trained for each\n",
    "        example. Note that provided similarity and perturbation functions will be\n",
    "        provided each example separately (first dimension = 1) in this case.\n",
    "        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n",
    "        still be computed using a single interpretable model for the full batch.\n",
    "        In this case, similarity and perturbation functions will be provided the\n",
    "        same original input containing the full batch.\n",
    "\n",
    "        The number of interpretable features is determined from the provided\n",
    "        feature mask, or if none is provided, from the default feature mask,\n",
    "        which considers each scalar input as a separate feature. It is\n",
    "        generally recommended to provide a feature mask which groups features\n",
    "        into a small number of interpretable features / components (e.g.\n",
    "        superpixels in images).\n",
    "\n",
    "        Args:\n",
    "\n",
    "            inputs (Tensor or tuple[Tensor, ...]): Input for which KernelShap\n",
    "                        is computed. If forward_func takes a single\n",
    "                        tensor as input, a single input tensor should be provided.\n",
    "                        If forward_func takes multiple tensors as input, a tuple\n",
    "                        of the input tensors should be provided. It is assumed\n",
    "                        that for all given input tensors, dimension 0 corresponds\n",
    "                        to the number of examples, and if multiple input tensors\n",
    "                        are provided, the examples must be aligned appropriately.\n",
    "            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n",
    "                        Baselines define the reference value which replaces each\n",
    "                        feature when the corresponding interpretable feature\n",
    "                        is set to 0.\n",
    "                        Baselines can be provided as:\n",
    "\n",
    "                        - a single tensor, if inputs is a single tensor, with\n",
    "                          exactly the same dimensions as inputs or the first\n",
    "                          dimension is one and the remaining dimensions match\n",
    "                          with inputs.\n",
    "\n",
    "                        - a single scalar, if inputs is a single tensor, which will\n",
    "                          be broadcasted for each input value in input tensor.\n",
    "\n",
    "                        - a tuple of tensors or scalars, the baseline corresponding\n",
    "                          to each tensor in the inputs' tuple can be:\n",
    "\n",
    "                          - either a tensor with matching dimensions to\n",
    "                            corresponding tensor in the inputs' tuple\n",
    "                            or the first dimension is one and the remaining\n",
    "                            dimensions match with the corresponding\n",
    "                            input tensor.\n",
    "\n",
    "                          - or a scalar, corresponding to a tensor in the\n",
    "                            inputs' tuple. This scalar value is broadcasted\n",
    "                            for corresponding input tensor.\n",
    "\n",
    "                        In the cases when `baselines` is not provided, we internally\n",
    "                        use zero scalar corresponding to each input tensor.\n",
    "                        Default: None\n",
    "            target (int, tuple, Tensor, or list, optional): Output indices for\n",
    "                        which surrogate model is trained\n",
    "                        (for classification cases,\n",
    "                        this is usually the target class).\n",
    "                        If the network returns a scalar value per example,\n",
    "                        no target index is necessary.\n",
    "                        For general 2D outputs, targets can be either:\n",
    "\n",
    "                        - a single integer or a tensor containing a single\n",
    "                          integer, which is applied to all input examples\n",
    "\n",
    "                        - a list of integers or a 1D tensor, with length matching\n",
    "                          the number of examples in inputs (dim 0). Each integer\n",
    "                          is applied as the target for the corresponding example.\n",
    "\n",
    "                        For outputs with > 2 dimensions, targets can be either:\n",
    "\n",
    "                        - A single tuple, which contains #output_dims - 1\n",
    "                          elements. This target index is applied to all examples.\n",
    "\n",
    "                        - A list of tuples with length equal to the number of\n",
    "                          examples in inputs (dim 0), and each tuple containing\n",
    "                          #output_dims - 1 elements. Each tuple is applied as the\n",
    "                          target for the corresponding example.\n",
    "\n",
    "                        Default: None\n",
    "            additional_forward_args (Any, optional): If the forward function\n",
    "                        requires additional arguments other than the inputs for\n",
    "                        which attributions should not be computed, this argument\n",
    "                        can be provided. It must be either a single additional\n",
    "                        argument of a Tensor or arbitrary (non-tuple) type or a\n",
    "                        tuple containing multiple additional arguments including\n",
    "                        tensors or any arbitrary python types. These arguments\n",
    "                        are provided to forward_func in order following the\n",
    "                        arguments in inputs.\n",
    "                        For a tensor, the first dimension of the tensor must\n",
    "                        correspond to the number of examples. It will be\n",
    "                        repeated for each of `n_steps` along the integrated\n",
    "                        path. For all other types, the given argument is used\n",
    "                        for all forward evaluations.\n",
    "                        Note that attributions are not computed with respect\n",
    "                        to these arguments.\n",
    "                        Default: None\n",
    "            feature_mask (Tensor or tuple[Tensor, ...], optional):\n",
    "                        feature_mask defines a mask for the input, grouping\n",
    "                        features which correspond to the same\n",
    "                        interpretable feature. feature_mask\n",
    "                        should contain the same number of tensors as inputs.\n",
    "                        Each tensor should\n",
    "                        be the same size as the corresponding input or\n",
    "                        broadcastable to match the input tensor. Values across\n",
    "                        all tensors should be integers in the range 0 to\n",
    "                        num_interp_features - 1, and indices corresponding to the\n",
    "                        same feature should have the same value.\n",
    "                        Note that features are grouped across tensors\n",
    "                        (unlike feature ablation and occlusion), so\n",
    "                        if the same index is used in different tensors, those\n",
    "                        features are still grouped and added simultaneously.\n",
    "                        If None, then a feature mask is constructed which assigns\n",
    "                        each scalar within a tensor as a separate feature.\n",
    "                        Default: None\n",
    "            n_samples (int, optional): The number of samples of the original\n",
    "                        model used to train the surrogate interpretable model.\n",
    "                        Default: `50` if `n_samples` is not provided.\n",
    "            perturbations_per_eval (int, optional): Allows multiple samples\n",
    "                        to be processed simultaneously in one call to forward_fn.\n",
    "                        Each forward pass will contain a maximum of\n",
    "                        perturbations_per_eval * #examples samples.\n",
    "                        For DataParallel models, each batch is split among the\n",
    "                        available devices, so evaluations on each available\n",
    "                        device contain at most\n",
    "                        (perturbations_per_eval * #examples) / num_devices\n",
    "                        samples.\n",
    "                        If the forward function returns a single scalar per batch,\n",
    "                        perturbations_per_eval must be set to 1.\n",
    "                        Default: 1\n",
    "            return_input_shape (bool, optional): Determines whether the returned\n",
    "                        tensor(s) only contain the coefficients for each interp-\n",
    "                        retable feature from the trained surrogate model, or\n",
    "                        whether the returned attributions match the input shape.\n",
    "                        When return_input_shape is True, the return type of attribute\n",
    "                        matches the input shape, with each element containing the\n",
    "                        coefficient of the corresponding interpretable feature.\n",
    "                        All elements with the same value in the feature mask\n",
    "                        will contain the same coefficient in the returned\n",
    "                        attributions. If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpretable model, with length\n",
    "                        num_interp_features.\n",
    "            show_progress (bool, optional): Displays the progress of computation.\n",
    "                        It will try to use tqdm if available for advanced features\n",
    "                        (e.g. time estimation). Otherwise, it will fallback to\n",
    "                        a simple output of progress.\n",
    "                        Default: False\n",
    "\n",
    "        Returns:\n",
    "            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n",
    "            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n",
    "                        The attributions with respect to each input feature.\n",
    "                        If return_input_shape = True, attributions will be\n",
    "                        the same size as the provided inputs, with each value\n",
    "                        providing the coefficient of the corresponding\n",
    "                        interpretale feature.\n",
    "                        If return_input_shape is False, a 1D\n",
    "                        tensor is returned, containing only the coefficients\n",
    "                        of the trained interpreatable models, with length\n",
    "                        num_interp_features.\n",
    "        Examples::\n",
    "            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n",
    "            >>> # and returns an Nx3 tensor of class probabilities.\n",
    "            >>> net = SimpleClassifier()\n",
    "\n",
    "            >>> # Generating random input with size 1 x 4 x 4\n",
    "            >>> input = torch.randn(1, 4, 4)\n",
    "\n",
    "            >>> # Defining KernelShap interpreter\n",
    "            >>> ks = KernelShap(net)\n",
    "            >>> # Computes attribution, with each of the 4 x 4 = 16\n",
    "            >>> # features as a separate interpretable feature\n",
    "            >>> attr = ks.attribute(input, target=1, n_samples=200)\n",
    "\n",
    "            >>> # Alternatively, we can group each 2x2 square of the inputs\n",
    "            >>> # as one 'interpretable' feature and perturb them together.\n",
    "            >>> # This can be done by creating a feature mask as follows, which\n",
    "            >>> # defines the feature groups, e.g.:\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 0 | 0 | 1 | 1 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # | 2 | 2 | 3 | 3 |\n",
    "            >>> # +---+---+---+---+\n",
    "            >>> # With this mask, all inputs with the same value are set to their\n",
    "            >>> # baseline value, when the corresponding binary interpretable\n",
    "            >>> # feature is set to 0.\n",
    "            >>> # The attributions can be calculated as follows:\n",
    "            >>> # feature mask has dimensions 1 x 4 x 4\n",
    "            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n",
    "            >>>                             [2,2,3,3],[2,2,3,3]]])\n",
    "\n",
    "            >>> # Computes KernelSHAP attributions with feature mask.\n",
    "            >>> attr = ks.attribute(input, target=1, feature_mask=feature_mask)\n",
    "        \"\"\"\n",
    "        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n",
    "        feature_mask, num_interp_features = construct_feature_mask(\n",
    "            feature_mask, formatted_inputs\n",
    "        )\n",
    "        num_features_list = torch.arange(num_interp_features, dtype=torch.float)\n",
    "        denom = num_features_list * (num_interp_features - num_features_list)\n",
    "        probs = torch.tensor((num_interp_features - 1)) / denom\n",
    "        probs[0] = 0.0\n",
    "        return self._attribute_kwargs(\n",
    "            inputs=inputs,\n",
    "            baselines=baselines,\n",
    "            target=target,\n",
    "            additional_forward_args=additional_forward_args,\n",
    "            feature_mask=feature_mask,\n",
    "            n_samples=n_samples,\n",
    "            perturbations_per_eval=perturbations_per_eval,\n",
    "            return_input_shape=return_input_shape,\n",
    "            num_select_distribution=Categorical(probs),\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n",
    "    def attribute_future(self) -> Callable:\n",
    "        r\"\"\"\n",
    "        This method is not implemented for KernelShap.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"attribute_future is not implemented for KernelShap\")\n",
    "\n",
    "    def kernel_shap_similarity_kernel(\n",
    "        self,\n",
    "        _,\n",
    "        __,\n",
    "        interpretable_sample: Tensor,\n",
    "        **kwargs: object,\n",
    "    ) -> Tensor:\n",
    "        assert (\n",
    "            \"num_interp_features\" in kwargs\n",
    "        ), \"Must provide num_interp_features to use default similarity kernel\"\n",
    "        num_selected_features = int(interpretable_sample.sum(dim=1).item())\n",
    "        num_features = kwargs[\"num_interp_features\"]\n",
    "        if num_selected_features == 0 or num_selected_features == num_features:\n",
    "            # weight should be theoretically infinite when\n",
    "            # num_selected_features = 0 or num_features\n",
    "            # enforcing that trained linear model must satisfy\n",
    "            # end-point criteria. In practice, it is sufficient to\n",
    "            # make this weight substantially larger so setting this\n",
    "            # weight to 1000000 (all other weights are 1).\n",
    "            similarities = self.inf_weight\n",
    "        else:\n",
    "            similarities = 1.0\n",
    "        return torch.tensor([similarities])\n",
    "\n",
    "    def kernel_shap_perturb_generator(\n",
    "        self,\n",
    "        original_inp: Union[Tensor, Tuple[Tensor, ...]],\n",
    "        **kwargs: object,\n",
    "    ) -> Generator[Tensor, None, None]:\n",
    "        r\"\"\"\n",
    "        Perturbations are sampled by the following process:\n",
    "         - Choose k (number of selected features), based on the distribution\n",
    "                p(k) = (M - 1) / (k * (M - k))\n",
    "\n",
    "            where M is the total number of features in the interpretable space\n",
    "\n",
    "         - Randomly select a binary vector with k ones, each sample is equally\n",
    "            likely. This is done by generating a random vector of normal\n",
    "            values and thresholding based on the top k elements.\n",
    "\n",
    "         Since there are M choose k vectors with k ones, this weighted sampling\n",
    "         is equivalent to applying the Shapley kernel for the sample weight,\n",
    "         defined as:\n",
    "         k(M, k) = (M - 1) / (k * (M - k) * (M choose k))\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            \"num_select_distribution\" in kwargs and \"num_interp_features\" in kwargs\n",
    "        ), (\n",
    "            \"num_select_distribution and num_interp_features are necessary\"\n",
    "            \" to use kernel_shap_perturb_func\"\n",
    "        )\n",
    "        if isinstance(original_inp, Tensor):\n",
    "            device = original_inp.device\n",
    "        else:\n",
    "            device = original_inp[0].device\n",
    "        num_features = cast(int, kwargs[\"num_interp_features\"])\n",
    "        yield torch.ones(1, num_features, device=device, dtype=torch.long)\n",
    "        yield torch.zeros(1, num_features, device=device, dtype=torch.long)\n",
    "        while True:\n",
    "            num_selected_features = cast(\n",
    "                Categorical, kwargs[\"num_select_distribution\"]\n",
    "            ).sample()\n",
    "            rand_vals = torch.randn(1, num_features)\n",
    "            threshold = torch.kthvalue(\n",
    "                rand_vals, num_features - num_selected_features\n",
    "            ).values.item()\n",
    "            yield (rand_vals > threshold).to(device=device).long()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfa6d53d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:10.649593Z",
     "iopub.status.busy": "2025-07-01T13:44:10.649329Z",
     "iopub.status.idle": "2025-07-01T13:44:10.654026Z",
     "shell.execute_reply": "2025-07-01T13:44:10.653351Z"
    },
    "papermill": {
     "duration": 0.040139,
     "end_time": "2025-07-01T13:44:10.655158",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.615019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an utility for annoying nnunetv2 preprocessing\n",
    "def nnunetv2_default_preprocessing(ct_img_path, predictor, dataset_json_path) -> np.ndarray:\n",
    "    plans_manager = predictor.plans_manager\n",
    "    configuration_manager = predictor.configuration_manager\n",
    "    \n",
    "    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n",
    "    rw = plans_manager.image_reader_writer_class()\n",
    "    if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "        rw = rw()\n",
    "    img_np, img_props = rw.read_images([str(ct_img_path)])\n",
    "    \n",
    "    preprocessed, _, _ = preprocessor.run_case_npy(\n",
    "        img_np, seg=None, properties=img_props,\n",
    "        plans_manager=plans_manager,\n",
    "        configuration_manager=configuration_manager,\n",
    "        dataset_json=dataset_json_path\n",
    "    )\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de394574",
   "metadata": {
    "papermill": {
     "duration": 0.033115,
     "end_time": "2025-07-01T13:44:10.721843",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.688728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Next step: define regular, fixed size superpixels and try to compute the attributions of each of them\n",
    "So we also need to define a metric to compare, since segmentation explanations, differently from classification, is intrinsically ambiguous. For example, let's select a priori a single region of the segmentation output, and use the average of these pixels to compute the impact of perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd3bd66",
   "metadata": {
    "papermill": {
     "duration": 0.032443,
     "end_time": "2025-07-01T13:44:10.787180",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.754737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Face-centered cubic (FCC) lattice induced supervoxel assignment\n",
    "-> more *isotropic* than simple cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9e18b",
   "metadata": {
    "papermill": {
     "duration": 0.033932,
     "end_time": "2025-07-01T13:44:10.855542",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.821610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1 affine transformation to translate isotropy from voxel space into the original geometrical space (measured in mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36270627",
   "metadata": {
    "papermill": {
     "duration": 0.034566,
     "end_time": "2025-07-01T13:44:10.924119",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.889553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2 Try to apply the original algorithm FCC it to an affine transformed volume that has the same proportion as the .nii in the physical space. Transform->apply the algorithm to derive the map-> back transform the map onto the voxel space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd9221bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:10.993004Z",
     "iopub.status.busy": "2025-07-01T13:44:10.992365Z",
     "iopub.status.idle": "2025-07-01T13:44:11.002644Z",
     "shell.execute_reply": "2025-07-01T13:44:11.002154Z"
    },
    "papermill": {
     "duration": 0.045403,
     "end_time": "2025-07-01T13:44:11.003691",
     "exception": false,
     "start_time": "2025-07-01T13:44:10.958288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def generate_supervoxel_map(img, S=200.0):\n",
    "    \"\"\"\n",
    "    Generate a supervoxel map using FCC tessellation in physical space (original version).\n",
    "    \n",
    "    Args:\n",
    "        img: Nibabel NIfTI image object\n",
    "        S (float): Desired supervoxel size in millimeters (default: 200.0)\n",
    "    \n",
    "    Returns:\n",
    "        supervoxel_map: 3D NumPy array with integer labels for supervoxels\n",
    "    \"\"\"\n",
    "    # Load volume and affine from image\n",
    "    volume = img.get_fdata()\n",
    "    affine = img.affine\n",
    "    W, H, D = volume.shape\n",
    "\n",
    "    # Compute the physical bounding box of the volume\n",
    "    corners_voxel = np.array([\n",
    "        [0, 0, 0],\n",
    "        [W-1, 0, 0],\n",
    "        [0, H-1, 0],\n",
    "        [0, 0, D-1],\n",
    "        [W-1, H-1, 0],\n",
    "        [W-1, 0, D-1],\n",
    "        [0, H-1, D-1],\n",
    "        [W-1, H-1, D-1]\n",
    "    ])\n",
    "    corners_hom = np.hstack((corners_voxel, np.ones((8, 1))))\n",
    "    corners_physical = (affine @ corners_hom.T).T[:, :3]\n",
    "    min_xyz = corners_physical.min(axis=0)\n",
    "    max_xyz = corners_physical.max(axis=0)\n",
    "\n",
    "    # Generate FCC lattice centers in physical space\n",
    "    a = S * np.sqrt(2)\n",
    "    factor = 2 / a\n",
    "    p_min = int(np.floor(factor * min_xyz[0])) - 1\n",
    "    p_max = int(np.ceil(factor * max_xyz[0])) + 1\n",
    "    q_min = int(np.floor(factor * min_xyz[1])) - 1\n",
    "    q_max = int(np.ceil(factor * max_xyz[1])) + 1\n",
    "    r_min = int(np.floor(factor * min_xyz[2])) - 1\n",
    "    r_max = int(np.ceil(factor * max_xyz[2])) + 1\n",
    "\n",
    "    # Create grid of possible indices\n",
    "    p_vals = np.arange(p_min, p_max + 1)\n",
    "    q_vals = np.arange(q_min, q_max + 1)\n",
    "    r_vals = np.arange(r_min, r_max + 1)\n",
    "    P, Q, R = np.meshgrid(p_vals, q_vals, r_vals, indexing='ij')\n",
    "    P = P.flatten()\n",
    "    Q = Q.flatten()\n",
    "    R = R.flatten()\n",
    "\n",
    "    # Filter for FCC lattice points (sum of indices is even)\n",
    "    mask = (P + Q + R) % 2 == 0\n",
    "    P = P[mask]\n",
    "    Q = Q[mask]\n",
    "    R = R[mask]\n",
    "\n",
    "    # Compute physical coordinates of centers\n",
    "    centers = np.column_stack((P * a / 2, Q * a / 2, R * a / 2))\n",
    "\n",
    "    # Keep only centers within the bounding box\n",
    "    inside = ((centers[:, 0] >= min_xyz[0]) & (centers[:, 0] <= max_xyz[0]) &\n",
    "              (centers[:, 1] >= min_xyz[1]) & (centers[:, 1] <= max_xyz[1]) &\n",
    "              (centers[:, 2] >= min_xyz[2]) & (centers[:, 2] <= max_xyz[2]))\n",
    "    centers = centers[inside]\n",
    "\n",
    "    # Check if any centers were generated\n",
    "    print(f\"Number of supervoxel centers: {len(centers)}\")\n",
    "    if len(centers) == 0:\n",
    "        raise ValueError(\"No supervoxel centers generated. Try reducing S.\")\n",
    "\n",
    "    # Generate voxel indices and transform to physical coordinates\n",
    "    voxel_indices = np.indices((W, H, D)).reshape(3, -1).T  # shape (W*H*D, 3)\n",
    "    voxel_indices_hom = np.hstack((voxel_indices, np.ones((voxel_indices.shape[0], 1))))  # shape (W*H*D, 4)\n",
    "    physical_coords = (affine @ voxel_indices_hom.T).T[:, :3]  # shape (W*H*D, 3)\n",
    "\n",
    "    # Assign each voxel to the nearest supervoxel center\n",
    "    tree = cKDTree(centers)\n",
    "    _, labels = tree.query(physical_coords)\n",
    "\n",
    "    # Create the supervoxel map\n",
    "    supervoxel_map = labels.reshape((W, H, D)).astype(np.int32)\n",
    "\n",
    "    return supervoxel_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3994a",
   "metadata": {
    "papermill": {
     "duration": 0.033927,
     "end_time": "2025-07-01T13:44:11.070624",
     "exception": false,
     "start_time": "2025-07-01T13:44:11.036697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cffc5e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:11.142494Z",
     "iopub.status.busy": "2025-07-01T13:44:11.141797Z",
     "iopub.status.idle": "2025-07-01T13:44:14.865495Z",
     "shell.execute_reply": "2025-07-01T13:44:14.864692Z"
    },
    "papermill": {
     "duration": 3.761188,
     "end_time": "2025-07-01T13:44:14.866878",
     "exception": false,
     "start_time": "2025-07-01T13:44:11.105690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n",
      "  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n"
     ]
    }
   ],
   "source": [
    "# 2) Initialise predictor ------------------------\n",
    "predictor = CustomNNUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=False, # == test time augmentation\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda', 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=False #it interfere with SHAP loading bar\n",
    ")\n",
    "# initializes the network architecture, loads the checkpoint\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    model_dir,\n",
    "    use_folds=(0,),\n",
    "    checkpoint_name='checkpoint_final.pth',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d574",
   "metadata": {
    "papermill": {
     "duration": 0.034949,
     "end_time": "2025-07-01T13:44:14.936621",
     "exception": false,
     "start_time": "2025-07-01T13:44:14.901672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define a ROI to explain segmentation in. \n",
    "Maybe this will provide a more useful attribution map, highlighting nearby organs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca968a68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:15.006392Z",
     "iopub.status.busy": "2025-07-01T13:44:15.005922Z",
     "iopub.status.idle": "2025-07-01T13:44:15.464003Z",
     "shell.execute_reply": "2025-07-01T13:44:15.463265Z"
    },
    "papermill": {
     "duration": 0.492958,
     "end_time": "2025-07-01T13:44:15.465240",
     "exception": false,
     "start_time": "2025-07-01T13:44:14.972282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 283)\n",
      "[[-1.17187500e+00  0.00000000e+00  0.00000000e+00  3.00000000e+02]\n",
      " [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.86100006e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.73950000e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# get the manually derived ROI mask from the dataset, where we manually added it\n",
    "ROI_mask_path = \"/kaggle/input/segmentation-masked-ROI.nii\"\n",
    "ROI_mask = nib.load(ROI_mask_path)\n",
    "\n",
    "print(ROI_mask.get_fdata().shape)\n",
    "print(ROI_mask.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7523d",
   "metadata": {
    "papermill": {
     "duration": 0.032862,
     "end_time": "2025-07-01T13:44:15.532799",
     "exception": false,
     "start_time": "2025-07-01T13:44:15.499937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ROI mask is a binary mask highlighting the lymphnodes of interest. We need a bounding box to crop the volume accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dc09056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:15.606474Z",
     "iopub.status.busy": "2025-07-01T13:44:15.605947Z",
     "iopub.status.idle": "2025-07-01T13:44:15.614334Z",
     "shell.execute_reply": "2025-07-01T13:44:15.613432Z"
    },
    "papermill": {
     "duration": 0.04941,
     "end_time": "2025-07-01T13:44:15.616127",
     "exception": false,
     "start_time": "2025-07-01T13:44:15.566717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def get_mask_bbox_slices(mask_nii_path):\n",
    "    \"\"\"\n",
    "    Load a binary ROI mask NIfTI and compute the minimal 3D bounding\n",
    "    box slices containing all positive voxels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask_nii_path : str or Path\n",
    "        Path to the input binary ROI mask NIfTI (.nii or .nii.gz).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3-tuple of Python slice objects (x_slice, y_slice, z_slice)\n",
    "        defining the minimal bounding box.\n",
    "    \"\"\"\n",
    "    # 1) Load mask\n",
    "    nii = nib.load(str(mask_nii_path))\n",
    "    data = nii.get_fdata()\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input NIfTI must be a 3D volume\")\n",
    "    \n",
    "    # 2) Find indices of positive voxels\n",
    "    pos_voxels = np.argwhere(data > 0)\n",
    "    if pos_voxels.size == 0:\n",
    "        raise ValueError(\"No positive voxels found in mask\")\n",
    "    \n",
    "    # 3) Compute min/max per axis\n",
    "    x_min, y_min, z_min = pos_voxels.min(axis=0)\n",
    "    x_max, y_max, z_max = pos_voxels.max(axis=0)\n",
    "    \n",
    "    # 4) Build slice objects (end is exclusive, hence +1)\n",
    "    bbox_slices = (\n",
    "        slice(int(x_min), int(x_max) + 1),\n",
    "        slice(int(y_min), int(y_max) + 1),\n",
    "        slice(int(z_min), int(z_max) + 1),\n",
    "    )\n",
    "    \n",
    "    return bbox_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b7d8f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:15.698760Z",
     "iopub.status.busy": "2025-07-01T13:44:15.698505Z",
     "iopub.status.idle": "2025-07-01T13:44:17.064058Z",
     "shell.execute_reply": "2025-07-01T13:44:17.063138Z"
    },
    "papermill": {
     "duration": 1.402863,
     "end_time": "2025-07-01T13:44:17.065382",
     "exception": false,
     "start_time": "2025-07-01T13:44:15.662519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box slices:\n",
      "  x: slice(219, 351, None)\n",
      "  y: slice(189, 316, None)\n",
      "  z: slice(156, 182, None)\n"
     ]
    }
   ],
   "source": [
    "x_slice, y_slice, z_slice = get_mask_bbox_slices(ROI_mask_path)\n",
    "print(\"Bounding box slices:\")\n",
    "print(\"  x:\", x_slice)\n",
    "print(\"  y:\", y_slice)\n",
    "print(\"  z:\", z_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c047c5a",
   "metadata": {
    "papermill": {
     "duration": 0.035935,
     "end_time": "2025-07-01T13:44:17.135522",
     "exception": false,
     "start_time": "2025-07-01T13:44:17.099587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### to **crop** correctly the volume around the *ROI*, we need to derive the **receptive field** of the sliding window inference, that depends on the *patch size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dec3d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:17.207355Z",
     "iopub.status.busy": "2025-07-01T13:44:17.207062Z",
     "iopub.status.idle": "2025-07-01T13:44:17.211837Z",
     "shell.execute_reply": "2025-07-01T13:44:17.211187Z"
    },
    "papermill": {
     "duration": 0.041711,
     "end_time": "2025-07-01T13:44:17.212896",
     "exception": false,
     "start_time": "2025-07-01T13:44:17.171185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size:  [ 72 160 160]\n",
      "Receptive field:  [142 318 318]\n"
     ]
    }
   ],
   "source": [
    "patch_size = np.array(predictor.configuration_manager.patch_size)\n",
    "print(\"Patch size: \", patch_size)\n",
    "\n",
    "# Receptive field is twice the patch size-1\n",
    "RF = 2*(patch_size-1)\n",
    "print(\"Receptive field: \", RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fbfb13",
   "metadata": {
    "papermill": {
     "duration": 0.033259,
     "end_time": "2025-07-01T13:44:17.280458",
     "exception": false,
     "start_time": "2025-07-01T13:44:17.247199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Consider the receptive field to compute the final slices for cropping\n",
    "Remember that model metadata are related to transposed volume (nnunetv2 takes (D, H, W) shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c14fe7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:17.349065Z",
     "iopub.status.busy": "2025-07-01T13:44:17.348797Z",
     "iopub.status.idle": "2025-07-01T13:44:17.357042Z",
     "shell.execute_reply": "2025-07-01T13:44:17.356451Z"
    },
    "papermill": {
     "duration": 0.043041,
     "end_time": "2025-07-01T13:44:17.358124",
     "exception": false,
     "start_time": "2025-07-01T13:44:17.315083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original volume shape: (512, 512, 283)\n",
      "new  x: slice(60, 510, None)\n",
      "new  y: slice(30, 475, None)\n",
      "new  z: slice(85, 253, None)\n"
     ]
    }
   ],
   "source": [
    "volume_path = ct_img_path\n",
    "volume_shape = nib.load(volume_path).shape # (W, H, D) -> x, y, z\n",
    "# RF shape -> (D, H, W) -> z, y, x (model input shape)\n",
    "print(\"Original volume shape:\", volume_shape)\n",
    "\n",
    "W, H, D = volume_shape\n",
    "\n",
    "# backward sorted receptive field axes\n",
    "RF_x, RF_y, RF_z = RF[2],RF[1],RF[0]\n",
    "\n",
    "x_slice_RF = slice(int(max(x_slice.start - RF_x/2, 0)), int(min(x_slice.stop + RF_x/2, W)))\n",
    "y_slice_RF = slice(int(max(y_slice.start - RF_y/2, 0)), int(min(y_slice.stop + RF_y/2, H)))\n",
    "z_slice_RF = slice(int(max(z_slice.start - RF_z/2, 0)), int(min(z_slice.stop + RF_z/2, D)))\n",
    "\n",
    "print(\"new  x:\", x_slice_RF)\n",
    "print(\"new  y:\", y_slice_RF)\n",
    "print(\"new  z:\", z_slice_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4032a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:17.429440Z",
     "iopub.status.busy": "2025-07-01T13:44:17.429041Z",
     "iopub.status.idle": "2025-07-01T13:44:17.434178Z",
     "shell.execute_reply": "2025-07-01T13:44:17.433671Z"
    },
    "papermill": {
     "duration": 0.042775,
     "end_time": "2025-07-01T13:44:17.435223",
     "exception": false,
     "start_time": "2025-07-01T13:44:17.392448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def crop_volume_and_affine(nii_path, bbox_slices, save_cropped_nii_path=None):\n",
    "    \"\"\"\n",
    "    Crop a 3D NIfTI volume using the given bounding-box slices and\n",
    "    recompute the affine so the cropped volume retains correct world coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nii_path : str or Path\n",
    "        Path to the input NIfTI volume (.nii or .nii.gz).\n",
    "    bbox_slices : tuple of slice\n",
    "        A 3-tuple (x_slice, y_slice, z_slice) as returned by get_mask_bbox_slices().\n",
    "    save_cropped_nii_path : str or Path, optional\n",
    "        If provided, the cropped volume will be saved here as a new NIfTI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_data : np.ndarray\n",
    "        The volume data cropped to the bounding box.\n",
    "    new_affine : np.ndarray\n",
    "        The updated 4×4 affine transform for the cropped volume.\n",
    "    \"\"\"\n",
    "    # 1) Load the original image\n",
    "    img = nib.load(str(nii_path))\n",
    "    data = img.get_fdata()\n",
    "    affine = img.affine\n",
    "\n",
    "    # 2) Crop the data array\n",
    "    cropped_data = data[bbox_slices]\n",
    "\n",
    "    # 3) Extract the voxel‐offsets for x, y, z from the slice starts\n",
    "    x_slice, y_slice, z_slice = bbox_slices\n",
    "    z0, y0, x0 = z_slice.start, y_slice.start, x_slice.start\n",
    "\n",
    "    # 4) Compute the new affine translation: shift the origin by the voxel offsets\n",
    "    # Note voxel coordinates are (i, j, k) = (x, y, z)\n",
    "    offset_vox = np.array([x0, y0, z0])\n",
    "    new_affine = affine.copy()\n",
    "    new_affine[:3, 3] += affine[:3, :3].dot(offset_vox)\n",
    "\n",
    "    # 5) Optionally save the cropped volume\n",
    "    if save_cropped_nii_path is not None:\n",
    "        cropped_img = nib.Nifti1Image(cropped_data, new_affine)\n",
    "        nib.save(cropped_img, str(save_cropped_nii_path))\n",
    "\n",
    "    return cropped_data, new_affine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "425e52ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:17.503106Z",
     "iopub.status.busy": "2025-07-01T13:44:17.502892Z",
     "iopub.status.idle": "2025-07-01T13:44:20.609507Z",
     "shell.execute_reply": "2025-07-01T13:44:20.608544Z"
    },
    "papermill": {
     "duration": 3.142175,
     "end_time": "2025-07-01T13:44:20.610730",
     "exception": false,
     "start_time": "2025-07-01T13:44:17.468555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped data shape: (450, 445, 168)\n",
      "New affine:\n",
      " [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  2.29687500e+02]\n",
      " [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.50943756e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.31450000e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "slices = (x_slice_RF, y_slice_RF, z_slice_RF)\n",
    "\n",
    "nii_path = ct_img_path\n",
    "\n",
    "cropped_volume, affine_cropped_volume = crop_volume_and_affine(\n",
    "    nii_path=nii_path,\n",
    "    bbox_slices=slices,\n",
    "    save_cropped_nii_path=Path(\"cropped_volume_with_RF.nii.gz\")\n",
    ")\n",
    "\n",
    "print(\"Cropped data shape:\", cropped_volume.shape)\n",
    "print(\"New affine:\\n\", affine_cropped_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80010e3d",
   "metadata": {
    "papermill": {
     "duration": 0.033402,
     "end_time": "2025-07-01T13:44:20.679166",
     "exception": false,
     "start_time": "2025-07-01T13:44:20.645764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### We need a way to check original mask overlapping in the new cropped volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "702375d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:20.750294Z",
     "iopub.status.busy": "2025-07-01T13:44:20.749690Z",
     "iopub.status.idle": "2025-07-01T13:44:21.842143Z",
     "shell.execute_reply": "2025-07-01T13:44:21.841341Z"
    },
    "papermill": {
     "duration": 1.130174,
     "end_time": "2025-07-01T13:44:21.843431",
     "exception": false,
     "start_time": "2025-07-01T13:44:20.713257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped mask shape: (450, 445, 168)\n",
      "New mask affine:\n",
      " [[-1.17187500e+00  0.00000000e+00  0.00000000e+00  2.29687500e+02]\n",
      " [ 0.00000000e+00 -1.17187500e+00  0.00000000e+00  1.50943756e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  5.00000000e+00 -1.31450000e+03]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "cropped_mask, affine_cropped_mask = crop_volume_and_affine(\n",
    "    nii_path=ROI_mask_path,\n",
    "    bbox_slices=slices,\n",
    "    save_cropped_nii_path=Path(\"cropped_mask_with_RF.nii.gz\")\n",
    ")\n",
    "\n",
    "print(\"Cropped mask shape:\", cropped_mask.shape)\n",
    "print(\"New mask affine:\\n\", affine_cropped_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd4513",
   "metadata": {
    "papermill": {
     "duration": 0.036299,
     "end_time": "2025-07-01T13:44:21.916652",
     "exception": false,
     "start_time": "2025-07-01T13:44:21.880353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## We execute SHAP on this cropped image, and we only consider our ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011722a",
   "metadata": {
    "papermill": {
     "duration": 0.033673,
     "end_time": "2025-07-01T13:44:21.986158",
     "exception": false,
     "start_time": "2025-07-01T13:44:21.952485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ad1c011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:22.054741Z",
     "iopub.status.busy": "2025-07-01T13:44:22.054446Z",
     "iopub.status.idle": "2025-07-01T13:44:22.058684Z",
     "shell.execute_reply": "2025-07-01T13:44:22.058034Z"
    },
    "papermill": {
     "duration": 0.039933,
     "end_time": "2025-07-01T13:44:22.059760",
     "exception": false,
     "start_time": "2025-07-01T13:44:22.019827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bc5804b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:22.128152Z",
     "iopub.status.busy": "2025-07-01T13:44:22.127906Z",
     "iopub.status.idle": "2025-07-01T13:44:24.332612Z",
     "shell.execute_reply": "2025-07-01T13:44:24.331714Z"
    },
    "papermill": {
     "duration": 2.23977,
     "end_time": "2025-07-01T13:44:24.333787",
     "exception": false,
     "start_time": "2025-07-01T13:44:22.094017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume shape: torch.Size([1, 1, 168, 445, 450])\n"
     ]
    }
   ],
   "source": [
    "# (a) load + cropped volume  (1, C, D, H, W) – nnU-Net order\n",
    "nii_path_cropped = \"cropped_volume_with_RF.nii.gz\"\n",
    "dataset_json_path = Path(model_dir) / \"dataset.json\"\n",
    "\n",
    "volume_np = nnunetv2_default_preprocessing(nii_path_cropped, predictor, dataset_json_path)\n",
    "\n",
    "volume = torch.from_numpy(volume_np).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\n",
    "\n",
    "print(\"Volume shape:\", volume.shape)                # (1, C, D, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c240c9e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:24.403646Z",
     "iopub.status.busy": "2025-07-01T13:44:24.403207Z",
     "iopub.status.idle": "2025-07-01T13:44:42.583162Z",
     "shell.execute_reply": "2025-07-01T13:44:42.582578Z"
    },
    "papermill": {
     "duration": 18.215736,
     "end_time": "2025-07-01T13:44:42.584550",
     "exception": false,
     "start_time": "2025-07-01T13:44:24.368814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of supervoxel centers: 384\n"
     ]
    }
   ],
   "source": [
    "# (b) super-voxel / organ-id map  (W, H, D)\n",
    "# Load the image\n",
    "img = nib.load(nii_path_cropped)\n",
    "\n",
    "# Generate and save supervoxel map using the \n",
    "supervoxel_map = generate_supervoxel_map(img, S=100.0)\n",
    "                    # (D, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24b0d95f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:42.658485Z",
     "iopub.status.busy": "2025-07-01T13:44:42.657963Z",
     "iopub.status.idle": "2025-07-01T13:44:43.513124Z",
     "shell.execute_reply": "2025-07-01T13:44:43.512481Z"
    },
    "papermill": {
     "duration": 0.893641,
     "end_time": "2025-07-01T13:44:43.514657",
     "exception": false,
     "start_time": "2025-07-01T13:44:42.621016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "supervoxel_map_path = 'FCC-supervoxel_map.nii.gz'\n",
    "supervoxel_map_img = nib.Nifti1Image(supervoxel_map, affine_cropped_mask)\n",
    "nib.save(supervoxel_map_img, supervoxel_map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1670bdf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:43.594002Z",
     "iopub.status.busy": "2025-07-01T13:44:43.593372Z",
     "iopub.status.idle": "2025-07-01T13:44:46.271402Z",
     "shell.execute_reply": "2025-07-01T13:44:46.270507Z"
    },
    "papermill": {
     "duration": 2.71804,
     "end_time": "2025-07-01T13:44:46.272672",
     "exception": false,
     "start_time": "2025-07-01T13:44:43.554632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of supervoxels:  384\n",
      "Mask shape: torch.Size([168, 445, 450])\n"
     ]
    }
   ],
   "source": [
    "supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\n",
    "# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\n",
    "sv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\n",
    "\n",
    "supervoxel_map = indexes.reshape(supervoxel_map.shape)\n",
    "print(\"number of supervoxels: \", np.unique(supervoxel_map).size)\n",
    "\n",
    "# IMPORTANT 🔸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\n",
    "supervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\n",
    "\n",
    "print(\"Mask shape:\", supervoxel_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477eba9",
   "metadata": {
    "papermill": {
     "duration": 0.035403,
     "end_time": "2025-07-01T13:44:46.345119",
     "exception": false,
     "start_time": "2025-07-01T13:44:46.309716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### derive baseline cached dictionary\n",
    "using planner, iterator, predictor (just temporary solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7ec2db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:46.414285Z",
     "iopub.status.busy": "2025-07-01T13:44:46.413624Z",
     "iopub.status.idle": "2025-07-01T13:44:46.419552Z",
     "shell.execute_reply": "2025-07-01T13:44:46.418856Z"
    },
    "papermill": {
     "duration": 0.041921,
     "end_time": "2025-07-01T13:44:46.420666",
     "exception": false,
     "start_time": "2025-07-01T13:44:46.378745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cached_output_dictionary(volume_file: Path,\n",
    "                                 predictor: CustomNNUNetPredictor,\n",
    "                                verbose: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Return a dictionary indexed by the slices for the sliding window, of the output of the inference for each patch\n",
    "        of the original volume\n",
    "        \"\"\"\n",
    "        rw = predictor.plans_manager.image_reader_writer_class()\n",
    "        # If nnU-Net returns a class instead of an instance, instantiate it\n",
    "        if callable(rw) and not hasattr(rw, \"read_images\"):\n",
    "            rw = rw()\n",
    "        orig_image, orig_props = rw.read_images(\n",
    "            [str(volume_file)]\n",
    "        )                     # (C, Z, Y, X)\n",
    "\n",
    "        preprocessor = predictor.configuration_manager.preprocessor_class()\n",
    "    \n",
    "        data_pp, _, _ = preprocessor.run_case_npy(\n",
    "                orig_image,\n",
    "                seg=None,\n",
    "                properties=orig_props,\n",
    "                plans_manager=predictor.plans_manager,\n",
    "                configuration_manager=predictor.configuration_manager,\n",
    "                dataset_json=predictor.dataset_json\n",
    "            )\n",
    "        # to torch, channel-first is already true\n",
    "        inp_tensor = torch.from_numpy(data_pp)\n",
    "        \n",
    "        slicers = predictor._internal_get_sliding_window_slicers(inp_tensor.shape[1:])\n",
    "        if verbose:\n",
    "            print(\"first 3 slicers of Iterator object: \", slicers[:3])\n",
    "\n",
    "        dictionary = predictor.get_output_dictionary_sliding_window(inp_tensor, slicers)\n",
    "\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06ddb9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:44:46.489018Z",
     "iopub.status.busy": "2025-07-01T13:44:46.488753Z"
    },
    "papermill": {
     "duration": 3.03817,
     "end_time": "2025-07-01T13:44:49.492278",
     "exception": false,
     "start_time": "2025-07-01T13:44:46.454108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"import pickle as pkl\n",
    "\n",
    "with open(\"baseline_output_dictionary_cache.pkl\", \"rb\") as f:\n",
    "    cropped_baseline_pred_cache = pkl.load(f)\"\"\"\n",
    "\n",
    "cropped_baseline_pred_cache = get_cached_output_dictionary(\n",
    "    volume_file = nii_path_cropped,\n",
    "    predictor = predictor,\n",
    "    verbose = True,\n",
    ")\n",
    "import pickle as pkl\n",
    "# Write to file\n",
    "with open(\"cropped_baseline_output_dictionary_cache.pkl\", \"wb\") as f:\n",
    "    pkl.dump(cropped_baseline_pred_cache, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb854b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T12:33:17.521192Z",
     "iopub.status.busy": "2025-07-01T12:33:17.520973Z",
     "iopub.status.idle": "2025-07-01T12:33:17.525733Z",
     "shell.execute_reply": "2025-07-01T12:33:17.524995Z",
     "shell.execute_reply.started": "2025-07-01T12:33:17.521175Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cropped_baseline_pred_cache[(None, None, None), (0, 72, None), (0, 160, None), (0, 160, None)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f082347",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### get our cropped ROI mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249e629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T12:33:17.526749Z",
     "iopub.status.busy": "2025-07-01T12:33:17.526451Z",
     "iopub.status.idle": "2025-07-01T12:33:17.729178Z",
     "shell.execute_reply": "2025-07-01T12:33:17.728563Z",
     "shell.execute_reply.started": "2025-07-01T12:33:17.526733Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROI_mask = np.transpose(cropped_mask, (2, 1, 0))\n",
    "ROI_mask = torch.from_numpy(ROI_mask).to(device)\n",
    "\n",
    "print(ROI_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a961973",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Include masking in the forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d6451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:27:06.423257Z",
     "iopub.status.busy": "2025-07-01T13:27:06.422982Z",
     "iopub.status.idle": "2025-07-01T13:27:06.429739Z",
     "shell.execute_reply": "2025-07-01T13:27:06.428940Z",
     "shell.execute_reply.started": "2025-07-01T13:27:06.423235Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chrabaszcz_aggregation(logits: torch.Tensor,\n",
    "                           scaling_factor: float = 1.0,\n",
    "                          ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n",
    "    Explanatory Analysis of 3D Segmentation Models\"\n",
    "    \"\"\"\n",
    "    seg_mask = torch.argmax(logits, dim=0)\n",
    "    aggregate = torch.sum(logits[1].double() * seg_mask) / scaling_factor  # normalize to avoid overflows in SHAP\n",
    "\n",
    "    return aggregate\n",
    "\n",
    "def chrabaszcz_aggregation_with_unperturbed_mask_filtering(logits: torch.Tensor,\n",
    "                          unperturbed_binary_mask: torch.Tensor,\n",
    "                           scaling_factor: float = 1.0,\n",
    "                          ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    aggregate the output logits in a sum, following the proposed method in \"Chrabaszcz et al. - 2024 - Aggregated Attributions for\n",
    "    Explanatory Analysis of 3D Segmentation Models\", with the  addition of filtering by the unperturbed segmentation.ù\n",
    "    We can use this to ignore \"false positive\" voxels\n",
    "    \"\"\"\n",
    "    seg_mask = torch.argmax(logits, dim=0)          # (D,H,W)\n",
    "    seg_mask = seg_mask * unperturbed_binary_mask\n",
    "    aggregate = torch.sum(logits[1].double() * seg_mask) / scaling_factor  # normalize to avoid overflows in SHAP\n",
    "\n",
    "    return aggregate\n",
    "\n",
    "def dice_aggregation(logits: torch.Tensor,\n",
    "                    unperturbed_binary_mask: torch.Tensor,\n",
    "                    scaling_factor: float = 1.0,\n",
    "                    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Use Dice score, the same aggregation measure from \"Hasany et al. - 2024 - MiSuRe is all you need to explain your image segmentation\"\n",
    "    Dice score provides a single aggregation metric that accounts for both false negatives and false positives penalization.\n",
    "    Specificly, we instead score each perturbation supervoxels by that Dice => supervoxels that contribute the most in reproducing\n",
    "    the baseline segmentation, will get an higher score\n",
    "    \"\"\"\n",
    "    seg_mask = torch.argmax(logits, dim=0)          # (D,H,W)\n",
    "    intersection = (seg_mask * unperturbed_binary_mask)\n",
    "    denominator = torch.sum(seg_mask) + torch.sum(unperturbed_binary_mask) + 1e-6    # add small epsilon for numerical stability\n",
    "    denominator *= scaling_factor\n",
    "    dice = 2 * torch.sum(intersection) / denominator\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c6ceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:28:51.858260Z",
     "iopub.status.busy": "2025-07-01T13:28:51.857993Z",
     "iopub.status.idle": "2025-07-01T13:30:48.083240Z",
     "shell.execute_reply": "2025-07-01T13:30:48.082646Z",
     "shell.execute_reply.started": "2025-07-01T13:28:51.858241Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ❷  Forward wrapper that nnU-Net expects\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "@torch.inference_mode()\n",
    "def forward_segmentation_output_to_explain(\n",
    "        input_image:         torch.Tensor,\n",
    "        perturbation_mask:   torch.BoolTensor,\n",
    "        ROI_binary_mask:            torch.Tensor,   # remember that must be cropped to the same size of the other tensors\n",
    "        baseline_prediction_dict: dict\n",
    ") -> torch.Tensor:           # returns a scalar per sample\n",
    "    \"\"\"\n",
    "    Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n",
    "    by the network – adapt to your real metric as needed.\n",
    "    \"\"\"\n",
    "    logits = predictor.predict_sliding_window_return_logits_with_caching(\n",
    "        input_image, perturbation_mask, baseline_prediction_dict,\n",
    "    )                              # (C, D, H, W)\n",
    "    # we now mask both by the segmentation prevalent class, and by ROI\n",
    "    D,H,W = logits.shape[1:]\n",
    "    \"\"\"aggregate = chrabaszcz_aggregation_with_unperturbed_mask_filtering(\n",
    "        logits = logits,\n",
    "        unperturbed_binary_mask = ROI_binary_mask,\n",
    "        scaling_factor = (D*H*W)\n",
    "    )\"\"\"\n",
    "    aggregate = dice_aggregation(logits, ROI_binary_mask)\n",
    "\n",
    "    return aggregate\n",
    "\n",
    "# c) wrap your cached‐forward method:\n",
    "explainer = KernelShapWithMask(\n",
    "    forward_func=lambda vol, mask: forward_segmentation_output_to_explain(\n",
    "        input_image=vol,\n",
    "        perturbation_mask=mask,\n",
    "        ROI_binary_mask=ROI_mask,\n",
    "        baseline_prediction_dict=cropped_baseline_pred_cache)\n",
    ")\n",
    "\n",
    "# d) compute SHAP\n",
    "attr = explainer.attribute(\n",
    "    inputs=volume,       # (1,C,D,H,W)\n",
    "    baselines=0.0, \n",
    "    feature_mask=supervoxel_map,\n",
    "    n_samples=1000,    \n",
    "    return_input_shape=True,\n",
    "    show_progress=True,\n",
    ")\n",
    "print(\"Attributions:\", attr.shape)  # → (1,C,D,H,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fac0b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T13:30:53.708257Z",
     "iopub.status.busy": "2025-07-01T13:30:53.708021Z",
     "iopub.status.idle": "2025-07-01T13:30:54.284502Z",
     "shell.execute_reply": "2025-07-01T13:30:54.283792Z",
     "shell.execute_reply.started": "2025-07-01T13:30:53.708240Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "attr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\n",
    "attr_img = nib.Nifti1Image(attr_postprocessed, affine_cropped_volume)\n",
    "nib.save(attr_img, 'attribution_map-FCC-ROI.nii.gz')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7037513,
     "sourceId": 12289924,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.483317,
   "end_time": "2025-07-01T13:44:49.529737",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-01T13:42:00.046420",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
