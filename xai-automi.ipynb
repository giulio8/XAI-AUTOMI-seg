{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12289924,"sourceType":"datasetVersion","datasetId":7037513}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import Packages for Google Colab","metadata":{"id":"THifmOYu9Cip"}},{"cell_type":"code","source":"# Import basic packages for later use\nimport os\nimport shutil\nfrom collections import OrderedDict\n\nimport json\nimport matplotlib.pyplot as plt\nimport nibabel as nib\n\nimport numpy as np\nimport torch","metadata":{"id":"k-hj38QV_raZ","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:39.098206Z","iopub.execute_input":"2025-07-03T09:14:39.098841Z","iopub.status.idle":"2025-07-03T09:14:42.508419Z","shell.execute_reply.started":"2025-07-03T09:14:39.098805Z","shell.execute_reply":"2025-07-03T09:14:42.507440Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# check whether GPU accelerated computing is available\nassert torch.cuda.is_available() # if there is an error here, enable GPU in the Runtime","metadata":{"id":"Ld3_-quILggy","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:42.510097Z","iopub.execute_input":"2025-07-03T09:14:42.510666Z","iopub.status.idle":"2025-07-03T09:14:42.593793Z","shell.execute_reply.started":"2025-07-03T09:14:42.510635Z","shell.execute_reply":"2025-07-03T09:14:42.592663Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install nnunetv2\n!pip install captum","metadata":{"id":"c4KbMYcEWZjU","outputId":"ba9a5149-e800-477a-9717-4f4796903bc1","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:42.594762Z","iopub.execute_input":"2025-07-03T09:14:42.595057Z","iopub.status.idle":"2025-07-03T09:14:49.900487Z","shell.execute_reply.started":"2025-07-03T09:14:42.595037Z","shell.execute_reply":"2025-07-03T09:14:49.899641Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nnunetv2 in /usr/local/lib/python3.11/dist-packages (2.6.2)\nRequirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.5.1+cu124)\nRequirement already satisfied: acvl-utils<0.3,>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.2.5)\nRequirement already satisfied: dynamic-network-architectures<0.5,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.4.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (4.67.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.15.2)\nRequirement already satisfied: batchgenerators>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.25.1)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (1.2.2)\nRequirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.25.1)\nRequirement already satisfied: SimpleITK>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.4.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.2.3)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.20.3)\nRequirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2025.1.10)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2.32.3)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (5.3.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.12.2)\nRequirement already satisfied: imagecodecs in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (2025.3.30)\nRequirement already satisfied: yacs in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.1.8)\nRequirement already satisfied: batchgeneratorsv2>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.3.0)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (0.8.1)\nRequirement already satisfied: blosc2>=3.0.0b1 in /usr/local/lib/python3.11/dist-packages (from nnunetv2) (3.1.0)\nRequirement already satisfied: connected-components-3d in /usr/local/lib/python3.11/dist-packages (from acvl-utils<0.3,>=0.2.3->nnunetv2) (3.24.0)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (11.1.0)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (1.0.0)\nRequirement already satisfied: unittest2 in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (1.1.0)\nRequirement already satisfied: threadpoolctl in /usr/local/lib/python3.11/dist-packages (from batchgenerators>=0.25.1->nnunetv2) (3.6.0)\nRequirement already satisfied: fft-conv-pytorch in /usr/local/lib/python3.11/dist-packages (from batchgeneratorsv2>=0.3.0->nnunetv2) (1.2.0)\nRequirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (1.9.2)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (1.1.0)\nRequirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (2.10.2)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (9.0.0)\nRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (0.28.1)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=3.0.0b1->nnunetv2) (4.3.7)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.0.14)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->nnunetv2) (2.4.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (2.37.0)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (24.2)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (4.13.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.2->nnunetv2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.2->nnunetv2) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nnunetv2) (2.9.0.post0)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->nnunetv2) (6.5.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunetv2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nnunetv2) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->nnunetv2) (2025.1.31)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nnunetv2) (1.4.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from yacs->nnunetv2) (6.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->blosc2>=3.0.0b1->nnunetv2) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->blosc2>=3.0.0b1->nnunetv2) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->blosc2>=3.0.0b1->nnunetv2) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->nnunetv2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->nnunetv2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->nnunetv2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->nnunetv2) (2024.2.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.20.1+cu124)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.30.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.5.2)\nCollecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: traceback2 in /usr/local/lib/python3.11/dist-packages (from unittest2->batchgenerators>=0.25.1->nnunetv2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->nnunetv2) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->blosc2>=3.0.0b1->nnunetv2) (1.3.1)\nRequirement already satisfied: linecache2 in /usr/local/lib/python3.11/dist-packages (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2) (1.0.0)\nUsing cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nInstalling collected packages: argparse\nSuccessfully installed argparse-1.4.0\nRequirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.8.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.7.5)\nRequirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (24.2)\nRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.5.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->captum) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->captum) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->captum) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->captum) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 2. Connect Google Colab with GoogleDrive","metadata":{"id":"tgssjiuNVvq5"}},{"cell_type":"code","source":"from batchgenerators.utilities.file_and_folder_operations import join\n\n\"\"\"# for colab users only - mounting the drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive',force_remount = True)\n\ndrive_dir = \"/content/drive/My Drive\"\nmount_dir = join(drive_dir, \"tesi\", \"automi\")\nbase_dir = os.getcwd()\"\"\"\nmount_dir = \"/kaggle/input/automi-seg\"\nbase_dir = os.getcwd()\nprint(base_dir)\n!cd \"/kaggle/input/automi-seg\" ; ls","metadata":{"id":"_WLi-mVRjbfb","outputId":"e8440113-7302-4a08-9b64-d35179d4e3b4","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:49.902509Z","iopub.execute_input":"2025-07-03T09:14:49.902820Z","iopub.status.idle":"2025-07-03T09:14:50.047119Z","shell.execute_reply.started":"2025-07-03T09:14:49.902787Z","shell.execute_reply":"2025-07-03T09:14:50.046096Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\ncode  nnunet_raw  preprocessed_files  results  segmentation-masked-ROI.nii\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 3. Setting up nnU-Nets folder structure and environment variables\nnnUnet expects a certain folder structure and environment variables.\n\nRoughly they tell nnUnet:\n1. Where to look for stuff\n2. Where to put stuff\n\nFor more information about this please check: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/setting_up_paths.md","metadata":{"id":"d-CV4Jc4W77P"}},{"cell_type":"code","source":"def make_if_dont_exist(folder_path,overwrite=False):\n    \"\"\"\n    creates a folder if it does not exists\n    input:\n    folder_path : relative path of the folder which needs to be created\n    over_write :(default: False) if True overwrite the existing folder\n    \"\"\"\n    if os.path.exists(folder_path):\n\n        if not overwrite:\n            print(f\"{folder_path} exists.\")\n        else:\n            print(f\"{folder_path} overwritten\")\n            shutil.rmtree(folder_path)\n            os.makedirs(folder_path)\n\n    else:\n      os.makedirs(folder_path)\n      print(f\"{folder_path} created!\")","metadata":{"id":"T9ifLrYhjfAT","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.048350Z","iopub.execute_input":"2025-07-03T09:14:50.048692Z","iopub.status.idle":"2025-07-03T09:14:50.054483Z","shell.execute_reply.started":"2025-07-03T09:14:50.048653Z","shell.execute_reply":"2025-07-03T09:14:50.053684Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3.1 Set environment Variables and creating folders","metadata":{"id":"JAvjVPF0_7t3"}},{"cell_type":"code","source":"# ===========================\n# 📦 SETUP nnUNet ENVIRONMENT\n# ===========================\n\n# Definisci i path da settare\npath_dict = {\n    \"nnUNet_raw\": join(mount_dir, \"nnunet_raw\"),\n    \"nnUNet_preprocessed\": join(mount_dir, \"preprocessed_files\"),#\"nnUNet_preprocessed\"),\n    \"nnUNet_results\": join(mount_dir, \"results\"),#\"nnUNet_results\"),\n    # \"RAW_DATA_PATH\": join(mount_dir, \"RawData\"),  # Facoltativo, se ti serve salvare zips\n}\n\n# Scrivi i path nelle variabili di ambiente, che vengono lette dal modulo paths di nnunetv2\nfor env_var, path in path_dict.items():\n    os.environ[env_var] = path\n\nfrom nnunetv2.paths import nnUNet_results, nnUNet_raw\n\nprint(\"nnUNet_raw:\", nnUNet_raw)\nprint(\"nnUNet_results:\", nnUNet_results)\nif nnUNet_raw == None:\n    nnUNet_raw = \"/kaggle/input/automi-seg/nnunet_raw\"\nif nnUNet_results == None:\n    nnUNet_results = \"/kaggle/input/automi-seg/results\"","metadata":{"id":"3rlqq-V-CWh8","outputId":"e16ea0fc-57b8-4478-e8df-14942c962c9c","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.055488Z","iopub.execute_input":"2025-07-03T09:14:50.055804Z","iopub.status.idle":"2025-07-03T09:14:50.069465Z","shell.execute_reply.started":"2025-07-03T09:14:50.055761Z","shell.execute_reply":"2025-07-03T09:14:50.068657Z"}},"outputs":[{"name":"stdout","text":"nnUNet_raw: /kaggle/input/automi-seg/nnunet_raw\nnnUNet_results: /kaggle/input/automi-seg/results\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import nibabel as nib\n\ndataset_name = 'Dataset003_AUTOMI_CTVLNF_NEWGL_results'\nimg = nib.load(join(nnUNet_results, dataset_name, 'nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/test_img/AUTOMI_00027_0000.nii'))\nprint(\"Shape:\", img.shape)\nprint(\"Data type:\", img.get_data_dtype())","metadata":{"id":"gUkthkO22NXV","outputId":"182ae085-cb5a-4668-b8a2-f823d83c8101","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.070340Z","iopub.execute_input":"2025-07-03T09:14:50.070637Z","iopub.status.idle":"2025-07-03T09:14:50.085248Z","shell.execute_reply.started":"2025-07-03T09:14:50.070612Z","shell.execute_reply":"2025-07-03T09:14:50.084313Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Shape: (512, 512, 236)\nData type: int16\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!ls '/kaggle/input/automi-seg/results'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.086229Z","iopub.execute_input":"2025-07-03T09:14:50.086521Z","iopub.status.idle":"2025-07-03T09:14:50.220092Z","shell.execute_reply.started":"2025-07-03T09:14:50.086486Z","shell.execute_reply":"2025-07-03T09:14:50.219094Z"}},"outputs":[{"name":"stdout","text":"Dataset003_AUTOMI_CTVLNF_NEWGL_results\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Some tests","metadata":{}},{"cell_type":"code","source":"ct_img_path = join(nnUNet_raw, \"imagesTr\", \"AUTOMI_00039_0000.nii\")\norgan_mask_path = join(nnUNet_raw, \"total_segmentator_structures\", \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\")\nct_img = nib.load(ct_img_path)\norgan_mask = nib.load(organ_mask_path)","metadata":{"id":"RO5hP5Ao79QG","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.221316Z","iopub.execute_input":"2025-07-03T09:14:50.221620Z","iopub.status.idle":"2025-07-03T09:14:50.232426Z","shell.execute_reply.started":"2025-07-03T09:14:50.221589Z","shell.execute_reply":"2025-07-03T09:14:50.231666Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"CT shape:\", ct_img.shape)\nprint(\"Organ shape:\", organ_mask.shape)\nprint(\"Spacing:\", ct_img.header.get_zooms())\nprint(\"Organ spacing:\", organ_mask.header.get_zooms())","metadata":{"id":"0RGc0dNi9Wop","outputId":"770675ee-5426-493d-a037-b7fdf58afe5d","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.236387Z","iopub.execute_input":"2025-07-03T09:14:50.236683Z","iopub.status.idle":"2025-07-03T09:14:50.241413Z","shell.execute_reply.started":"2025-07-03T09:14:50.236666Z","shell.execute_reply":"2025-07-03T09:14:50.240780Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"CT shape: (512, 512, 283)\nOrgan shape: (512, 512, 283)\nSpacing: (1.171875, 1.171875, 5.0)\nOrgan spacing: (1.171875, 1.171875, 5.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Re-align CT scan with its own organ segmentation mask","metadata":{}},{"cell_type":"code","source":"import SimpleITK as sitk\n\n# Load CT and misaligned organ mask\nct = sitk.ReadImage(ct_img_path, sitk.sitkFloat32)\norgan_mask = sitk.ReadImage(organ_mask_path, sitk.sitkUInt8)\n\n# Resample organ mask to match CT space\nresampler = sitk.ResampleImageFilter()\nresampler.SetReferenceImage(ct)\nresampler.SetInterpolator(sitk.sitkNearestNeighbor)\norgan_resampled = resampler.Execute(organ_mask)\n\n# Save aligned output\n#sitk.WriteImage(organ_resampled, join(nnUNet_raw, \"organ_mask_resampled_to_ct.nii.gz\"))\nsitk.WriteImage(organ_resampled, \"organ_mask_resampled_to_ct.nii.gz\")","metadata":{"id":"2UGYxNOXBhmN","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:50.242530Z","iopub.execute_input":"2025-07-03T09:14:50.242849Z","iopub.status.idle":"2025-07-03T09:14:51.813009Z","shell.execute_reply.started":"2025-07-03T09:14:50.242830Z","shell.execute_reply":"2025-07-03T09:14:51.812248Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#organ_mask_path = join(nnUNet_raw, \"organ_mask_resampled_to_ct.nii.gz\")\norgan_mask_path = \"organ_mask_resampled_to_ct.nii.gz\"\nct_img = nib.load(ct_img_path)\norgan_mask = nib.load(organ_mask_path)\nprint(\"CT shape:\", ct_img.shape)\nprint(\"Organ shape:\", organ_mask.shape)\nprint(\"Spacing:\", ct_img.header.get_zooms())\nprint(\"Organ spacing:\", organ_mask.header.get_zooms())","metadata":{"id":"Fq-CUTS2Omeq","outputId":"1ab8dd02-93d3-4430-9a08-ec72940d36a0","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:51.813994Z","iopub.execute_input":"2025-07-03T09:14:51.814243Z","iopub.status.idle":"2025-07-03T09:14:51.836503Z","shell.execute_reply.started":"2025-07-03T09:14:51.814224Z","shell.execute_reply":"2025-07-03T09:14:51.835820Z"}},"outputs":[{"name":"stdout","text":"CT shape: (512, 512, 283)\nOrgan shape: (512, 512, 283)\nSpacing: (1.171875, 1.171875, 5.0)\nOrgan spacing: (1.171875, 1.171875, 5.0)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Create test perturbation covering one, full organ each","metadata":{}},{"cell_type":"code","source":"#output_dir = join(nnUNet_raw, \"perturbed_images\")\noutput_dir = \"perturbed_image\"\nmake_if_dont_exist(output_dir, overwrite=False)\n\nct_data = ct_img.get_fdata()\norgan_mask_data = organ_mask.get_fdata().astype(np.int32)\n\n# --- IDENTIFY ORGANS PRESENT ---\norgan_ids = np.unique(organ_mask_data)\norgan_ids = organ_ids[organ_ids != 0]  # Exclude background\n\nprint(f\"Found {len(organ_ids)} organs in the mask: {organ_ids}\")\n\n# --- CREATE PERTURBED CTs IF NEEDED ---\nperturbed_paths = []\n\nfor organ_id in organ_ids:\n    out_path = join(output_dir, f\"ct_perturbed_without_organ_{organ_id}.nii.gz\")\n\n    if os.path.exists(out_path):\n        print(f\"✔ Organ {organ_id} already processed — skipping.\")\n        perturbed_paths.append((organ_id, out_path))\n        continue\n\n    print(f\"Generating perturbation for organ ID: {organ_id}\")\n\n    # Mask out voxels belonging to this organ\n    ct_perturbed = np.where(organ_mask_data == organ_id, 0, ct_data)\n\n    # Create and save NIfTI image\n    perturbed_img = nib.Nifti1Image(ct_perturbed, affine=ct_img.affine)\n    nib.save(perturbed_img, out_path)\n\n    perturbed_paths.append((organ_id, out_path))\n\nprint(\"\\n✅ Perturbed volume generation completed!\")","metadata":{"id":"7caePLk9PB2A","outputId":"5b56d0f2-df82-4f8f-9a3d-7c020e732513","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:51.837414Z","iopub.execute_input":"2025-07-03T09:14:51.837672Z","iopub.status.idle":"2025-07-03T09:14:56.949220Z","shell.execute_reply.started":"2025-07-03T09:14:51.837652Z","shell.execute_reply":"2025-07-03T09:14:56.948258Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"perturbed_image exists.\nFound 9 organs in the mask: [ 50  75 100 125 150 175 200 225 255]\n✔ Organ 50 already processed — skipping.\n✔ Organ 75 already processed — skipping.\n✔ Organ 100 already processed — skipping.\n✔ Organ 125 already processed — skipping.\n✔ Organ 150 already processed — skipping.\n✔ Organ 175 already processed — skipping.\n✔ Organ 200 already processed — skipping.\n✔ Organ 225 already processed — skipping.\n✔ Organ 255 already processed — skipping.\n\n✅ Perturbed volume generation completed!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# NO MORE NEEDED \n\n# command for copying in kaggle working directory\n#!cp -r /kaggle/input/automi-seg/results/Dataset003_AUTOMI_CTVLNF_NEWGL_results ./\n\nmodel_dir_readonly = join(nnUNet_results, 'Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres')\nmodel_dir = join('Dataset003_AUTOMI_CTVLNF_NEWGL_results', 'nnUNetTrainer__nnUNetPlans__3d_fullres')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:56.950379Z","iopub.execute_input":"2025-07-03T09:14:56.950680Z","iopub.status.idle":"2025-07-03T09:14:56.955394Z","shell.execute_reply.started":"2025-07-03T09:14:56.950654Z","shell.execute_reply":"2025-07-03T09:14:56.954651Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#!ls Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0\n#from torch import load as torchload\n#prova = torchload(\"Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth\", weights_only=True)\n#prova = torchload(join(nnUNet_results, \"Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth\"), weights_only=False)\n\n# crazy bug happening caused by weights ￼Grafici interattivi di questo tipo non ancora supportatifile corrupted for unknown reason, reloading possible with the following\n#!cp '/kaggle/input/automi-seg/results/Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth' 'Dataset003_AUTOMI_CTVLNF_NEWGL_results/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0'\n\n# updata, corruption of working directory is a serious possibility \n\"\"\"Dustin\nKaggle Staff\nPosted 2 years ago\n@gohonleeds Session persistence is best-effort (not guaranteed). If you are restarting the notebook frequently it may not always succeed to upload all the files (especially if there are many). Its supposed to \"wait\" between start/stop of new sessions for the upload to complete, but the upload might fail if it takes too long, or the \"wait\" logic might get confused if there are too many start/stops too quickly.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:56.956305Z","iopub.execute_input":"2025-07-03T09:14:56.956544Z","iopub.status.idle":"2025-07-03T09:14:56.970085Z","shell.execute_reply.started":"2025-07-03T09:14:56.956527Z","shell.execute_reply":"2025-07-03T09:14:56.969483Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Dustin\\nKaggle Staff\\nPosted 2 years ago\\n@gohonleeds Session persistence is best-effort (not guaranteed). If you are restarting the notebook frequently it may not always succeed to upload all the files (especially if there are many). Its supposed to \"wait\" between start/stop of new sessions for the upload to complete, but the upload might fail if it takes too long, or the \"wait\" logic might get confused if there are too many start/stops too quickly.\\n'"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## kaggle decompresses i .nii.gz in .nii, while nnunetv2 expects compressed volumes","metadata":{}},{"cell_type":"code","source":"\n\"\"\"\nimport gzip\nimport shutil\nimport os\n\n# Indica qui la cartella dove sono i .nii\nfolder = join(model_dir, 'fold_0', 'test_img')  # esempio: '/kaggle/input/your_dataset'\n\nfor filename in os.listdir(folder):\n    if filename.endswith('.nii') and not filename.endswith('.nii.gz'):\n        nii_path = os.path.join(folder, filename)\n        gz_path = nii_path + '.gz'\n\n        # Comprime il file .nii in .nii.gz\n        with open(nii_path, 'rb') as f_in:\n            with gzip.open(gz_path, 'wb') as f_out:\n                shutil.copyfileobj(f_in, f_out)\n\n        # Dopo la compressione, elimina il file .nii originale\n        os.remove(nii_path)\n\nprint(\"Compressione completata! Tutti i file sono ora in formato .nii.gz\")\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:56.970859Z","iopub.execute_input":"2025-07-03T09:14:56.971570Z","iopub.status.idle":"2025-07-03T09:14:56.984782Z","shell.execute_reply.started":"2025-07-03T09:14:56.971545Z","shell.execute_reply":"2025-07-03T09:14:56.984025Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\nimport gzip\\nimport shutil\\nimport os\\n\\n# Indica qui la cartella dove sono i .nii\\nfolder = join(model_dir, \\'fold_0\\', \\'test_img\\')  # esempio: \\'/kaggle/input/your_dataset\\'\\n\\nfor filename in os.listdir(folder):\\n    if filename.endswith(\\'.nii\\') and not filename.endswith(\\'.nii.gz\\'):\\n        nii_path = os.path.join(folder, filename)\\n        gz_path = nii_path + \\'.gz\\'\\n\\n        # Comprime il file .nii in .nii.gz\\n        with open(nii_path, \\'rb\\') as f_in:\\n            with gzip.open(gz_path, \\'wb\\') as f_out:\\n                shutil.copyfileobj(f_in, f_out)\\n\\n        # Dopo la compressione, elimina il file .nii originale\\n        os.remove(nii_path)\\n\\nprint(\"Compressione completata! Tutti i file sono ora in formato .nii.gz\")'"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Do some test inference on the perturbed images","metadata":{}},{"cell_type":"code","source":"from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n\n# instantiate the nnUNetPredictor\npredictor = nnUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=True, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=False,\n    verbose_preprocessing=False,\n    allow_tqdm=True\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir_readonly,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)\n\n\n# variant 1: give input and output folders\nimgs_dir = join(model_dir_readonly, 'fold_0', 'test_img')\n\npredictor.predict_from_files(#[[join(imgs_dir, 'AUTOMI_00004_0000.nii.gz')],\n                             #[join(imgs_dir, 'AUTOMI_00005_0000.nii.gz')]],\n  imgs_dir,#, 'test_img_prova_v2'),\n                          #join(nnUNet_raw, 'Dataset003_AUTOMI_CTVLNF_NEWGL', 'predTs_v2'),\n                           #['provaOutputSingolo/AUTOMI_00004.nii.gz',\n                           #'provaOutputSingolo/AUTOMI_00005.nii.gz'],\n                           \"predTs_v3\",\n                          save_probabilities=False, overwrite=False,\n                          num_processes_preprocessing=3, num_processes_segmentation_export=3,\n                          folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:14:56.985620Z","iopub.execute_input":"2025-07-03T09:14:56.985981Z","iopub.status.idle":"2025-07-03T09:15:02.605563Z","shell.execute_reply.started":"2025-07-03T09:14:56.985958Z","shell.execute_reply":"2025-07-03T09:15:02.604664Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n","output_type":"stream"},{"name":"stdout","text":"There are 0 cases in the source folder\nI am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\nThere are 0 cases that I would like to predict\noverwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 0 cases.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# SHAP full organ, first test: \n## Produce a list of perturbations and apply them one by one to a single volume, and store predictions","metadata":{}},{"cell_type":"code","source":"\"\"\"\nProduce and save a *perturbation schedule* for organ-wise SHAP analysis.\n\nEach entry in the schedule is a lightweight dict:\n{\n    \"id\"          : 7,                       # running integer\n    \"organs_off\"  : [1, 4, 5],               # organ labels to zero-out\n    \"pert_type\"   : \"zero\",                  # could be \"blur\", \"noise\", …\n    \"seed\"        : 12345                    # for stochastic perturbations\n}\n\nNothing is pre-computed or stored in RAM apart from this list.\n\"\"\"\n\nfrom __future__ import annotations\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nimport json\nimport itertools\nimport numpy as np\nimport nibabel as nib\nfrom typing import List, Sequence, Dict, Union, Optional, Tuple\nimport hashlib\nimport random\n\n__all__ = [\"OrganMaskPerturbationPlanner\", \"PerturbationDescriptor\"]\n\n\n@dataclass(frozen=True)\nclass PerturbationDescriptor:\n    \"\"\"\n    An immutable description of *one* perturbation.\n    \"\"\"\n    id: int\n    organs_off: Tuple[int, ...]          # tuple so dataclass is hashable\n    pert_type: str = \"zero\"\n    seed: int = 0                        # default deterministic\n\n    # convenient JSON-serialisation\n    def to_json_dict(self) -> Dict:\n        return {\"id\": self.id,\n                \"organs_off\": list(self.organs_off),\n                \"pert_type\": self.pert_type,\n                \"seed\": self.seed}\n\n    @staticmethod\n    def from_json_dict(d: Dict) -> \"PerturbationDescriptor\":\n        return PerturbationDescriptor(id=int(d[\"id\"]),\n                                      organs_off=tuple(d[\"organs_off\"]),\n                                      pert_type=d[\"pert_type\"],\n                                      seed=int(d[\"seed\"]))\n\n\nclass OrganMaskPerturbationPlanner:\n    \"\"\"\n    Reads a volume + organ mask, enumerates organs present, and\n    *builds a perturbation plan* for KernelSHAP.\n\n    Notes\n    -----\n    * The *baseline* (all organs ON) is id==0 by convention.\n    * For classic KernelSHAP you usually need **all single-organ OFF**\n      plus a random sample of joint subsets – we implement that.\n    * The resulting JSON is < 100 kB even for hundreds of organs,\n      so it is safe to keep it entirely in RAM.\n    \"\"\"\n\n    #: version string – bump if JSON schema ever changes\n    schema_version = \"1.0\"\n\n    def __init__(self,\n                 volume_file: Union[str, Path],\n                 organ_mask_file: Union[str, Path],\n                 strategy: str = \"single-off+random\",     # default\n                 n_random: int = 0,                       # extra subsets\n                 perturbation_type: str = \"zero\",\n                 seed: int = 12345,\n                 out_json: Optional[Union[str, Path]] = None):\n\n        self.volume_file = Path(volume_file)\n        self.organ_mask_file = Path(organ_mask_file)\n        self.strategy = strategy\n        self.n_random = int(n_random)\n        self.perturbation_type = perturbation_type\n        self.seed = int(seed)\n        self._rng = random.Random(seed)\n\n        # ------------------------------------------------------------------\n        # 1) parse mask → list of organ labels\n        # ------------------------------------------------------------------\n        mask = nib.load(str(self.organ_mask_file)).get_fdata().astype(np.int32)\n        organ_ids = np.unique(mask)\n        organ_ids = organ_ids[organ_ids != 0]          # drop background\n        self.organs: Tuple[int, ...] = tuple(int(x) for x in organ_ids)\n\n        # ------------------------------------------------------------------\n        # 2) build schedule\n        # ------------------------------------------------------------------\n        self.schedule: List[PerturbationDescriptor] = self._build_schedule()\n\n        # ------------------------------------------------------------------\n        # 3) optionally dump to JSON\n        # ------------------------------------------------------------------\n        if out_json is not None:\n            self.save_json(out_json)\n\n    # ------------------------------------------------------------------\n    # public helpers\n    # ------------------------------------------------------------------\n    def save_json(self, path: Union[str, Path]) -> Path:\n        path = Path(path)\n        obj = {\"schema\": self.schema_version,\n               \"volume_file\": str(self.volume_file),\n               \"organ_mask_file\": str(self.organ_mask_file),\n               \"strategy\": self.strategy,\n               \"n_random\": self.n_random,\n               \"perturbation_type\": self.perturbation_type,\n               \"seed\": self.seed,\n               \"organs\": list(self.organs),\n               \"schedule\": [p.to_json_dict() for p in self.schedule]}\n        path.parent.mkdir(parents=True, exist_ok=True)\n        with path.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(obj, f, indent=2)\n        return path\n\n    @classmethod\n    def load_json(cls, path: Union[str, Path]) -> \"OrganMaskPerturbationPlanner\":\n        with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n            obj = json.load(f)\n        planner = cls(volume_file=obj[\"volume_file\"],\n                      organ_mask_file=obj[\"organ_mask_file\"],\n                      strategy=obj[\"strategy\"],\n                      n_random=obj[\"n_random\"],\n                      perturbation_type=obj[\"perturbation_type\"],\n                      seed=obj[\"seed\"])\n        # overwrite schedule that __init__ just built\n        planner.schedule = [PerturbationDescriptor.from_json_dict(d)\n                            for d in obj[\"schedule\"]]\n        return planner\n\n    # ------------------------------------------------------------------\n    # implementation details\n    # ------------------------------------------------------------------\n    def _build_schedule(self) -> List[PerturbationDescriptor]:\n        \"\"\"\n        Build a list of PerturbationDescriptor according to the chosen strategy.\n        *id==0* is always the **baseline** (no perturbation).\n        \"\"\"\n        sched: List[PerturbationDescriptor] = [\n            PerturbationDescriptor(id=0,\n                                   organs_off=tuple(),       # none\n                                   pert_type=\"identity\",\n                                   seed=self.seed)\n        ]\n\n        if self.strategy in {\"single-off\", \"single-off+random\"}:\n            # --- (a) single-organ OFF ---\n            for i, organ in enumerate(self.organs, start=1):\n                sched.append(PerturbationDescriptor(id=i,\n                                                    organs_off=(organ,),\n                                                    pert_type=self.perturbation_type,\n                                                    seed=self.seed))\n            next_id = len(sched)                                # keep running id\n\n            # --- (b) plus random K subsets if requested ---\n            if self.strategy.endswith(\"+random\") and self.n_random > 0:\n                power_set = list(itertools.chain.from_iterable(\n                    itertools.combinations(self.organs, r)\n                    for r in range(1, len(self.organs) + 1)))\n                self._rng.shuffle(power_set)\n\n                for subset in power_set[: self.n_random]:\n                    sched.append(PerturbationDescriptor(\n                        id=next_id,\n                        organs_off=tuple(sorted(subset)),\n                        pert_type=self.perturbation_type,\n                        seed=self._rng.randint(0, 2 ** 31 - 1)))\n                    next_id += 1\n        else:\n            raise ValueError(f\"Unknown strategy '{self.strategy}'\")\n\n        return sched\n\n    # ------------------------------------------------------------------\n    # magic methods\n    # ------------------------------------------------------------------\n    def __len__(self):\n        return len(self.schedule)\n\n    def __iter__(self):\n        return iter(self.schedule)\n\n    def __getitem__(self, idx: int) -> PerturbationDescriptor:\n        return self.schedule[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:02.607008Z","iopub.execute_input":"2025-07-03T09:15:02.607895Z","iopub.status.idle":"2025-07-03T09:15:02.631376Z","shell.execute_reply.started":"2025-07-03T09:15:02.607864Z","shell.execute_reply":"2025-07-03T09:15:02.630630Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\"\"\"\nSHAPPredictionIterator\n======================\n\nA memory-light wrapper that drives\n\n    1.   *Perturbation application*   (zeroing-out selected organs)\n    2.   *nnU-Net preprocessing*      (DefaultPreprocessor.run_case_npy)\n    3.   *nnU-Net inference*          (predict_sliding_window_return_logits)\n\nand yields (descriptor, prediction_logits).\n\nOnly one full-resolution volume (the **original**) plus one *preprocessed* patch\nsit in RAM/GPU at any time.\n\nThis iterator is **stateless** w.r.t. SHAP bookkeeping – that will be handled\nby the next module.\n\"\"\"\n\nfrom __future__ import annotations\nfrom pathlib import Path\nfrom typing import Iterator, Tuple, Optional, List\nimport itertools\nimport numpy as np\nimport nibabel as nib\nimport torch\n\nfrom nnunetv2.preprocessing.preprocessors.default_preprocessor import DefaultPreprocessor\n\n__all__ = [\"SHAPPredictionIterator\"]\n\n\nclass SHAPPredictionIterator:\n    \"\"\"\n    Parameters\n    ----------\n    planner\n        A previously built (or loaded) OrganMaskPerturbationPlanner.\n    predictor\n        An *already initialised* `nnUNetPredictor` with\n        `.plans_manager`, `.configuration_manager`, `.device`, `.network`, etc.\n        The iterator never changes those attributes.\n    skip_completed_ids\n        Optional set/list of integer perturbation ids that should be skipped\n        (for resume functionality).\n    cache_sw_inference\n        Cache the sliding window inference patches, running only the perturbed ones\n    \"\"\"\n\n    def __init__(self,\n                 planner: OrganMaskPerturbationPlanner,\n                 predictor,\n                 skip_completed_ids: Optional[List[int]] = None,\n                 cache_sw_inference: bool = True,\n                 pre_cached_output: dict | None = None,\n                 verbose: bool = False):\n\n        self.planner = planner\n        self.predictor = predictor\n        self.verbose = verbose\n        self.skip_completed_ids = set(skip_completed_ids or [])\n        self.cache_sw_inference = cache_sw_inference\n\n        # ------------------------------------------------------------------\n        # 1) Read & cache original image + mask only ONCE.\n        #    They are kept in CPU RAM for the whole iterator lifetime.\n        # ------------------------------------------------------------------\n        # 1) Read *both* image and organ-mask via the same reader  ✅\n        # ----------------------------------------------------------------------\n        rw = self.predictor.plans_manager.image_reader_writer_class()\n        # If nnU-Net returns a class instead of an instance, instantiate it\n        if callable(rw) and not hasattr(rw, \"read_images\"):\n            rw = rw()\n        \n        # ---- image -----------------------------------------------------------\n        self._orig_image, self._orig_props = rw.read_images(\n            [str(planner.volume_file)]\n        )                     # (C, Z, Y, X)\n        \n        # ---- organ mask ------------------------------------------------------\n        seg_arr, _ = rw.read_seg(str(planner.organ_mask_file))  # shape (1, Z, Y, X)\n        self._organ_mask = seg_arr[0].astype(np.int32, copy=False)  # drop channel axis\n\n        # convenience\n        self._preprocessor = self.predictor.configuration_manager.preprocessor_class(\n            verbose=self.verbose\n        )\n\n        # ----- baseline output dictionary -------------------------------------\n        if self.cache_sw_inference:\n            if pre_cached_output == None:\n                self._baseline_output_dictionary = self.get_orig_image_output_dictionary()\n            else:\n                if self.verbose:\n                    print(\"pre-cached output dictionary found\")\n                self._baseline_output_dictionary = pre_cached_output\n\n    # ------------------------------------------------------------------ #\n    # Python iterator protocol                                            #\n    # ------------------------------------------------------------------ #\n    def __iter__(self) -> Iterator[Tuple[PerturbationDescriptor, np.ndarray]]:\n        \"\"\"\n        Yields\n        ------\n        (descriptor, logits)  where\n            descriptor : PerturbationDescriptor\n            logits     : np.ndarray  shape (C, X, Y, Z) on **CPU**\n        \"\"\"\n        for desc in self.planner.schedule:\n            if desc.id in self.skip_completed_ids:\n                if self.verbose:\n                    print(f\"[SHAP] skipping id={desc.id} (already done)\")\n                continue\n\n            # --------------------------------------------------------------\n            # a) apply perturbation to a *copy* of the original volume\n            # --------------------------------------------------------------\n            data_pert = self._apply_perturbation(desc)\n\n            # --------------------------------------------------------------\n            # b) preprocess → network input tensor\n            # --------------------------------------------------------------\n            data_pp, _, _ = self._preprocessor.run_case_npy(\n                data_pert,\n                seg=None,\n                properties=self._orig_props,\n                plans_manager=self.predictor.plans_manager,\n                configuration_manager=self.predictor.configuration_manager,\n                dataset_json=self.predictor.dataset_json\n            )\n            # to torch, channel-first is already true\n            inp_tensor = torch.from_numpy(data_pp)\n\n            # -------------------------------------------------------------\n            # c) get perturbation mask optionally for caching utility\n            # -------------------------------------------------------------\n            if self.cache_sw_inference:\n                pert_mask = self._get_perturbation_mask(desc)\n\n            # --------------------------------------------------------------\n            # d) sliding-window prediction\n            # --------------------------------------------------------------\n            if self.cache_sw_inference:\n                logits = self.predictor.predict_sliding_window_return_logits_with_caching(\n                    inp_tensor,\n                    pert_mask,\n                    self._baseline_output_dictionary\n                ).cpu().numpy()          # low-level array, channel-first\n            else:\n                logits = self.predictor.predict_sliding_window_return_logits(\n                    inp_tensor\n                ).cpu().numpy()         # low-level array, channel-first\n\n            # free GPU cache ASAP\n            torch.cuda.empty_cache()\n\n            # yield results to caller\n            yield desc, logits\n\n    def __len__(self):\n        return len(self.planner) - len(self.skip_completed_ids)\n\n    # -------------------------------------------------------1Introduction----------- #\n    # internal helpers                                                   #\n    # ------------------------------------------------------------------ #\n    def _apply_perturbation(self, desc: PerturbationDescriptor) -> np.ndarray:\n        \"\"\"\n        Return a **new** NumPy array `data_pert` with the specified organs\n        perturbed (currently only 'zero' implemented).\n        \"\"\"\n        if desc.pert_type not in {\"identity\", \"zero\"}:\n            raise NotImplementedError(f\"Perturbation '{desc.pert_type}' not yet implemented\")\n\n        # Shallow copy when no perturbation needed (baseline) –\n        # avoids wasting RAM while staying side-effect-free.\n        if not desc.organs_off:\n            return self._orig_image\n\n        # Materialise a *copy* because we will mutate values\n        data = self._orig_image.copy()\n\n        # Build a Boolean mask once; broadcast over channels\n        # mask shape (X,Y,Z) – True where voxel belongs to any organ in organs_off\n        mask = np.isin(self._organ_mask, desc.organs_off)\n\n        if desc.pert_type == \"zero\":\n            # data shape (C, X, Y, Z)\n            data[:, mask] = 0\n\n        return data\n\n    def _get_perturbation_mask(\n                                self,\n                                desc: PerturbationDescriptor,\n                                num_channels: int = 1\n                            ) -> torch.BoolTensor:\n        \"\"\"\n        Ritorna una maschera (C, X, Y, Z) booleana, True sui voxel degli organi da azzerare.\n        \"\"\"\n        # --- costruisci mask (Z, Y, X) su CPU ------------------------------\n        mask_zyx = np.isin(self._organ_mask, desc.organs_off)          # (Z,Y,X) bool\n        mask_zyx = torch.from_numpy(mask_zyx)        # torch.bool\n    \n        # --- porta in ordine (X, Y, Z) -------------------------------------\n        mask_xyz = mask_zyx.permute(2, 1, 0).contiguous()              # (X,Y,Z)\n    \n        # --- replica sui canali -------------------------------------------\n        mask_cxyz = mask_xyz.unsqueeze(0)                              # (1,X,Y,Z)\n        mask_cxyz = mask_cxyz.expand(num_channels, -1, -1, -1)         # (C,X,Y,Z)\n    \n        return mask_cxyz\n\n    def get_orig_image_output_dictionary(self) -> dict:\n        \"\"\"\n        Return a dictionary indexed by the slices for the sliding window, of the output of the inference for each patch\n        of the original volume\n        \"\"\"\n        if not self.cache_sw_inference:\n            raise Exception(\"get_orig_image_output_dictionary non è compatibile con cache_sw_inference = False\")\n            \n        data_pp, _, _ = self._preprocessor.run_case_npy(\n                self._orig_image,\n                seg=None,\n                properties=self._orig_props,\n                plans_manager=self.predictor.plans_manager,\n                configuration_manager=self.predictor.configuration_manager,\n                dataset_json=self.predictor.dataset_json\n            )\n        # to torch, channel-first is already true\n        inp_tensor = torch.from_numpy(data_pp)\n        if self.verbose:\n            print(\"tensor shape before _internal_get_sliding_window_slicers called in Iterator: \", inp_tensor.shape)\n        \n        slicers = self.predictor._internal_get_sliding_window_slicers(inp_tensor.shape[1:])\n        if self.verbose:\n            print(\"first 3 slicers of Iterator object: \", slicers[:3])\n\n        dictionary = self.predictor.get_output_dictionary_sliding_window(inp_tensor, slicers)\n\n        return dictionary\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:02.632557Z","iopub.execute_input":"2025-07-03T09:15:02.632860Z","iopub.status.idle":"2025-07-03T09:15:02.652594Z","shell.execute_reply.started":"2025-07-03T09:15:02.632835Z","shell.execute_reply":"2025-07-03T09:15:02.651777Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\"\"\"\nSHAPAccumulator\n===============\n\nResponsibilities\n----------------\n1.  Accept (descriptor, logits) tuples from `SHAPPredictionIterator`.\n2.  Write the logits to a compressed .npz file (one per perturbation).\n3.  Append a tiny JSON-Lines record so we can resume reliably.\n4.  Expose `.completed_ids` so the iterator can skip work that is done.\n5.  Offer `finalize()` which – for now – just returns file paths, but gives\n    you one place to plug in an actual KernelSHAP post-processor later.\n\"\"\"\n\nfrom __future__ import annotations\nfrom pathlib import Path\nimport json\nfrom typing import Dict, List, Set, Tuple, Optional\nimport numpy as np\n\n__all__ = [\"SHAPAccumulator\"]\n\n\nclass SHAPAccumulator:\n    \"\"\"\n    Parameters\n    ----------\n    planner\n        The same planner you pass to the iterator – purely for metadata.\n    out_dir\n        A directory that will contain two things:\n\n        1.  `pred_00012.npz`     (compressed logits for perturbation id 12)\n        2.  `progress.jsonl`     (one JSON record per *finished* perturbation)\n\n    mode            \"r\" (read-only) or \"a\" (append / create)\n    \"\"\"\n\n    PROGRESS_FILE = \"progress.jsonl\"          # newline-delimited JSON\n\n    def __init__(self,\n                 planner: OrganMaskPerturbationPlanner,\n                 out_dir: str | Path,\n                 mode: str = \"a\",\n                 npz_compression_kwargs: Optional[Dict] = None):\n\n        self.planner = planner\n        self.out_dir = Path(out_dir)\n        self.out_dir.mkdir(parents=True, exist_ok=True)\n        self.progress_path = self.out_dir / self.PROGRESS_FILE\n\n        self._compression_kwargs = npz_compression_kwargs or {}\n        self._mode = mode\n\n        # ------------------------------------------------------------------\n        # 1) read existing progress (if any) → set of completed ids\n        # ------------------------------------------------------------------\n        self.completed_ids: Set[int] = set()\n        self._progress_fp = None\n\n        if self.progress_path.exists():\n            with self.progress_path.open(\"r\", encoding=\"utf-8\") as f:\n                for ln in f:\n                    rec = json.loads(ln.strip())\n                    self.completed_ids.add(int(rec[\"id\"]))\n\n        if mode == \"a\":\n            # open file in append mode so every .update() is flushed immediately\n            self._progress_fp = self.progress_path.open(\"a\", encoding=\"utf-8\")\n        elif mode == \"r\":\n            # read-only – no writing allowed\n            self._progress_fp = None\n        else:\n            raise ValueError(\"mode must be 'a' or 'r'\")\n\n    # ------------------------------------------------------------------ #\n    # public API                                                          #\n    # ------------------------------------------------------------------ #\n    def update(self,\n               desc: PerturbationDescriptor,\n               logits: np.ndarray) -> Path:\n        \"\"\"\n        Persist *one* prediction and mark it as completed.\n\n        Returns\n        -------\n        Path to the .npz file that was written.\n        \"\"\"\n        if self._mode != \"a\":\n            raise RuntimeError(\"Accumulator opened read-only\")\n\n        if desc.id in self.completed_ids:\n            # defensive – shouldn't happen if iterator skips\n            return self._logit_path(desc.id)\n\n        # 1) save logits on disk – compressed, float16 by default\n        npz_path = self._logit_path(desc.id)\n        np.savez_compressed(npz_path, logits=logits, **self._compression_kwargs)\n\n        # 2) append a *tiny* progress record\n        rec = {\n            \"id\":        int(desc.id),\n            \"organs_off\": list(desc.organs_off),\n            \"pert_type\": desc.pert_type,\n            \"logit_file\": str(npz_path.name)\n        }\n        self._progress_fp.write(json.dumps(rec) + \"\\n\")\n        self._progress_fp.flush()\n\n        # 3) keep in RAM only the id\n        self.completed_ids.add(int(desc.id))\n\n        return npz_path\n\n    def finalize(self):\n        \"\"\"\n        Placeholder for KernelSHAP post-processing.\n\n        At present it simply returns a *list of disk files* in the order\n        of `planner.schedule`.  Replace this implementation with whatever\n        SHAP fitting you need (e.g. weighted linear regression).\n\n        Notes\n        -----\n        * Because all logits are on disk, the memory footprint is tiny: you\n          load one .npz at a time if you want to aggregate voxel-wise stats.\n        * You may also decide to compute SHAP values per-organ *metric*\n          (Dice, volume, etc.) instead of raw voxel map – totally up to you.\n        \"\"\"\n        files_in_order = [\n            self._logit_path(desc.id)\n            for desc in self.planner.schedule\n            if desc.id in self.completed_ids\n        ]\n        return files_in_order\n\n    def close(self):\n        if self._progress_fp is not None:\n            self._progress_fp.close()\n\n    # ------------------------------------------------------------------ #\n    # helpers                                                            #\n    # ------------------------------------------------------------------ #\n    def _logit_path(self, perturbation_id: int) -> Path:\n        return self.out_dir / f\"pred_{perturbation_id:05d}.npz\"\n\n    # make the object a context manager for convenience\n    def __enter__(self):   return self\n    def __exit__(self, exc_type, exc_val, exc_tb): self.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:02.653479Z","iopub.execute_input":"2025-07-03T09:15:02.653685Z","iopub.status.idle":"2025-07-03T09:15:02.666630Z","shell.execute_reply.started":"2025-07-03T09:15:02.653669Z","shell.execute_reply":"2025-07-03T09:15:02.665963Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"plan = OrganMaskPerturbationPlanner(\n    volume_file=join(imgs_dir, 'AUTOMI_00039_0000.nii.gz'),\n    organ_mask_file=join(nnUNet_raw, \"total_segmentator_structures\", \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\"),\n    strategy=\"single-off+random\",\n    n_random=1,                 # n_random extra random subsets, added to singletons\n    perturbation_type=\"zero\",\n    out_json=\"SHAP/output_plan.json\"\n)\n\nprint(f\"Schedule contains {len(plan)} perturbations \"\n      f\"(baseline + {len(plan)-1} variants)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:02.667337Z","iopub.execute_input":"2025-07-03T09:15:02.667561Z","iopub.status.idle":"2025-07-03T09:15:07.133532Z","shell.execute_reply.started":"2025-07-03T09:15:02.667544Z","shell.execute_reply":"2025-07-03T09:15:07.132670Z"}},"outputs":[{"name":"stdout","text":"Schedule contains 11 perturbations (baseline + 10 variants)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\nfrom nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n\n# 1) Load plan -----------------------------------------------------------\nplan = OrganMaskPerturbationPlanner.load_json(\"SHAP/output_plan.json\")\n\n# 2) Initialise predictor (standard nnU-Net code) ------------------------\npredictor = nnUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=False, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=False,\n    verbose_preprocessing=False,\n    allow_tqdm=True\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir_readonly,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)\n\n\n# 3) Run iterator + accumulator -----------------------------------------\nwith SHAPAccumulator(plan, \"SHAP/shap_run\", mode=\"a\") as acc:\n    for desc, logits in SHAPPredictionIterator(plan, predictor,\n                                               skip_completed_ids=acc.completed_ids,\n                                               cache_sw_inference=False,\n                                               verbose=True):\n        acc.update(desc, logits)\n\n# 4) After all perturbations are done ------------------------------------\npaths = acc.finalize()   # list of .npz files on disk (baseline first)\nprint(\"All predictions saved, ready for KernelSHAP fitting.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:07.134653Z","iopub.execute_input":"2025-07-03T09:15:07.135191Z","iopub.status.idle":"2025-07-03T09:15:13.747550Z","shell.execute_reply.started":"2025-07-03T09:15:07.135159Z","shell.execute_reply":"2025-07-03T09:15:13.746799Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n","output_type":"stream"},{"name":"stdout","text":"[SHAP] skipping id=0 (already done)\n[SHAP] skipping id=1 (already done)\n[SHAP] skipping id=2 (already done)\n[SHAP] skipping id=3 (already done)\n[SHAP] skipping id=4 (already done)\n[SHAP] skipping id=5 (already done)\n[SHAP] skipping id=6 (already done)\n[SHAP] skipping id=7 (already done)\n[SHAP] skipping id=8 (already done)\n[SHAP] skipping id=9 (already done)\n[SHAP] skipping id=10 (already done)\nAll predictions saved, ready for KernelSHAP fitting.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Eventually export some predicted volume if needed to show","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nfrom pathlib import Path\nfrom nnunetv2.configuration import default_num_processes\nfrom nnunetv2.inference.export_prediction import export_prediction_from_logits\n\ndef export_logits_to_nifty_segmentation(\n    predictor,\n    volume_file: Path,\n    model_dir_readonly: str,\n    logits: Union[str, np.ndarray, torch.Tensor],\n    npz_dir: str | None,\n    output_dir: str = \"\",\n    fold: int = 0,\n    save_probs: bool = False,\n    from_file: bool = True\n):\n    \"\"\"\n    Converts a saved .npz logits file into a native-space NIfTI segmentation using nnU-Net's helper.\n\n    Args:\n        predictor: An instantiated nnU-Net predictor object with loaded plans/configs.\n        volume_file: An Path object pointing to the raw image file.\n        model_dir_readonly (str): Path to the nnU-Net mode1Introductionl directory containing dataset.json.\n        logits (str or tensor): Base name of the .npz logits file (no extension), when from_file=True\n        npz_dir (str): Directory where the .npz file is stored.\n        output_dir (str): Directory where the .nii.gz segmentation will be saved.\n        fold (int): The fold number used for prediction (default is 0).\n        save_probs (bool): Whether to save softmax probabilities as a .npz file.\n        from_file (bool). Whether to convert from a file instead of from the logits (default true)\n    \"\"\"\n    if from_file:\n        npz_logits = Path(npz_dir) / f\"{logits}.npz\"\n        output_nii = Path(output_dir) / f\"{logits}_seg.nii.gz\"\n        logits = np.load(npz_logits)[\"logits\"]\n    else:\n        output_nii = Path(output_dir) / \"exported_seg.nii.gz\"\n\n    plans_manager = predictor.plans_manager\n    configuration_manager = predictor.configuration_manager\n    dataset_json = Path(model_dir_readonly) / \"dataset.json\"\n\n    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n    rw = plans_manager.image_reader_writer_class()\n    if callable(rw) and not hasattr(rw, \"read_images\"):\n        rw = rw()\n    img_np, img_props = rw.read_images([str(plan.volume_file)])\n\n    _, _, data_props = preprocessor.run_case_npy(\n        img_np, seg=None, properties=img_props,\n        plans_manager=plans_manager,\n        configuration_manager=configuration_manager,\n        dataset_json=dataset_json\n    )\n\n\n    export_prediction_from_logits(\n        predicted_array_or_file=logits,\n        properties_dict=data_props,\n        configuration_manager=configuration_manager,\n        plans_manager=plans_manager,\n        dataset_json_dict_or_file=str(dataset_json),\n        output_file_truncated=os.path.splitext(str(output_nii))[0],\n        save_probabilities=save_probs,\n        num_threads_torch=default_num_processes\n    )\n\n    print(f\"✅  NIfTI segmentation written → {output_nii}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:13.748427Z","iopub.execute_input":"2025-07-03T09:15:13.748634Z","iopub.status.idle":"2025-07-03T09:15:13.756867Z","shell.execute_reply.started":"2025-07-03T09:15:13.748608Z","shell.execute_reply":"2025-07-03T09:15:13.756044Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\"\"\"export_logits_to_nifty_segmentation(\n    predictor=predictor,\n    plan=plan,\n    model_dir_readonly=Path(model_dir_readonly),\n    logits_filename=\"pred_00007\",\n    npz_dir=\"SHAP/shap_run\",\n    output_dir=\"SHAP/shap_run\",\n    fold=0,\n    save_probs=False\n)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:13.757636Z","iopub.execute_input":"2025-07-03T09:15:13.757889Z","iopub.status.idle":"2025-07-03T09:15:13.773788Z","shell.execute_reply.started":"2025-07-03T09:15:13.757872Z","shell.execute_reply":"2025-07-03T09:15:13.772861Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'export_logits_to_nifty_segmentation(\\n    predictor=predictor,\\n    plan=plan,\\n    model_dir_readonly=Path(model_dir_readonly),\\n    logits_filename=\"pred_00007\",\\n    npz_dir=\"SHAP/shap_run\",\\n    output_dir=\"SHAP/shap_run\",\\n    fold=0,\\n    save_probs=False\\n)'"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"## We start testing sliding window caching for faster multi-inference scenario, like SHAP","metadata":{}},{"cell_type":"markdown","source":"### Try to override the sliding_window_function","metadata":{}},{"cell_type":"code","source":"from typing import Union\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom queue import Queue\nfrom threading import Thread\nfrom acvl_utils.cropping_and_padding.padding import pad_nd_image\nfrom nnunetv2.utilities.helpers import empty_cache, dummy_context\nfrom nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\nfrom nnunetv2.inference.sliding_window_prediction import compute_gaussian, compute_steps_for_sliding_window\n\nclass CustomNNUNetPredictor(nnUNetPredictor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n    \n    @torch.inference_mode()\n    def predict_sliding_window_return_logits_with_caching(self, input_image: torch.Tensor,\n                                                          perturbation_mask: torch.BoolTensor,\n                                                          baseline_prediction_dict: dict) \\\n            -> Union[np.ndarray, torch.Tensor]:\n        \"\"\"\n        Method predict_sliding_window_return_logits taken from official nnunetv2 documentation:\n        https://github.com/MIC-DKFZ/nnUNet/blob/58a3b121a6d1846a978306f6c79a7c005b7d669b/nnunetv2/inference/predict_from_raw_data.py\n        We add a perturbation_mask parameter to check each patch for the actual presence of a perturbation\n        \"\"\"\n        assert isinstance(input_image, torch.Tensor)\n        self.network = self.network.to(self.device)\n        self.network.eval()\n\n        empty_cache(self.device)\n\n        # DEBUG --------------\n        \"\"\"voxels  = np.prod(input_image.shape[1:])          # (X*Y*Z)\n        bytes_per_voxel = 2                        # fp16\n        needed  = voxels * self.label_manager.num_segmentation_heads * bytes_per_voxel\n        print(f\"≈{needed/1e9:.1f} GB per predicted_logits\")\"\"\"\n\n        # Autocast can be annoying\n        # If the device_type is 'cpu' then it's slow as heck on some CPUs (no auto bfloat16 support detection)\n        # and needs to be disabled.\n        # If the device_type is 'mps' then it will complain that mps is not implemented, even if enabled=False\n        # is set. Whyyyyyyy. (this is why we don't make use of enabled=False)\n        # So autocast will only be active if we have a cuda device.\n        with torch.autocast(self.device.type, enabled=True) if self.device.type == 'cuda' else dummy_context():\n            assert input_image.ndim == 4, 'input_image must be a 4D np.ndarray or torch.Tensor (c, x, y, z)'\n\n            if self.verbose:\n                print(f'Input shape: {input_image.shape}')\n                print(\"step_size:\", self.tile_step_size)\n                print(\"mirror_axes:\", self.allowed_mirroring_axes if self.use_mirroring else None)\n                print(f'Perturbation mask shape: {perturbation_mask.shape}')\n\n\n            # if input_image is smaller than tile_size we need to pad it to tile_size.\n            data, slicer_revert_padding = pad_nd_image(input_image, self.configuration_manager.patch_size,\n                                                       'constant', {'value': 0}, True,\n                                                       None)\n\n            # slicers can be applied to both perturbed volume and \n            slicers = self._internal_get_sliding_window_slicers(data.shape[1:])\n\n            if self.perform_everything_on_device and self.device != 'cpu':\n                # behavior changed\n                try:\n                    predicted_logits = self._internal_predict_sliding_window_return_logits(\n                        data, slicers, True, perturbation_mask, baseline_prediction_dict, caching=True\n                    )\n                except RuntimeError as e:\n                    if \"CUDA out of memory\" in str(e):\n                        print(\"⚠️  CUDA OOM, cambiare batch size o patch size!\")\n                        raise\n                    else:\n                        # Mostra l'errore reale e aborta: niente CPU fallback\n                        raise\n            else:\n                predicted_logits = self._internal_predict_sliding_window_return_logits(data, slicers,\n                                                                                       self.perform_everything_on_device)\n\n            empty_cache(self.device)\n            # revert padding\n            predicted_logits = predicted_logits[(slice(None), *slicer_revert_padding[1:])]\n        return predicted_logits\n                \n\n    def _slice_key(self, slicer_tuple):\n        # make slicer object hashable to use it for cache lookup\n        return tuple((s.start, s.stop, s.step) for s in slicer_tuple)\n\n\n    @torch.inference_mode()\n    def _internal_predict_sliding_window_return_logits(self,\n                                                       data: torch.Tensor,\n                                                       slicers,\n                                                       do_on_device: bool = True,\n                                                       perturbation_mask: torch.BoolTensor | None = None,\n                                                       baseline_prediction_dict: dict | None = None,\n                                                       caching: bool = False,\n                                                       ):\n        \"\"\"\n        Modified to manage the caching of patches\n        \"\"\"\n        predicted_logits = n_predictions = prediction = gaussian = workon = None\n        results_device = self.device if do_on_device else torch.device('cpu')\n        if next(self.network.parameters()).device != results_device:\n            self.network = self.network.to(results_device)\n\n        def producer(d, slh, q):\n            for s in slh:\n                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(results_device), s))\n            q.put('end')\n\n        try:\n            empty_cache(self.device)\n\n            # move data to device\n            if self.verbose:\n                print(f'move image to device {results_device}')\n            data = data.to(results_device)\n            queue = Queue(maxsize=2)\n            t = Thread(target=producer, args=(data, slicers, queue))\n            t.start()\n\n            # preallocate arrays\n            if self.verbose:\n                print(f'preallocating results arrays on device {results_device}')\n            predicted_logits = torch.zeros((self.label_manager.num_segmentation_heads, *data.shape[1:]),\n                                           dtype=torch.half,\n                                           device=results_device)\n            n_predictions = torch.zeros(data.shape[1:], dtype=torch.half, device=results_device)\n\n            if self.use_gaussian:\n                gaussian = compute_gaussian(tuple(self.configuration_manager.patch_size), sigma_scale=1. / 8,\n                                            value_scaling_factor=10,\n                                            device=results_device)\n            else:\n                gaussian = 1\n\n        \n\n            if not self.allow_tqdm and self.verbose:\n                print(f'running prediction: {len(slicers)} steps')\n\n            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n                cache_hits = 0\n                while True:\n                    item = queue.get()\n                    if item == 'end':\n                        queue.task_done()\n                        break\n                    workon, sl = item\n                    try:\n                        if caching and not self.check_overlapping(sl, perturbation_mask):\n                            prediction = baseline_prediction_dict[self._slice_key(sl)].to(results_device)\n                            cache_hits += 1\n                        else:\n                            prediction = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n                    except Exception as e:\n                        raise RuntimeError(\"Errore nella predizione del patch\") from e\n\n                    # 2) sanity-check device\n                    assert prediction.device == predicted_logits.device\n\n                    if self.use_gaussian:\n                        prediction *= gaussian\n                    predicted_logits[sl] += prediction\n                    n_predictions[sl[1:]] += gaussian\n\n                    # free up gpu memory\n                    del prediction, workon\n                    \n                    queue.task_done()\n                    pbar.set_postfix(\n                        cache=f\"{cache_hits}\",\n                        mem=f\"{torch.cuda.memory_allocated()/1e9:.2f} GB\"\n                    )\n                    pbar.update(1)\n            queue.join()\n            if self.verbose and not self.allow_tqdm:\n                print(f\"Cache hits: {cache_hits}\\\\{len(slicers)}\")\n            \n\n            # predicted_logits /= n_predictions\n            torch.div(predicted_logits, n_predictions, out=predicted_logits)\n            # check for infs\n            if torch.any(torch.isinf(predicted_logits)):\n                raise RuntimeError('Encountered inf in predicted array. Aborting... If this problem persists, '\n                                   'reduce value_scaling_factor in compute_gaussian or increase the dtype of '\n                                   'predicted_logits to fp32')\n        except Exception as e:\n            del predicted_logits, n_predictions, prediction, gaussian, workon\n            empty_cache(self.device)\n            empty_cache(results_device)\n            raise e\n        return predicted_logits\n  \n\n\n    def get_output_dictionary_sliding_window(self, data: torch.Tensor, slicers,\n                                            do_on_device: bool = True,\n                                            ) -> torch.Tensor:\n        \"\"\"\n        # create a dictionary that associates the output of the inference, to each slicer of the sliding window module\n        # this way we can set ready for cache the output for the untouched patches.\n        \"\"\"\n        \n        dictionary = dict()\n        prediction = workon = None\n        results_device = self.device if do_on_device else torch.device('cpu')\n        if next(self.network.parameters()).device != results_device:\n            self.network = self.network.to(results_device)\n\n        def producer(d, slh, q):\n            for s in slh:\n                #tqdm.write(f\"put patch {s} on queue\")    # dentro producer\n                q.put((torch.clone(d[s][None], memory_format=torch.contiguous_format).to(self.device), s))\n            q.put('end')\n\n        try:\n            empty_cache(self.device)\n\n            # move data and network to device\n            if self.verbose:\n                print(f'move image and model to device {results_device}')\n\n            self.network = self.network.to(results_device)\n            data = data.to(results_device)\n            queue = Queue(maxsize=2)\n            t = Thread(target=producer, args=(data, slicers, queue))\n            t.start()\n\n            if not self.allow_tqdm and self.verbose:\n                print(f'running prediction: {len(slicers)} steps')\n\n            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n                while True:\n                    item = queue.get()\n                    if item == 'end':\n                        queue.task_done()\n                        break\n                    workon, sl = item\n                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n\n                    pred_cpu = pred_gpu.cpu()\n                    # save prediction in the dictionary\n                    dictionary[self._slice_key(sl)] = pred_cpu\n                    # immediately free gpu memory\n                    del pred_gpu\n                    \n                    queue.task_done()\n                    pbar.update()\n            queue.join()\n\n        except Exception as e:\n            del workon#, prediction\n            empty_cache(self.device)\n            empty_cache(results_device)\n            raise e\n        return dictionary\n\n        try:\n            empty_cache(self.device)\n\n            # move data and network to device\n            if self.verbose:\n                print(f'move image and model to device {results_device}')\n\n            self.network = self.network.to(results_device)\n            data = data.to(results_device)\n            queue = Queue(maxsize=2)\n            t = Thread(target=producer, args=(data, slicers, queue))\n            t.start()\n\n            if not self.allow_tqdm and self.verbose:\n                print(f'running prediction: {len(slicers)} steps')\n\n            with tqdm(desc=None, total=len(slicers), disable=not self.allow_tqdm) as pbar:\n                while True:\n                    item = queue.get()\n                    if item == 'end':\n                        queue.task_done()\n                        break\n                    workon, sl = item\n                    pred_gpu = self._internal_maybe_mirror_and_predict(workon)[0].to(results_device)\n\n                    pred_cpu = pred_gpu.cpu()\n                    # save prediction in the dictionary\n                    dictionary[self._slice_key(sl)] = pred_cpu\n                    # immediately free gpu memory\n                    del pred_gpu\n                    \n                    queue.task_done()\n                    pbar.update()\n            queue.join()\n\n        except Exception as e:\n            del workon#, prediction\n            empty_cache(self.device)\n            empty_cache(results_device)\n            raise e\n        return dictionary\n\n    def check_overlapping(self, slicer, perturbation_mask: torch.BoolTensor) -> bool:\n        \"\"\"\n        Restituisce True se la patch definita da `slicer`\n        contiene almeno un voxel perturbato.\n    \n        Parameters\n        ----------\n        slicer : tuple\n            Quello prodotto da `_internal_get_sliding_window_slicers`,\n            cioè (slice(None), slice(x0,x1), slice(y0,y1), slice(z0,z1)).\n        perturbation_mask : torch.BoolTensor\n            Maschera (C, X, Y, Z) con True nei voxel da perturbare\n            (di solito C==1 o replicata sui canali).\n    \n        Returns\n        -------\n        bool\n            True ↔ almeno un voxel True nella patch.\n        \"\"\"\n        # NB: il primo elemento del tuple è sempre slice(None) (canali).\n        #     Lo manteniamo: non ha overhead e semplifica.\n        return perturbation_mask[slicer].any().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:13.774863Z","iopub.execute_input":"2025-07-03T09:15:13.775133Z","iopub.status.idle":"2025-07-03T09:15:13.804799Z","shell.execute_reply.started":"2025-07-03T09:15:13.775107Z","shell.execute_reply":"2025-07-03T09:15:13.803978Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Create plan ------------------------------------------------------------\nplan = OrganMaskPerturbationPlanner(\n    volume_file=join(imgs_dir, 'AUTOMI_00039_0000.nii.gz'),\n    organ_mask_file=join(nnUNet_raw, \"total_segmentator_structures\", \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\"),\n    strategy=\"single-off+random\",\n    n_random=1,                 # n_random extra random subsets, added to singletons\n    perturbation_type=\"zero\",\n    out_json=\"SHAP-test/output_plan.json\"\n)\n\nprint(f\"Schedule contains {len(plan)} perturbations \"\n      f\"(baseline + {len(plan)-1} variants)\")\n\n# 1) Load plan -----------------------------------------------------------\nplan = OrganMaskPerturbationPlanner.load_json(\"SHAP-test/output_plan.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:13.805784Z","iopub.execute_input":"2025-07-03T09:15:13.806074Z","iopub.status.idle":"2025-07-03T09:15:22.915935Z","shell.execute_reply.started":"2025-07-03T09:15:13.806049Z","shell.execute_reply":"2025-07-03T09:15:22.915094Z"}},"outputs":[{"name":"stdout","text":"Schedule contains 11 perturbations (baseline + 10 variants)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"\nfrom nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\ndel predictor.network\n# 2) Initialise predictor ------------------------\npredictor = CustomNNUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=False, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=True,\n    verbose_preprocessing=False,\n    allow_tqdm=True\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir_readonly,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)\n\"\"\"\ntorch.cuda.empty_cache()\n\niterator = SHAPPredictionIterator(plan, predictor,\n                                               skip_completed_ids=acc.completed_ids,\n                                               cache_sw_inference=True,\n                                               verbose=True)\npre_cached = iterator.get_orig_image_output_dictionary()\n\n\n\n\n# 3) Run iterator + accumulator -----------------------------------------\nwith SHAPAccumulator(plan, \"SHAP-test/shap_run\", mode=\"a\") as acc:\n    for desc, logits in SHAPPredictionIterator(plan, predictor,\n                                               skip_completed_ids=acc.completed_ids,\n                                               cache_sw_inference = True,\n                                               #pre_cached_output = pre_cached,\n                                               verbose=False):\n        acc.update(desc, logits)\n\n# 4) After all perturbations are done ------------------------------------\npaths = acc.finalize()   # list of .npz files on disk (baseline first)\nprint(\"All predictions saved, ready for KernelSHAP fitting.\")\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:22.916909Z","iopub.execute_input":"2025-07-03T09:15:22.917186Z","iopub.status.idle":"2025-07-03T09:15:23.925872Z","shell.execute_reply.started":"2025-07-03T09:15:22.917162Z","shell.execute_reply":"2025-07-03T09:15:23.925041Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'\\ntorch.cuda.empty_cache()\\n\\niterator = SHAPPredictionIterator(plan, predictor,\\n                                               skip_completed_ids=acc.completed_ids,\\n                                               cache_sw_inference=True,\\n                                               verbose=True)\\npre_cached = iterator.get_orig_image_output_dictionary()\\n\\n\\n\\n\\n# 3) Run iterator + accumulator -----------------------------------------\\nwith SHAPAccumulator(plan, \"SHAP-test/shap_run\", mode=\"a\") as acc:\\n    for desc, logits in SHAPPredictionIterator(plan, predictor,\\n                                               skip_completed_ids=acc.completed_ids,\\n                                               cache_sw_inference = True,\\n                                               #pre_cached_output = pre_cached,\\n                                               verbose=False):\\n        acc.update(desc, logits)\\n\\n# 4) After all perturbations are done ------------------------------------\\npaths = acc.finalize()   # list of .npz files on disk (baseline first)\\nprint(\"All predictions saved, ready for KernelSHAP fitting.\")'"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"\"\"\"export_logits_to_nifty_segmentation(\n    predictor=predictor,\n    plan=plan,\n    model_dir_readonly=Path(model_dir_readonly),\n    logits_filename=\"pred_00006\",\n    npz_dir=\"SHAP-test/shap_run\",\n    output_dir=\"SHAP-test/shap_run\",\n    fold=0,\n    save_probs=False\n)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:23.930720Z","iopub.execute_input":"2025-07-03T09:15:23.930992Z","iopub.status.idle":"2025-07-03T09:15:23.936284Z","shell.execute_reply.started":"2025-07-03T09:15:23.930973Z","shell.execute_reply":"2025-07-03T09:15:23.935615Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'export_logits_to_nifty_segmentation(\\n    predictor=predictor,\\n    plan=plan,\\n    model_dir_readonly=Path(model_dir_readonly),\\n    logits_filename=\"pred_00006\",\\n    npz_dir=\"SHAP-test/shap_run\",\\n    output_dir=\"SHAP-test/shap_run\",\\n    fold=0,\\n    save_probs=False\\n)'"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"## Check that the 2 methods provide the same results","metadata":{}},{"cell_type":"code","source":"\"\"\"# Load the segmentations\nseg1 = nib.load(\"SHAP/shap_run/pred_00006_seg.nii.gz\").get_fdata()\nseg2 = nib.load(\"SHAP-test/shap_run/pred_00006_seg.nii.gz\").get_fdata()\n\n# Check for voxel-wise identity\nif np.array_equal(seg1, seg2):\n    print(\"✅ The segmentations are exactly the same.\")\nelse:\n    print(\"❌ Differences found between the segmentations.\")\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:23.936994Z","iopub.execute_input":"2025-07-03T09:15:23.937180Z","iopub.status.idle":"2025-07-03T09:15:23.949349Z","shell.execute_reply.started":"2025-07-03T09:15:23.937166Z","shell.execute_reply":"2025-07-03T09:15:23.948806Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'# Load the segmentations\\nseg1 = nib.load(\"SHAP/shap_run/pred_00006_seg.nii.gz\").get_fdata()\\nseg2 = nib.load(\"SHAP-test/shap_run/pred_00006_seg.nii.gz\").get_fdata()\\n\\n# Check for voxel-wise identity\\nif np.array_equal(seg1, seg2):\\n    print(\"✅ The segmentations are exactly the same.\")\\nelse:\\n    print(\"❌ Differences found between the segmentations.\")'"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"\"\"\"def dice_coefficient(seg1, seg2):\n    intersection = np.logical_and(seg1, seg2).sum()\n    return 2. * intersection / (seg1.sum() + seg2.sum())\n\nprint(\"Dice:\", dice_coefficient(seg1, seg2))\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:23.950199Z","iopub.execute_input":"2025-07-03T09:15:23.950564Z","iopub.status.idle":"2025-07-03T09:15:23.964198Z","shell.execute_reply.started":"2025-07-03T09:15:23.950538Z","shell.execute_reply":"2025-07-03T09:15:23.963419Z"},"jupyter":{"source_hidden":true}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'def dice_coefficient(seg1, seg2):\\n    intersection = np.logical_and(seg1, seg2).sum()\\n    return 2. * intersection / (seg1.sum() + seg2.sum())\\n\\nprint(\"Dice:\", dice_coefficient(seg1, seg2))'"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"### A dice score of nearly 1 means that the only difference may be Floating Point precision errors accumulating in overlap regions across patches. No worries!","metadata":{}},{"cell_type":"markdown","source":"## Try Captum's kernel SHAP on the organ mask","metadata":{}},{"cell_type":"markdown","source":"### first derive a customized class from Captum library, to use sliding window caching","metadata":{}},{"cell_type":"markdown","source":"## Try to customize KernelShap as a \"sibling\", so let's inherit the parent, LimeBase\nthat's because we need to override (to-and-from)/interpret_rep_transform methods used to map the (1,M) binary mask vector with the perturbed volume AND the perturbation mask we need for caching","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n\n# pyre-strict\nimport inspect\nimport math\nimport typing\nimport warnings\nfrom collections.abc import Iterator\nfrom typing import Any, Callable, cast, List, Literal, Optional, Tuple, Union\n\nimport torch\nfrom captum._utils.common import (\n    _expand_additional_forward_args,\n    _expand_target,\n    _flatten_tensor_or_tuple,\n    _format_output,\n    _format_tensor_into_tuples,\n    _get_max_feature_index,\n    _is_tuple,\n    _reduce_list,\n    _run_forward,\n)\nfrom captum._utils.models.linear_model import SkLearnLasso\nfrom captum._utils.models.model import Model\nfrom captum._utils.progress import progress\nfrom captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\nfrom captum.attr._utils.attribution import PerturbationAttribution\nfrom captum.attr._utils.batching import _batch_example_iterator\nfrom captum.attr._utils.common import (\n    _construct_default_feature_mask,\n    _format_input_baseline,\n)\nfrom captum.log import log_usage\nfrom torch import Tensor, BoolTensor\nfrom torch.nn import CosineSimilarity\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nclass LimeBaseWithCustomArgumentToForwardFunc(PerturbationAttribution):\n    r\"\"\"\n    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_func: Callable[..., Tensor],\n        interpretable_model: Model,\n        similarity_func: Callable[\n            ...,\n            Union[float, Tensor],\n        ],\n        perturb_func: Callable[..., object],\n        perturb_interpretable_space: bool,\n        from_interp_rep_transform: Optional[\n            Callable[..., Union[Tensor, Tuple[Tensor, ...]]]\n        ],\n        to_interp_rep_transform: Optional[Callable[..., Tensor]],\n    ) -> None:\n        r\"\"\"\n\n        Args:\n\n\n            forward_func (Callable): The forward function of the model or any\n                    modification of it. If a batch is provided as input for\n                    attribution, it is expected that forward_func returns a scalar\n                    representing the entire batch.\n            interpretable_model (Model): Model object to train interpretable model.\n                    A Model object provides a `fit` method to train the model,\n                    given a dataloader, with batches containing three tensors:\n\n                    - interpretable_inputs: Tensor\n                      [2D num_samples x num_interp_features],\n                    - expected_outputs: Tensor [1D num_samples],\n                    - weights: Tensor [1D num_samples]\n\n                    The model object must also provide a `representation` method to\n                    access the appropriate coefficients or representation of the\n                    interpretable model after fitting.\n                    Some predefined interpretable linear models are provided in\n                    captum._utils.models.linear_model including wrappers around\n                    SkLearn linear models as well as SGD-based PyTorch linear\n                    models.\n\n                    Note that calling fit multiple times should retrain the\n                    interpretable model, each attribution call reuses\n                    the same given interpretable model object.\n            similarity_func (Callable): Function which takes a single sample\n                    along with its corresponding interpretable representation\n                    and returns the weight of the interpretable sample for\n                    training interpretable model. Weight is generally\n                    determined based on similarity to the original input.\n                    The original paper refers to this as a similarity kernel.\n\n                    The expected signature of this callable is:\n\n                    >>> similarity_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_interpretable_input:\n                    >>>        Tensor [2D 1 x num_interp_features],\n                    >>>    **kwargs: Any\n                    >>> ) -> float or Tensor containing float scalar\n\n                    perturbed_input and original_input will be the same type and\n                    contain tensors of the same shape (regardless of whether or not\n                    the sampling function returns inputs in the interpretable\n                    space). original_input is the same as the input provided\n                    when calling attribute.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n            perturb_func (Callable): Function which returns a single\n                    sampled input, generally a perturbation of the original\n                    input, which is used to train the interpretable surrogate\n                    model. Function can return samples in either\n                    the original input space (matching type and tensor shapes\n                    of original input) or in the interpretable input space,\n                    which is a vector containing the intepretable features.\n                    Alternatively, this function can return a generator\n                    yielding samples to train the interpretable surrogate\n                    model, and n_samples perturbations will be sampled\n                    from this generator.\n\n                    The expected signature of this callable is:\n\n                    >>> perturb_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor, tuple[Tensor, ...], or\n                    >>>    generator yielding tensor or tuple[Tensor, ...]\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n\n                    Returned sampled input should match the input type (Tensor\n                    or Tuple of Tensor and corresponding shapes) if\n                    perturb_interpretable_space = False. If\n                    perturb_interpretable_space = True, the return type should\n                    be a single tensor of shape 1 x num_interp_features,\n                    corresponding to the representation of the\n                    sample to train the interpretable model.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n            perturb_interpretable_space (bool): Indicates whether\n                    perturb_func returns a sample in the interpretable space\n                    (tensor of shape 1 x num_interp_features) or a sample\n                    in the original space, matching the format of the original\n                    input. Once sampled, inputs can be converted to / from\n                    the interpretable representation with either\n                    to_interp_rep_transform or from_interp_rep_transform.\n            from_interp_rep_transform (Callable): Function which takes a\n                    single sampled interpretable representation (tensor\n                    of shape 1 x num_interp_features) and returns\n                    the corresponding representation in the input space\n                    (matching shapes of original input to attribute).\n\n                    This argument is necessary if perturb_interpretable_space\n                    is True, otherwise None can be provided for this argument.\n\n                    The expected signature of this callable is:\n\n                    >>> from_interp_rep_transform(\n                    >>>    curr_sample: Tensor [2D 1 x num_interp_features]\n                    >>>    original_input: Tensor or Tuple of Tensors,\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor or tuple[Tensor, ...]\n\n                    Returned sampled input should match the type of original_input\n                    and corresponding tensor shapes.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n\n            to_interp_rep_transform (Callable): Function which takes a\n                    sample in the original input space and converts to\n                    its interpretable representation (tensor\n                    of shape 1 x num_interp_features).\n\n                    This argument is necessary if perturb_interpretable_space\n                    is False, otherwise None can be provided for this argument.\n\n                    The expected signature of this callable is:\n\n                    >>> to_interp_rep_transform(\n                    >>>    curr_sample: Tensor or Tuple of Tensors,\n                    >>>    original_input: Tensor or Tuple of Tensors,\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor [2D 1 x num_interp_features]\n\n                    curr_sample will match the type of original_input\n                    and corresponding tensor shapes.\n\n                    All kwargs passed to the attribute method are\n                    provided as keyword arguments (kwargs) to this callable.\n        \"\"\"\n        PerturbationAttribution.__init__(self, forward_func)\n        self.interpretable_model = interpretable_model\n        self.similarity_func = similarity_func\n        self.perturb_func = perturb_func\n        self.perturb_interpretable_space = perturb_interpretable_space\n        self.from_interp_rep_transform = from_interp_rep_transform\n        self.to_interp_rep_transform = to_interp_rep_transform\n\n        if self.perturb_interpretable_space:\n            assert (\n                self.from_interp_rep_transform is not None\n            ), \"Must provide transform from interpretable space to original input space\"\n            \" when sampling from interpretable space.\"\n        else:\n            assert (\n                self.to_interp_rep_transform is not None\n            ), \"Must provide transform from original input space to interpretable space\"\n\n    @log_usage(part_of_slo=True)\n    @torch.no_grad()\n    def attribute(\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        target: TargetType = None,\n        additional_forward_args: Optional[Tuple[object, ...]] = None,\n        n_samples: int = 50,\n        perturbations_per_eval: int = 1,\n        show_progress: bool = False,\n        **kwargs: object,\n    ) -> Tensor:\n        r\"\"\"\n        This method attributes the output of the model with given target index\n        (in case it is provided, otherwise it assumes that output is a\n        scalar) to the inputs of the model using the approach described above.\n        It trains an interpretable model and returns a representation of the\n        interpretable model.\n\n        It is recommended to only provide a single example as input (tensors\n        with first dimension or batch size = 1). This is because LIME is generally\n        used for sample-based interpretability, training a separate interpretable\n        model to explain a model's prediction on each individual example.\n\n        A batch of inputs can be provided as inputs only if forward_func\n        returns a single value per batch (e.g. loss).\n        The interpretable feature representation should still have shape\n        1 x num_interp_features, corresponding to the interpretable\n        representation for the full batch, and perturbations_per_eval\n        must be set to 1.\n\n        Args:\n\n            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n                        is computed. If forward_func takes a single\n                        tensor as input, a single input tensor should be provided.\n                        If forward_func takes multiple tensors as input, a tuple\n                        of the input tensors should be provided. It is assumed\n                        that for all given input tensors, dimension 0 corresponds\n                        to the number of examples, and if multiple input tensors\n                        are provided, the examples must be aligned appropriately.\n            target (int, tuple, Tensor, or list, optional): Output indices for\n                        which surrogate model is trained\n                        (for classification cases,\n                        this is usually the target class).\n                        If the network returns a scalar value per example,\n                        no target index is necessary.\n                        For general 2D outputs, targets can be either:\n\n                        - a single integer or a tensor containing a single\n                          integer, which is applied to all input examples\n\n                        - a list of integers or a 1D tensor, with length matching\n                          the number of examples in inputs (dim 0). Each integer\n                          is applied as the target for the corresponding example.\n\n                        For outputs w            except --------ith > 2 dimensions, targets can be either:\n\n                        - A single tuple, which contains #output_dims - 1\n                          elements. This target index is applied to all examples.\n\n                        - A list of tuples with length equal to the number of\n                          examples in inputs (dim 0), and each tuple containing\n                          #output_dims - 1 elements. Each tuple is applied as the\n                          target for the corresponding example.\n\n                        Default: None\n            additional_forward_args (Any, optional): If the forward function\n                        requires additional arguments other than the inputs for\n                        which attributions should not be computed, this argument\n                        can be provided. It must be either a single additional\n                        argument of a Tensor or arbitrary (non-tuple) type or a\n                        tuple containing multiple additional arguments including\n                        tensors or any arbitrary python types. These arguments\n                        are provided to forward_func in order following the\n                        arguments in inputs.\n                        For a tensor, the first dimension of the tensor must\n                        correspond to the number of examples. For all other types,\n                        the given argument is used for all forward evaluations.\n                        Note that attributions are not computed with respect\n                        to these arguments.\n                        Default: None\n            n_samples (int, optional): The number of samples of the original\n                        model used to train the surrogate interpretable model.\n                        Default: `50` if `n_samples` is not provided.\n            perturbations_per_eval (int, optional): Allows multiple samples\n                        to be processed simultaneously in one call to forward_fn.\n                        Each forward pass will contain a maximum of\n                        perturbations_per_eval * #examples samples.\n                        For DataParallel models, each batch is split among the\n                        available devices, so evaluations on each available\n                        device contain at most\n                        (perturbations_per_eval * #examples) / num_devices\n                        samples.\n                        If the forward function returns a single scalar per batch,\n                        perturbations_per_eval must be set to 1.\n                        Default: 1\n            show_progress (bool, optional): Displays the progress of computation.\n                        It will try to use tqdm if available for advanced features\n                        (e.g. time estimation). Otherwise, it will fallback to\n                        a simple output of progress.\n                        Default: False\n            **kwargs (Any, optional): Any additional arguments necessary for\n                        sampling and transformation functions (provided to\n                        constructor).\n                        Default: None\n\n        Returns:\n            **interpretable model representation**:\n            - **interpretable model representation** (*Any*):\n                    A representation of the interpretable model trained. The return\n                    type matches the return type of train_interpretable_model_func.\n                    For example, this could contain coefficients of a\n                    linear surrogate model.\n\n        Examples::\n\n            >>> # SimpleClassifier takes a single input tensor of\n            >>> # float features with size N x 5,\n            >>> # and returns an Nx3 tensor of class probabilities.\n            >>> net = SimpleClassifier()\n            >>>\n            >>> # We will train an interpretable model with the same\n            >>> # features by simply sampling with added Gaussian noise\n            >>> # to the inputs and training a model to predict the\n            >>> # score of the target class.\n            >>>\n            >>> # For interpretable model training, we will use sklearn\n            >>> # linear model in this example. We have provided wrappers\n            >>> # around sklearn linear models to fit the Model interface.\n            >>> # Any arguments provided to the sklearn constructor can also\n            >>> # be provided to the wrapper, e.g.:\n            >>> # SkLearnLinearModel(\"linear_model.Ridge\", alpha=2.0)\n            >>> from captum._utils.models.linear_model import SkLearnLinearModel\n            >>>\n            >>>\n            >>> # Define similarity kernel (exponential kernel based on L2 norm)\n            >>> def similarity_kernel(\n            >>>     original_input: Tensor,\n            >>>     perturbed_input: Tensor,\n            >>>     perturbed_interpretable_input: Tensor,\n            >>>     **kwargs)->Tensor:\n            >>>         # kernel_width will be provided to attribute as a kwarg\n            >>>         kernel_width = kwargs[\"kernel_width\"]\n            >>>         l2_dist = torch.norm(original_input - perturbed_input)\n            >>>         return torch.exp(- (l2_dist**2) / (kernel_width**2))\n            >>>\n            >>>\n            >>> # Define sampling function\n            >>> # This function samples in original input space\n            >>> def perturb_func(\n            >>>     original_input: Tensor,\n            >>>     **kwargs)->Tensor:\n            >>>         return original_input + torch.randn_like(original_input)\n            >>>\n            >>> # For this example, we are setting the interpretable input to\n            >>> # match the model input, so the to_interp_rep_transform\n            >>> # function simply returns the input. In most cases, the interpretable\n            >>> # input will be different and may have a smaller feature set, so\n            >>> # an appropriate transformation function should be provided.\n            >>>\n            >>> def to_interp_transform(curr_sample, original_inp,\n            >>>                                      **kwargs):\n            >>>     return curr_sample\n            >>>\n            >>> # Generating random input with size 1 x 5\n            >>> input = torch.randn(1, 5)\n            >>> # Defining LimeBase interpreter\n            >>> lime_attr = LimeBase(net,\n                                     SkLearnLinearModel(\"linear_model.Ridge\"),\n                                     similarity_func=similarity_kernel,\n                                     perturb_func=perturb_func,\n                                     perturb_interpretable_space=False,\n                                     from_interp_rep_transform=None,\n                                     to_interp_rep_transform=to_interp_transform)\n            >>> # Computes interpretable model, returning coefficients of linear\n            >>> # model.\n            >>> attr_coefs = lime_attr.attribute(input, target=1, kernel_width=1.1)\n        \"\"\"\n        inp_tensor = cast(Tensor, inputs) if isinstance(inputs, Tensor) else inputs[0]\n        device = inp_tensor.device\n\n        interpretable_inps = []\n        similarities = []\n        outputs = []\n\n        curr_model_inputs = []\n        expanded_additional_args = None\n        expanded_target = None\n        gen_perturb_func = self._get_perturb_generator_func(inputs, **kwargs)\n\n        if show_progress:\n            attr_progress = progress(\n                total=math.ceil(n_samples / perturbations_per_eval),\n                desc=f\"{self.get_name()} attribution\",\n            )\n            attr_progress.update(0)\n\n        # LOOP FUNCTION -> HERE WE NEED TO GET THE PERTURBED INPUT, BUILD OUR PERTURBATION_MASK AND PASS IT TO THE FORWARD FUNCTION\n        # one convoluted thing is feature mask scope, it is passed to this method from Lime via super().attribute(), but it's not in the \n        # method declaration\n        feature_mask = kwargs[\"feature_mask\"]\n        batch_count = 0\n        for _ in range(n_samples):\n            try:\n                interpretable_inp, curr_model_input = gen_perturb_func()\n                perturbation_mask = self._get_perturbation_mask(interpretable_inp, curr_model_input, feature_mask)\n            except StopIteration:\n                warnings.warn(\n                    \"Generator completed prior to given n_samples iterations!\",\n                    stacklevel=1,\n                )\n                break\n            except:\n                print(\"error in the perturbation mask generation\")\n                raise\n               \n            # add the perturbation mask as an additional parameter for the forward function\n            if additional_forward_args is None:\n                additional_forward_args_with_mask = (perturbation_mask,)\n            elif isinstance(additional_forward_args, tuple):\n                additional_forward_args_with_mask = additional_forward_args + (perturbation_mask,)\n            else:\n                additional_forward_args_with_mask = (additional_forward_args, perturbation_mask)\n             #---------------------------------------------\n            batch_count += 1\n            interpretable_inps.append(interpretable_inp)\n            curr_model_inputs.append(curr_model_input)\n\n            curr_sim = self.similarity_func(\n                inputs, curr_model_input, interpretable_inp, **kwargs\n            )\n            similarities.append(\n                curr_sim.flatten()\n                if isinstance(curr_sim, Tensor)\n                else torch.tensor([curr_sim], device=device)\n            )\n\n            if len(curr_model_inputs) == perturbations_per_eval:\n                # change: removed if condition, to rebuild final expanded_additional_forward_args at each iteration\n                #if expanded_additional_args is None:\n                expanded_additional_args = _expand_additional_forward_args(\n                    additional_forward_args_with_mask, len(curr_model_inputs)\n                )\n                if expanded_target is None:\n                    expanded_target = _expand_target(target, len(curr_model_inputs))\n            \n                \n                model_out = self._evaluate_batch(\n                    curr_model_inputs,\n                    expanded_target,\n                    expanded_additional_args,\n                    device,\n                )\n\n                if show_progress:\n                    attr_progress.update()\n\n                outputs.append(model_out)\n\n                curr_model_inputs = []\n\n        if len(curr_model_inputs) > 0:\n            expanded_additional_args = _expand_additional_forward_args(\n                additional_forward_args_with_mask, len(curr_model_inputs)\n            )\n            expanded_target = _expand_target(target, len(curr_model_inputs))\n\n            model_out = self._evaluate_batch(\n                curr_model_inputs,\n                expanded_target,\n                expanded_additional_args,\n                device,\n            )\n            if show_progress:\n                attr_progress.update()\n            outputs.append(model_out)\n\n        if show_progress:\n            attr_progress.close()\n\n        # Argument 1 to \"cat\" has incompatible type\n        # \"list[Tensor | tuple[Tensor, ...]]\";\n        # expected \"tuple[Tensor, ...] | list[Tensor]\"  [arg-type]\n        combined_interp_inps = torch.cat(interpretable_inps).float()  # type: ignore\n        combined_outputs = (\n            torch.cat(outputs) if len(outputs[0].shape) > 0 else torch.stack(outputs)\n        ).float()\n        combined_sim = (\n            torch.cat(similarities)\n            if len(similarities[0].shape) > 0\n            else torch.stack(similarities)\n        ).float()\n        self.dataset = TensorDataset(combined_interp_inps, combined_outputs, combined_sim)\n        self.interpretable_model.fit(DataLoader(self.dataset, batch_size=batch_count))\n        return self.interpretable_model.representation()\n\n\n    def _get_perturbation_mask(\n        self,\n        interpretable_input: torch.Tensor,       # shape = (B, M)\n        original_inputs: TensorOrTupleOfTensors, # shape = (B, C, D, W, H) or tuple thereof\n        feature_mask,\n    ) -> Union[torch.BoolTensor, Tuple[torch.BoolTensor, ...]]:\n        \"\"\"\n        Build a Boolean mask of shape (B, *input_dims) indicating which\n        elements should be perturbed (True) vs. left untouched (False).\n        \"\"\"\n    \n        # Case 1: single‐Tensor input\n        if isinstance(feature_mask, torch.Tensor):\n            # advanced indexing over the batch dimension\n            # result has shape (B, *feature_mask.shape)\n            mask = interpretable_input[:, feature_mask]\n            mask = ~mask.bool()\n\n            return mask\n    \n        # Case 2: multi‐input (tuple) model\n        else:\n            masks = []\n            for fm_i in feature_mask:\n                mask_i = interpretable_input[:, fm_i]  # → (B, *fm_i.shape)\n                masks.append(~mask_i.bool())\n            return tuple(masks)\n\n    \n\n    def _get_perturb_generator_func(\n        self, inputs: TensorOrTupleOfTensorsGeneric, **kwargs: Any\n    ) -> Callable[\n        [], Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n    ]:\n        perturb_generator: Optional[Iterator[TensorOrTupleOfTensorsGeneric]]\n        perturb_generator = None\n        if inspect.isgeneratorfunction(self.perturb_func):\n            perturb_generator = self.perturb_func(inputs, **kwargs)\n\n        def generate_perturbation() -> (\n            Tuple[TensorOrTupleOfTensorsGeneric, TensorOrTupleOfTensorsGeneric]\n        ):\n            if perturb_generator:\n                curr_sample = next(perturb_generator)\n            else:\n                curr_sample = self.perturb_func(inputs, **kwargs)\n\n            if self.perturb_interpretable_space:\n                interpretable_inp = curr_sample\n                curr_model_input = self.from_interp_rep_transform(  # type: ignore\n                    curr_sample, inputs, **kwargs\n                )\n            else:\n                curr_model_input = curr_sample\n                interpretable_inp = self.to_interp_rep_transform(  # type: ignore\n                    curr_sample, inputs, **kwargs\n                )\n\n            return interpretable_inp, curr_model_input  # type: ignore\n\n        return generate_perturbation\n\n    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.)\n    def attribute_future(self) -> Callable:\n        r\"\"\"\n        This method is not implemented for LimeBase.\n        \"\"\"\n        raise NotImplementedError(\n            \"LimeBase does not support attribution of future samples.\"\n        )\n\n    def _evaluate_batch(\n        self,\n        curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n        expanded_target: TargetType,\n        expanded_additional_args: object,\n        device: torch.device,\n    ) -> Tensor:\n        model_out = _run_forward(\n            self.forward_func,\n            #MOMENTANEAOUS---> sliding_window forward function only works with single items (no batch) --> take first\n            #_reduce_list(curr_model_inputs),\n            _reduce_list(curr_model_inputs)[0],\n            expanded_target,\n            expanded_additional_args,\n        )\n        if isinstance(model_out, Tensor):\n            assert model_out.numel() == len(curr_model_inputs), (\n                \"Number of outputs is not appropriate, must return \"\n                \"one output per perturbed input\"\n            )\n        if isinstance(model_out, Tensor):\n            return model_out.flatten()\n        return torch.tensor([model_out], device=device)\n\n    def has_convergence_delta(self) -> bool:\n        return False\n\n    @property\n    def multiplies_by_inputs(self) -> bool:\n        return False\n\n\n# Default transformations and methods\n# for Lime child implementation.\n\n\n# pyre-fixme[3]: Return type must be annotated.\n# pyre-fixme[2]: Parameter must be annotated.\ndef default_from_interp_rep_transform(curr_sample, original_inputs, **kwargs):\n    assert (\n        \"feature_mask\" in kwargs\n    ), \"Must provide feature_mask to use default interpretable representation transform\"\n    assert (\n        \"baselines\" in kwargs\n    ), \"Must provide baselines to use default interpretable representation transform\"\n    feature_mask = kwargs[\"feature_mask\"]\n    if isinstance(feature_mask, Tensor):\n        binary_mask = curr_sample[0][feature_mask].bool()\n        input_space_transformed = (\n            binary_mask.to(original_inputs.dtype) * original_inputs\n            + (~binary_mask).to(original_inputs.dtype) * kwargs[\"baselines\"]\n        )\n        \n        return input_space_transformed\n    else:\n        binary_mask = tuple(\n            curr_sample[0][feature_mask[j]].bool() for j in range(len(feature_mask))\n        )\n        input_space_transformed = tuple(\n            binary_mask[j].to(original_inputs[j].dtype) * original_inputs[j]\n            + (~binary_mask[j]).to(original_inputs[j].dtype) * kwargs[\"baselines\"][j]\n            for j in range(len(feature_mask))\n        )\n        return input_space_transformed\n\n\ndef get_exp_kernel_similarity_function(\n    distance_mode: str = \"cosine\",\n    kernel_width: float = 1.0,\n) -> Callable[..., float]:\n    r\"\"\"\n    This method constructs an appropriate similarity function to compute\n    weights for perturbed sample in LIME. Distance between the original\n    and perturbed inputs is computed based on the provided distance mode,\n    and the distance is passed through an exponential kernel with given\n    kernel width to convert to a range between 0 and 1.\n\n    The callable returned can be provided as the similarity_fn for\n    Lime or LimeBase.\n\n    Args:\n\n        distance_mode (str, optional): Distance mode can be either \"cosine\" or\n                    \"euclidean\" corresponding to either cosine distance\n                    or Euclidean distance respectively. Distance is computed\n                    by flattening the original inputs and perturbed inputs\n                    (concatenating tuples of inputs if necessary) and computing\n                    distances between the resulting vectors.\n                    Default: \"cosine\"\n        kernel_width (float, optional):\n                    Kernel width for exponential kernel applied to distance.\n                    Default: 1.0\n\n    Returns:\n\n        *Callable*:\n        - **similarity_fn** (*Callable*):\n            Similarity function. This callable can be provided as the\n            similarity_fn for Lime or LimeBase.\n    \"\"\"\n\n    # pyre-fixme[3]: Return type must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def default_exp_kernel(original_inp, perturbed_inp, __, **kwargs):\n        flattened_original_inp = _flatten_tensor_or_tuple(original_inp).float()\n        flattened_perturbed_inp = _flatten_tensor_or_tuple(perturbed_inp).float()\n        if distance_mode == \"cosine\":\n            cos_sim = CosineSimilarity(dim=0)\n            distance = 1 - cos_sim(flattened_original_inp, flattened_perturbed_inp)\n        elif distance_mode == \"euclidean\":\n            distance = torch.norm(flattened_original_inp - flattened_perturbed_inp)\n        else:\n            raise ValueError(\"distance_mode must be either cosine or euclidean.\")\n        return math.exp(-1 * (distance**2) / (2 * (kernel_width**2)))\n\n    return default_exp_kernel\n\n\ndef default_perturb_func(\n    original_inp: TensorOrTupleOfTensorsGeneric, **kwargs: object\n) -> Tensor:\n    assert (\n        \"num_interp_features\" in kwargs\n    ), \"Must provide num_interp_features to use default interpretable sampling function\"\n    if isinstance(original_inp, Tensor):\n        device = original_inp.device\n    else:\n        device = original_inp[0].device\n\n    probs = torch.ones(1, cast(int, kwargs[\"num_interp_features\"])) * 0.5\n    return torch.bernoulli(probs).to(device=device).long()\n\n\ndef construct_feature_mask(\n    feature_mask: Union[None, Tensor, Tuple[Tensor, ...]],\n    formatted_inputs: Tuple[Tensor, ...],\n) -> Tuple[Tuple[Tensor, ...], int]:\n    feature_mask_tuple: Tuple[Tensor, ...]\n    if feature_mask is None:\n        feature_mask_tuple, num_interp_features = _construct_default_feature_mask(\n            formatted_inputs\n        )\n    else:\n        feature_mask_tuple = _format_tensor_into_tuples(feature_mask)\n        min_interp_features = int(\n            min(\n                torch.min(single_mask).item()\n                for single_mask in feature_mask_tuple\n                if single_mask.numel()\n            )\n        )\n        if min_interp_features != 0:\n            warnings.warn(\n                \"Minimum element in feature mask is not 0, shifting indices to\"\n                \" start at 0.\",\n                stacklevel=2,\n            )\n            feature_mask_tuple = tuple(\n                single_mask - min_interp_features for single_mask in feature_mask_tuple\n            )\n\n        num_interp_features = _get_max_feature_index(feature_mask_tuple) + 1\n    return feature_mask_tuple, num_interp_features\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:23.965237Z","iopub.execute_input":"2025-07-03T09:15:23.965460Z","iopub.status.idle":"2025-07-03T09:15:24.097263Z","shell.execute_reply.started":"2025-07-03T09:15:23.965434Z","shell.execute_reply":"2025-07-03T09:15:24.096472Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class LimeWithCustomArgumentToForwardFunc(LimeBaseWithCustomArgumentToForwardFunc):\n    r\"\"\"\n    Here we create a modification of Lime class from Captum Library (https://captum.ai/api/_modules/captum/attr/_core/lime.html)\n    This will just inherit our modified LimeBase class\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_func: Callable[..., Tensor],\n        interpretable_model: Optional[Model] = None,\n        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n        similarity_func: Optional[Callable] = None,\n        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n        perturb_func: Optional[Callable] = None,\n    ) -> None:\n        r\"\"\"\n\n        Args:\n\n\n            forward_func (Callable): The forward function of the model or any\n                    modification of it\n            interpretable_model (Model, optional): Model object to train\n                    interpretable model.\n\n                    This argument is optional and defaults to SkLearnLasso(alpha=0.01),\n                    which is a wrapper around the Lasso linear model in SkLearn.\n                    This requires having sklearn version >= 0.23 available.\n\n                    Other predefined interpretable linear models are provided in\n                    captum._utils.models.linear_model.\n\n                    Alternatively, a custom model object must provide a `fit` method to\n                    train the model, given a dataloader, with batches containing\n                    three tensors:\n\n                    - interpretable_inputs: Tensor\n                      [2D num_samples x num_interp_features],\n                    - expected_outputs: Tensor [1D num_samples],\n                    - weights: Tensor [1D num_samples]\n\n                    The model object must also provide a `representation` method to\n                    access the appropriate coefficients or representation of the\n                    interpretable model after fitting.\n\n                    Note that calling fit multiple times should retrain the\n                    interpretable model, each attribution call reuses\n                    the same given interpretable model object.\n            similarity_func (Callable, optional): Function which takes a single sample\n                    along with its corresponding interpretable representation\n                    and returns the weight of the interpretable sample for\n                    training the interpretable model.\n                    This is often referred to as a similarity kernel.\n\n                    This argument is optional and defaults to a function which\n                    applies an exponential kernel to the cosine distance between\n                    the original input and perturbed input, with a kernel width\n                    of 1.0.\n\n                    A similarity function applying an exponential\n                    kernel to cosine / euclidean distances can be constructed\n                    using the provided get_exp_kernel_similarity_function in\n                    captum.attr._core.lime.\n\n                    Alternately, a custom callable can also be provided.\n                    The expected signature of this callable is:\n\n                    >>> def similarity_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_input: Tensor or tuple[Tensor, ...],\n                    >>>    perturbed_interpretable_input:\n                    >>>        Tensor [2D 1 x num_interp_features],\n                    >>>    **kwargs: Any\n                    >>> ) -> float or Tensor containing float scalar\n\n                    perturbed_input and original_input will be the same type and\n                    contain tensors of the same shape, with original_input\n                    being the same as the input provided when calling attribute.\n\n                    kwargs includes baselines, feature_mask, num_interp_features\n                    (integer, determined from feature mask).\n            perturb_func (Callable, optional): Function which returns a single\n                    sampled input, which is a binary vector of length\n                    num_interp_features, or a generator of such tensors.\n\n                    This function is optional, the default function returns\n                    a binary vector where each element is selected\n                    independently and uniformly at LimeWithCustomArgumentToForwardFuncrandom. Custom\n                    logic for selecting sampled binary vectors can\n                    be implemented by providing a function with the\n                    following expected signature:\n\n                    >>> perturb_func(\n                    >>>    original_input: Tensor or tuple[Tensor, ...],\n                    >>>    **kwargs: Any\n                    >>> ) -> Tensor [Binary 2D Tensor 1 x num_interp_features]\n                    >>>  or generator yielding such tensors\n\n                    kwargs includes baselines, feature_mask, num_interp_features\n                    (integer, determined from feature mask).\n\n        \"\"\"\n        if interpretable_model is None:\n            interpretable_model = SkLearnLasso(alpha=0.01)\n\n        if similarity_func is None:\n            similarity_func = get_exp_kernel_similarity_function()\n\n        if perturb_func is None:\n            perturb_func = default_perturb_func\n\n        LimeBase.__init__(\n            self,\n            forward_func,\n            interpretable_model,\n            similarity_func,\n            perturb_func,\n            True,\n            default_from_interp_rep_transform,\n            None,\n        )\n\n    @log_usage(part_of_slo=True)\n    def attribute(  # type: ignore\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        baselines: BaselineType = None,\n        target: TargetType = None,\n        additional_forward_args: Optional[object] = None,\n        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n        n_samples: int = 25,\n        perturbations_per_eval: int = 1,\n        return_input_shape: bool = True,\n        show_progress: bool = False,\n    ) -> TensorOrTupleOfTensorsGeneric:\n        r\"\"\"\n        This method attributes the output of the model with given target index\n        (in case it is provided, otherwise it assumes that output is a\n        scalar) to the inputs of the model using the approach described above,\n        training an interpretable model and returning a representation of the\n        interpretable model.\n\n        It is recommended to only provide a single example as input (tensors\n        with first dimension or batch size = 1). This is because LIME is generally\n        used for sample-based interpretability, training a separate interpretable\n        model to explain a model's prediction on each individual example.\n\n        A batch of inputs can also be provided as inputs, similar to\n        other perturbation-based attribution methods. In this case, if forward_fn\n        returns a scalar per example, attributions will be computed for each\n        example independently, with a separate interpretable model trained for each\n        example. Note that provided similarity and pertforward_funcurbation functions will be\n        provided each example separately (first dimension = 1) in this case.\n        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n        still be computed using a single interpretable model for the full batch.\n        In this case, similarity and perturbation functions will be provided the\n        same original input containing the full batch.\n\n        The number of interpretable features is determined from the provided\n        feature mask, or if none is provided, from the default feature mask,\n        which considers each scalar input as a separate feature. It is\n        generally recommended to provide a feature mask which groups features\n        into a small number of interpretable features / components (e.g.\n        superpixels in images).\n\n        Args:\n\n            inputs (Tensor or tuple[Tensor, ...]): Input for which LIME\n                        is computed. If forward_func takes a single\n                        tensor as input, a single input tensor should be provided.\n                        If forward_func takes multiple tensors as input, a tuple\n                        of the input tensors should be provided. It is assumed\n                        that for all given input tensors, dimension 0 corresponds\n                        to the number of examples, and if multiple input tensors\n                        are provided, the examples must be aligned appropriately.\n            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n                        Baselines define reference value which replaces each\n                        feature when the corresponding interpretable feature\n                        is set to 0.\n                        Baselines can be provided as:\n\n                        - a single tensor, if inputs is a single tensor, with\n                          exactly the same dimensions as inputs or the first\n                          dimension is one and the remaining dimensions match\n                          with inputs.\n\n                        - a single scalar, if inputs is a single tensor, which will\n                          be broadcasted for each input value in input tensor.\n\n                        - a tuple of tensors or scalars, the baseline corresponding\n                          to each tensor in the inputs' tuple can be:\n\n                          - either a tensor with matching dimensions to\n                            corresponding tensor in the inputs' tuple\n                            or the first dimension is one and the remaining\n                            dimensions match with the corresponding\n                            input tensor.\n\n                          - or a scalar, corresponding to a tensor in the\n                            inputs' tuple. This scalar value is broadcasted\n                            for corresponding input tensor.\n\n                        In the cases when `baselines` iforward_funcs not provided, we internally\n                        use zero scalar corresponding to each input tensor.\n                        Default: None\n            target (int, tuple, Tensor, or list, optional): Output indices for\n                        which surrogate model is trained\n                        (for classification cases,\n                        this is usually the target class).\n                        If the network returns a scalar value per example,\n                        no target index is necessary.\n                        For general 2D outputs, targets can be either:\n\n                        - a single integer or a tensor containing a single\n                          integer, which is applied to all input examples\n\n                        - a list of integers or a 1D tensor, with length matching\n                          the number of examples in inputs (dim 0). Each integer\n                          is applied as the target for the corresponding example.\n\n                        For outputs with > 2 dimensions, targets can be either:\n\n                        - A single tuple, which contains #output_dims - 1\n                          elements. This target index is applied to all examples.\n\n                        - A list of tuples with length equal to the number of\n                          examples in inputs (dim 0), and each tuple containing\n                          #output_dims - 1 elements. Each tuple is applied as the\n                          target for the corresponding example.\n\n                        Default: None\n            additional_forward_args (Any, optional): If the forward function\n                        requires additional arguments other than the inputs for\n                        which attributions should not be computed, this argument\n                        can be provided. It must be either a single additional\n                        argument of a Tensor or arbitrary (non-tuple) type or a\n                        tuple containing multiple additional arguments including\n                        tensors or any arbitrary python types. These arguments\n                        are provided to forward_func in order following the\n                        arguments in inputs.\n                        For a tensor, the first dimension of the tensor must\n                        correspond to the number of examples. It will be\n                        repeated for each of `n_steps` along the integrated\n                        path. For all other types, the given argument is used\n                        for all forward evaluations.\n                        Note that attributions are not computed with respect\n                        to these arguments.\n                        Default: None\n            feature_mask (Tensor or tuple[Tensor, ...], optional):\n                        feature_mask defines a mask for the input, grouping\n                        features which correspond to the same\n                        interpretable feature. feature_mask\n                        should contain the same number of tensors as inputs.\n                        Each tensor should\n                        be the same size as the corresponding input or\n                        broadcastable to match the inpuforward_funct tensor. Values across\n                        all tensors should be integers in the range 0 to\n                        num_interp_features - 1, and indices corresponding to the\n                        same feature should have the same value.\n                        Note that features are grouped across tensors\n                        (unlike feature ablation and occlusion), so\n                        if the same index is used in different tensors, those\n                        features are still grouped and added simultaneously.\n                        If None, then a feature mask is constructed which assigns\n                        each scalar within a tensor as a separate feature.\n                        Default: None\n            n_samples (int, optional): The number of samples of the original\n                        model used to train the surrogate interpretable model.\n                        Default: `50` if `n_samples` is not provided.\n            perturbations_per_eval (int, optional): Allows multiple samples\n                        to be processed simultaneously in one call to forward_fn.\n                        Each forward pass will contain a maximum of\n                        perturbations_per_eval * #examples samples.\n                        For DataParallel models, each batch is split among the\n                        available devices, so evaluations on each available\n                        device contain at most\n                        (perturbations_per_eval * #examples) / num_devices\n                        samples.\n                        If the forward function returns a single scalar per batch,\n                        perturbations_per_eval must be set to 1.\n                        Default: 1\n            return_input_shape (bool, optional): Determines whether the returned\n                        tensor(s) only contain the coefficients for each interp-\n                        retable feature from the trained surrogate model, or\n                        whether the returned attributions match the input shape.\n                        When return_input_shape is True, the return type of attribute\n                        matches the input shape, with each element containing the\n                        coefficient of the corresponding interpretale feature.\n                        All elements with the same value in the feature mask\n                        will contain the same coefficient in the returned\n                        attributions.\n                        If forward_func returns a single element per batch, then the\n                        first dimension of each tensor will be 1, and the remaining\n                        dimensions will have the same shape as the original input\n                        tensor.\n                        If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpreatable models, with length\n                        num_interp_features.\n            show_progress (bool, optional): Displays the progress of computation.\n                        It will try to use tqdm if available for advanced features\n                        (e.g. time estimation). Otherwise, it will fallback to\n                        a simple output of progress.\n                        Default: False\n\n        Returns:\n            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n                        The attributions with respect to each input feature.\n                        If return_input_shape = True, attributions will be\n                        the same size as the provided inputs, with each value\n                        providing the coefficient of the corresponding\n                        interpretale feature.\n                        If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpreatable models, with length\n                        num_interp_features.\n        Examples::\n\n            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n            >>> # and returns an Nx3 tensor of class probabilities.\n            >>> net = SimpleClassifier()\n\n            >>> # Generating random input with size 1 x 4 x 4\n            >>> input = torch.randn(1, 4, 4)\n\n            >>> # Defining Lime interpreter\n            >>> lime = Lime(net)\n            >>> # Computes attribution, with each of the 4 x 4 = 16\n            >>> # features as a separate interpretable feature\n            >>> attr = lime.attribute(input, target=1, n_samples=200)\n\n            >>> # Alternatively, we can group each 2x2 square of the inputs\n            >>> # as one 'interpretable' feature and perturb them together.\n            >>> # This can be done by creating a feature mask as follows, which\n            >>> # defines the feature groups, e.g.:\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # With this mask, all inputs with the same value are set to their\n            >>> # baseline value, when the corresponding binary interpretable\n            >>> # feature is set to 0.\n            >>> # The attributions can be calculated as follows:\n            >>> # feature mask has dimensions 1 x 4 x 4\n            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n            >>>                             [2,2,3,3],[2,2,3,3]]])\n\n            >>> # Computes interpretable model and returning attributions\n            >>> # matching input shape.\n            >>> attr = lime.attribute(input, target=1, feature_mask=feature_mask)\n        \"\"\"\n        return self._attribute_kwargs(\n            inputs=inputs,\n            baselines=baselines,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            feature_mask=feature_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            return_input_shape=return_input_shape,\n            show_progress=show_progress,\n        )\n\n    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n    def attribute_future(self) -> Callable:\n        return super().attribute_future()\n\n    def _attribute_kwargs(  # type: ignore\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        baselines: BaselineType = None,\n        target: TargetType = None,\n        additional_forward_args: Optional[object] = None,\n        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n        n_samples: int = 25,\n        perturbations_per_eval: int = 1,\n        return_input_shape: bool = True,\n        show_progress: bool = False,\n        **kwargs: object,\n    ) -> TensorOrTupleOfTensorsGeneric:\n        is_inputs_tuple = _is_tuple(inputs)\n        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n        bsz = formatted_inputs[0].shape[0]\n\n        feature_mask, num_interp_features = construct_feature_mask(\n            feature_mask, formatted_inputs\n        )\n\n        if num_interp_features > 10000:\n            warnings.warn(\n                \"Attempting to construct interpretable model with > 10000 features.\"\n                \"This can be very slow or lead to OOM issues. Please provide a feature\"\n                \"mask which groups input features to reduce the number of interpretable\"\n                \"features. \",\n                stacklevel=1,\n            )\n\n        coefs: Tensor\n        if bsz > 1:\n            test_output = _run_forward(\n                self.forward_func, inputs, target, additional_forward_args\n            )\n            if isinstance(test_output, Tensor) and torch.numel(test_output) > 1:\n                if torch.numel(test_output) == bsz:\n                    warnings.warn(\n                        \"You are providing multiple inputs for Lime / Kernel SHAP \"\n                        \"attributions. This trains a separate interpretable model \"\n                        \"for each example, which can be time consuming. It is \"\n                        \"recommended to compute attributions for one example at a \"\n                        \"time.\",\n                        stacklevel=1,\n                    )\n                    output_list = []\n                    for (\n                        curr_inps,\n                        curr_target,\n                        curr_additional_args,\n                        curr_baselines,\n                        curr_feature_mask,\n                    ) in _batch_example_iterator(\n                        bsz,\n                        formatted_inputs,\n                        target,\n                        additional_forward_args,# -----> CAN BE ALSO BATCHED AUTOMATICALLY BY THE LIBRARY ITERATOR\n                        baselines,\n                        feature_mask,\n                    ):\n                        coefs = super().attribute.__wrapped__(\n                            self,\n                            inputs=curr_inps if is_inputs_tuple else curr_inps[0],\n                            target=curr_target,\n                            additional_forward_args=curr_additional_args,\n                            n_samples=n_samples,\n                            perturbations_per_eval=perturbations_per_eval,\n                            baselines=(\n                                curr_baselines if is_inputs_tuple else curr_baselines[0]\n                            ),\n                            feature_mask=(\n                                curr_feature_mask\n                                if is_inputs_tuple\n                                else curr_feature_mask[0]\n                            ),\n                            num_interp_features=num_interp_features,\n                            show_progress=show_progress,\n                            **kwargs,\n                        )\n                        if return_input_shape:\n                            output_list.append(\n                                self._convert_output_shape(\n                                    curr_inps,\n                                    curr_feature_mask,\n                                    coefs,\n                                    num_interp_features,\n                                    is_inputs_tuple,\n                                )\n                            )\n                        else:\n                            output_list.append(coefs.reshape(1, -1))  # type: ignore\n\n                    return _reduce_list(output_list)\n                else:\n                    raise AssertionError(\n                        \"Invalid number of outputs, forward function should return a\"\n                        \"scalar per example or a scalar per input batch.\"\n                    )\n            else:\n                assert perturbations_per_eval == 1, (\n                    \"Perturbations per eval must be 1 when forward function\"\n                    \"returns single value per batch!\"\n                )\n\n        coefs = super().attribute.__wrapped__(\n            self,\n            inputs=inputs,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            baselines=baselines if is_inputs_tuple else baselines[0],\n            feature_mask=feature_mask if is_inputs_tuple else feature_mask[0],\n            num_interp_features=num_interp_features,\n            show_progress=show_progress,\n            **kwargs,\n        )\n        if return_input_shape:\n            # pyre-fixme[7]: Expected `TensorOrTupleOfTensorsGeneric` but got\n            #  `Tuple[Tensor, ...]`.\n            return self._convert_output_shape(\n                formatted_inputs,\n                feature_mask,\n                coefs,\n                num_interp_features,\n                is_inputs_tuple,\n    \n            leading_dim_one=(bsz > 1),\n            )\n        else:\n            return coefs\n\n    @typing.overload\n    def _convert_output_shape(\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: Literal[True],\n        leading_dim_one: bool = False,\n    ) -> Tuple[Tensor, ...]: ...\n\n    @typing.overload\n    def _convert_output_shape(  # type: ignore\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: Literal[False],\n        leading_dim_one: bool = False,\n    ) -> Tensor: ...\n\n    @typing.overload\n    def _convert_output_shape(\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: bool,\n        leading_dim_one: bool = False,\n    ) -> Union[Tensor, Tuple[Tensor, ...]]: ...\n\n    def _convert_output_shape(\n        self,\n        formatted_inp: Tuple[Tensor, ...],\n        feature_mask: Tuple[Tensor, ...],\n        coefs: Tensor,\n        num_interp_features: int,\n        is_inputs_tuple: bool,\n        leading_dim_one: bool = False,\n    ) -> Union[Tensor, Tuple[Tensor, ...]]:\n        coefs = coefs.flatten()\n        attr = [\n            torch.zeros_like(single_inp, dtype=torch.float)\n            for single_inp in formatted_inp\n        ]\n        for tensor_ind in range(len(formatted_inp)):\n            for single_feature in range(num_interp_features):\n                attr[tensor_ind] += (\n                    coefs[single_feature].item()\n                    * (feature_mask[tensor_ind] == single_feature).float()\n                )\n        if leading_dim_one:\n            for i in range(len(attr)):\n                attr[i] = attr[i][0:1]\n        return _format_output(is_inputs_tuple, tuple(attr))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:24.098238Z","iopub.execute_input":"2025-07-03T09:15:24.098450Z","iopub.status.idle":"2025-07-03T09:15:24.127096Z","shell.execute_reply.started":"2025-07-03T09:15:24.098435Z","shell.execute_reply":"2025-07-03T09:15:24.126350Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"#!/usr/bin/env python3\n\n# pyre-strict\n\nfrom typing import Callable, cast, Generator, Optional, Tuple, Union\n\nimport torch\nfrom captum._utils.models.linear_model import SkLearnLinearRegression\nfrom captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric\nfrom captum.attr._core.lime import construct_feature_mask, Lime\nfrom captum.attr._utils.common import _format_input_baseline\nfrom captum.log import log_usage\nfrom torch import Tensor\nfrom torch.distributions.categorical import Categorical\n\n\nclass KernelShapWithMask(LimeWithCustomArgumentToForwardFunc):\n    r\"\"\"\n    Kernel SHAP is a method that uses the LIME framework to compute\n    Shapley Values. Setting the loss function, weighting kernel and\n    regularization terms appropriately in the LIME framework allows\n    theoretically obtaining Shapley Values more efficiently than\n    directly computing Shapley Values.\n\n    More information regarding this method and proof of equivalence\n    can be found in the original paper here:\n    https://arxiv.org/abs/1705.07874\n    \"\"\"\n\n    def __init__(self, forward_func: Callable[..., Tensor]) -> None:\n        r\"\"\"\n        Args:\n\n            forward_func (Callable): The forward function of the model or\n                        any modification of it.\n        \"\"\"\n        Lime.__init__(\n            self,\n            forward_func,\n            interpretable_model=SkLearnLinearRegression(),\n            similarity_func=self.kernel_shap_similarity_kernel,\n            perturb_func=self.kernel_shap_perturb_generator,\n        )\n        self.inf_weight = 1000000.0\n\n    @log_usage(part_of_slo=True)\n    def attribute(  # type: ignore\n        self,\n        inputs: TensorOrTupleOfTensorsGeneric,\n        baselines: BaselineType = None,\n        target: TargetType = None,\n        additional_forward_args: Optional[object] = None,\n        feature_mask: Union[None, Tensor, Tuple[Tensor, ...]] = None,\n        n_samples: int = 25,\n        perturbations_per_eval: int = 1,\n        return_input_shape: bool = True,\n        show_progress: bool = False,\n    ) -> TensorOrTupleOfTensorsGeneric:\n        r\"\"\"\n        This method attributes the output of the model with given target index\n        (in case it is provided, otherwise it assumes that output is a\n        scalar) to the inputs of the model using the approach described above,\n        training an interpretable model based on KernelSHAP and returning a\n        representation of the interpretable model.\n\n        It is recommended to only provide a single example as input (tensors\n        with first dimension or batch size = 1). This is because LIME / KernelShap\n        is generally used for sample-based interpretability, training a separate\n        interpretable model to explain a model's prediction on each individual example.\n\n        A batch of inputs can also be provided as inputs, similar to\n        other perturbation-based attribution methods. In this case, if forward_fn\n        returns a scalar per example, attributions will be computed for each\n        example independently, with a separate interpretable model trained for each\n        example. Note that provided similarity and perturbation functions will be\n        provided each example separately (first dimension = 1) in this case.\n        If forward_fn returns a scalar per batch (e.g. loss), attributions will\n        still be computed using a single interpretable model for the full batch.\n        In this case, similarity and perturbation functions will be provided the\n        same original input containing the full batch.\n\n        The number of interpretable features is determined from the provided\n        feature mask, or if none is provided, from the default feature mask,\n        which considers each scalar input as a separate feature. It is\n        generally recommended to provide a feature mask which groups features\n        into a small number of interpretable features / components (e.g.\n        superpixels in images).\n\n        Args:\n\n            inputs (Tensor or tuple[Tensor, ...]): Input for which KernelShap\n                        is computed. If forward_func takes a single\n                        tensor as input, a single input tensor should be provided.\n                        If forward_func takes multiple tensors as input, a tuple\n                        of the input tensors should be provided. It is assumed\n                        that for all given input tensors, dimension 0 corresponds\n                        to the number of examples, and if multiple input tensors\n                        are provided, the examples must be aligned appropriately.\n            baselines (scalar, Tensor, tuple of scalar, or Tensor, optional):\n                        Baselines define the reference value which replaces each\n                        feature when the corresponding interpretable feature\n                        is set to 0.\n                        Baselines can be provided as:\n\n                        - a single tensor, if inputs is a single tensor, with\n                          exactly the same dimensions as inputs or the first\n                          dimension is one and the remaining dimensions match\n                          with inputs.\n\n                        - a single scalar, if inputs is a single tensor, which will\n                          be broadcasted for each input value in input tensor.\n\n                        - a tuple of tensors or scalars, the baseline corresponding\n                          to each tensor in the inputs' tuple can be:\n\n                          - either a tensor with matching dimensions to\n                            corresponding tensor in the inputs' tuple\n                            or the first dimension is one and the remaining\n                            dimensions match with the corresponding\n                            input tensor.\n\n                          - or a scalar, corresponding to a tensor in the\n                            inputs' tuple. This scalar value is broadcasted\n                            for corresponding input tensor.\n\n                        In the cases when `baselines` is not provided, we internally\n                        use zero scalar corresponding to each input tensor.\n                        Default: None\n            target (int, tuple, Tensor, or list, optional): Output indices for\n                        which surrogate model is trained\n                        (for classification cases,\n                        this is usually the target class).\n                        If the network returns a scalar value per example,\n                        no target index is necessary.\n                        For general 2D outputs, targets can be either:\n\n                        - a single integer or a tensor containing a single\n                          integer, which is applied to all input examples\n\n                        - a list of integers or a 1D tensor, with length matching\n                          the number of examples in inputs (dim 0). Each integer\n                          is applied as the target for the corresponding example.\n\n                        For outputs with > 2 dimensions, targets can be either:\n\n                        - A single tuple, which contains #output_dims - 1\n                          elements. This target index is applied to all examples.\n\n                        - A list of tuples with length equal to the number of\n                          examples in inputs (dim 0), and each tuple containing\n                          #output_dims - 1 elements. Each tuple is applied as the\n                          target for the corresponding example.\n\n                        Default: None\n            additional_forward_args (Any, optional): If the forward function\n                        requires additional arguments other than the inputs for\n                        which attributions should not be computed, this argument\n                        can be provided. It must be either a single additional\n                        argument of a Tensor or arbitrary (non-tuple) type or a\n                        tuple containing multiple additional arguments including\n                        tensors or any arbitrary python types. These arguments\n                        are provided to forward_func in order following the\n                        arguments in inputs.\n                        For a tensor, the first dimension of the tensor must\n                        correspond to the number of examples. It will be\n                        repeated for each of `n_steps` along the integrated\n                        path. For all other types, the given argument is used\n                        for all forward evaluations.\n                        Note that attributions are not computed with respect\n                        to these arguments.\n                        Default: None\n            feature_mask (Tensor or tuple[Tensor, ...], optional):\n                        feature_mask defines a mask for the input, grouping\n                        features which correspond to the same\n                        interpretable feature. feature_mask\n                        should contain the same number of tensors as inputs.\n                        Each tensor should\n                        be the same size as the corresponding input or\n                        broadcastable to match the input tensor. Values across\n                        all tensors should be integers in the range 0 to\n                        num_interp_features - 1, and indices corresponding to the\n                        same feature should have the same value.\n                        Note that features are grouped across tensors\n                        (unlike feature ablation and occlusion), so\n                        if the same index is used in different tensors, those\n                        features are still grouped and added simultaneously.\n                        If None, then a feature mask is constructed which assigns\n                        each scalar within a tensor as a separate feature.\n                        Default: None\n            n_samples (int, optional): The number of samples of the original\n                        model used to train the surrogate interpretable model.\n                        Default: `50` if `n_samples` is not provided.\n            perturbations_per_eval (int, optional): Allows multiple samples\n                        to be processed simultaneously in one call to forward_fn.\n                        Each forward pass will contain a maximum of\n                        perturbations_per_eval * #examples samples.\n                        For DataParallel models, each batch is split among the\n                        available devices, so evaluations on each available\n                        device contain at most\n                        (perturbations_per_eval * #examples) / num_devices\n                        samples.\n                        If the forward function returns a single scalar per batch,\n                        perturbations_per_eval must be set to 1.\n                        Default: 1\n            return_input_shape (bool, optional): Determines whether the returned\n                        tensor(s) only contain the coefficients for each interp-\n                        retable feature from the trained surrogate model, or\n                        whether the returned attributions match the input shape.\n                        When return_input_shape is True, the return type of attribute\n                        matches the input shape, with each element containing the\n                        coefficient of the corresponding interpretable feature.\n                        All elements with the same value in the feature mask\n                        will contain the same coefficient in the returned\n                        attributions. If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpretable model, with length\n                        num_interp_features.\n            show_progress (bool, optional): Displays the progress of computation.\n                        It will try to use tqdm if available for advanced features\n                        (e.g. time estimation). Otherwise, it will fallback to\n                        a simple output of progress.\n                        Default: False\n\n        Returns:\n            *Tensor* or *tuple[Tensor, ...]* of **attributions**:\n            - **attributions** (*Tensor* or *tuple[Tensor, ...]*):\n                        The attributions with respect to each input feature.\n                        If return_input_shape = True, attributions will be\n                        the same size as the provided inputs, with each value\n                        providing the coefficient of the corresponding\n                        interpretale feature.\n                        If return_input_shape is False, a 1D\n                        tensor is returned, containing only the coefficients\n                        of the trained interpreatable models, with length\n                        num_interp_features.\n        Examples::\n            >>> # SimpleClassifier takes a single input tensor of size Nx4x4,\n            >>> # and returns an Nx3 tensor of class probabilities.\n            >>> net = SimpleClassifier()\n\n            >>> # Generating random input with size 1 x 4 x 4\n            >>> input = torch.randn(1, 4, 4)\n\n            >>> # Defining KernelShap interpreter\n            >>> ks = KernelShap(net)\n            >>> # Computes attribution, with each of the 4 x 4 = 16\n            >>> # features as a separate interpretable feature\n            >>> attr = ks.attribute(input, target=1, n_samples=200)\n\n            >>> # Alternatively, we can group each 2x2 square of the inputs\n            >>> # as one 'interpretable' feature and perturb them together.\n            >>> # This can be done by creating a feature mask as follows, which\n            >>> # defines the feature groups, e.g.:\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 0 | 0 | 1 | 1 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # | 2 | 2 | 3 | 3 |\n            >>> # +---+---+---+---+\n            >>> # With this mask, all inputs with the same value are set to their\n            >>> # baseline value, when the corresponding binary interpretable\n            >>> # feature is set to 0.\n            >>> # The attributions can be calculated as follows:\n            >>> # feature mask has dimensions 1 x 4 x 4\n            >>> feature_mask = torch.tensor([[[0,0,1,1],[0,0,1,1],\n            >>>                             [2,2,3,3],[2,2,3,3]]])\n\n            >>> # Computes KernelSHAP attributions with feature mask.\n            >>> attr = ks.attribute(input, target=1, feature_mask=feature_mask)\n        \"\"\"\n        formatted_inputs, baselines = _format_input_baseline(inputs, baselines)\n        feature_mask, num_interp_features = construct_feature_mask(\n            feature_mask, formatted_inputs\n        )\n        num_features_list = torch.arange(num_interp_features, dtype=torch.float)\n        denom = num_features_list * (num_interp_features - num_features_list)\n        probs = torch.tensor((num_interp_features - 1)) / denom\n        probs[0] = 0.0\n        return self._attribute_kwargs(\n            inputs=inputs,\n            baselines=baselines,\n            target=target,\n            additional_forward_args=additional_forward_args,\n            feature_mask=feature_mask,\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            return_input_shape=return_input_shape,\n            num_select_distribution=Categorical(probs),\n            show_progress=show_progress,\n        )\n\n    # pyre-fixme[24] Generic type `Callable` expects 2 type parameters.\n    def attribute_future(self) -> Callable:\n        r\"\"\"\n        This method is not implemented for KernelShap.\n        \"\"\"\n        raise NotImplementedError(\"attribute_future is not implemented for KernelShap\")\n\n    def kernel_shap_similarity_kernel(\n        self,\n        _,\n        __,\n        interpretable_sample: Tensor,\n        **kwargs: object,\n    ) -> Tensor:\n        assert (\n            \"num_interp_features\" in kwargs\n        ), \"Must provide num_interp_features to use default similarity kernel\"\n        num_selected_features = int(interpretable_sample.sum(dim=1).item())\n        num_features = kwargs[\"num_interp_features\"]\n        if num_selected_features == 0 or num_selected_features == num_features:\n            # weight should be theoretically infinite when\n            # num_selected_features = 0 or num_features\n            # enforcing that trained linear model must satisfy\n            # end-point criteria. In practice, it is sufficient to\n            # make this weight substantially larger so setting this\n            # weight to 1000000 (all other weights are 1).\n            similarities = self.inf_weight\n        else:\n            similarities = 1.0\n        return torch.tensor([similarities])\n\n    def kernel_shap_perturb_generator(\n        self,\n        original_inp: Union[Tensor, Tuple[Tensor, ...]],\n        **kwargs: object,\n    ) -> Generator[Tensor, None, None]:\n        r\"\"\"\n        Perturbations are sampled by the following process:\n         - Choose k (number of selected features), based on the distribution\n                p(k) = (M - 1) / (k * (M - k))\n\n            where M is the total number of features in the interpretable space\n\n         - Randomly select a binary vector with k ones, each sample is equally\n            likely. This is done by generating a random vector of normal\n            values and thresholding based on the top k elements.\n\n         Since there are M choose k vectors with k ones, this weighted sampling\n         is equivalent to applying the Shapley kernel for the sample weight,\n         defined as:\n         k(M, k) = (M - 1) / (k * (M - k) * (M choose k))\n        \"\"\"\n        assert (\n            \"num_select_distribution\" in kwargs and \"num_interp_features\" in kwargs\n        ), (\n            \"num_select_distribution and num_interp_features are necessary\"\n            \" to use kernel_shap_perturb_func\"\n        )\n        if isinstance(original_inp, Tensor):\n            device = original_inp.device\n        else:\n            device = original_inp[0].device\n        num_features = cast(int, kwargs[\"num_interp_features\"])\n        yield torch.ones(1, num_features, device=device, dtype=torch.long)\n        yield torch.zeros(1, num_features, device=device, dtype=torch.long)\n        while True:\n            num_selected_features = cast(\n                Categorical, kwargs[\"num_select_distribution\"]\n            ).sample()\n            rand_vals = torch.randn(1, num_features)\n            threshold = torch.kthvalue(\n                rand_vals, num_features - num_selected_features\n            ).values.item()\n            yield (rand_vals > threshold).to(device=device).long()\n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:24.127968Z","iopub.execute_input":"2025-07-03T09:15:24.128278Z","iopub.status.idle":"2025-07-03T09:15:24.146967Z","shell.execute_reply.started":"2025-07-03T09:15:24.128260Z","shell.execute_reply":"2025-07-03T09:15:24.146118Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"\"\"\"    # Part 3: Quick sanity-check of KernelShapWithMask on 8³ toy data\n    \n    import torch\n    from torch import Tensor\n    \n    # 1) Dummy predictor that just counts perturbed voxels (so SHAP ≈ region size)\n    class DummyPredictor(torch.nn.Module):\n        def forward(self, volume: Tensor, pert_mask: Tensor) -> Tensor:\n            # volume: (B,C,X,Y,Z), pert_mask: (B,C,X,Y,Z)\n            # Our “score” is negative #perturbed voxels per example\n            return -(pert_mask.sum(dim=[1,2,3,4]).float())  # shape (B,)\n    \n    # 2) Synthesise an 8×8×8 volume and a 4-supervoxel map\n    vol = torch.randn(1, 1, 8, 8, 8)                  # (B=1, C=1, X=8,Y=8,Z=8)\n    sv_map = torch.randint(0, 4, (8, 8, 8))           # labels 0–3\n    \n    \n    # 4) Instantiate explainer (assumes KernelShapWithMask is already in scope)\n    explainer = KernelShapWithMask(\n        forward_func=DummyPredictor\n    )\n    attrs = explainer.attribute(\n            inputs = vol,\n            baselines = 0.0,\n            feature_mask= sv_map,\n            n_samples = 2,\n            show_progress = True\n    )\n    \n    print(\"Attributions shape:\", attrs.shape)  \n    # should be (1,1,8,8,8), and values ≈ size of each supervoxel region\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:24.147881Z","iopub.execute_input":"2025-07-03T09:15:24.148161Z","iopub.status.idle":"2025-07-03T09:15:24.165454Z","shell.execute_reply.started":"2025-07-03T09:15:24.148136Z","shell.execute_reply":"2025-07-03T09:15:24.164510Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'    # Part 3: Quick sanity-check of KernelShapWithMask on 8³ toy data\\n    \\n    import torch\\n    from torch import Tensor\\n    \\n    # 1) Dummy predictor that just counts perturbed voxels (so SHAP ≈ region size)\\n    class DummyPredictor(torch.nn.Module):\\n        def forward(self, volume: Tensor, pert_mask: Tensor) -> Tensor:\\n            # volume: (B,C,X,Y,Z), pert_mask: (B,C,X,Y,Z)\\n            # Our “score” is negative #perturbed voxels per example\\n            return -(pert_mask.sum(dim=[1,2,3,4]).float())  # shape (B,)\\n    \\n    # 2) Synthesise an 8×8×8 volume and a 4-supervoxel map\\n    vol = torch.randn(1, 1, 8, 8, 8)                  # (B=1, C=1, X=8,Y=8,Z=8)\\n    sv_map = torch.randint(0, 4, (8, 8, 8))           # labels 0–3\\n    \\n    \\n    # 4) Instantiate explainer (assumes KernelShapWithMask is already in scope)\\n    explainer = KernelShapWithMask(\\n        forward_func=DummyPredictor\\n    )\\n    attrs = explainer.attribute(\\n            inputs = vol,\\n            baselines = 0.0,\\n            feature_mask= sv_map,\\n            n_samples = 2,\\n            show_progress = True\\n    )\\n    \\n    print(\"Attributions shape:\", attrs.shape)  \\n    # should be (1,1,8,8,8), and values ≈ size of each supervoxel region\\n'"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"### try to use it in our pipeline","metadata":{}},{"cell_type":"code","source":"# define an utility for annoying nnunetv2 preprocessing\ndef nnunetv2_default_preprocessing(ct_img_path, predictor, dataset_json_path) -> np.ndarray:\n    plans_manager = predictor.plans_manager\n    configuration_manager = predictor.configuration_manager\n    \n    preprocessor = configuration_manager.preprocessor_class(verbose=False)\n    rw = plans_manager.image_reader_writer_class()\n    if callable(rw) and not hasattr(rw, \"read_images\"):\n        rw = rw()\n    img_np, img_props = rw.read_images([str(ct_img_path)])\n    \n    preprocessed, _, _ = preprocessor.run_case_npy(\n        img_np, seg=None, properties=img_props,\n        plans_manager=plans_manager,\n        configuration_manager=configuration_manager,\n        dataset_json=dataset_json_path\n    )\n    return preprocessed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:24.166457Z","iopub.execute_input":"2025-07-03T09:15:24.166930Z","iopub.status.idle":"2025-07-03T09:15:24.181843Z","shell.execute_reply.started":"2025-07-03T09:15:24.166910Z","shell.execute_reply":"2025-07-03T09:15:24.181089Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:24.182888Z","iopub.execute_input":"2025-07-03T09:15:24.183240Z","iopub.status.idle":"2025-07-03T09:15:24.195453Z","shell.execute_reply.started":"2025-07-03T09:15:24.183213Z","shell.execute_reply":"2025-07-03T09:15:24.194639Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# 2) Initialise predictor ------------------------\npredictor = CustomNNUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=False, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=False,\n    verbose_preprocessing=False,\n    allow_tqdm=False #it interfere with SHAP loading bar\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir_readonly,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:24.196362Z","iopub.execute_input":"2025-07-03T09:15:24.196686Z","iopub.status.idle":"2025-07-03T09:15:25.206846Z","shell.execute_reply.started":"2025-07-03T09:15:24.196665Z","shell.execute_reply":"2025-07-03T09:15:25.205921Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import pickle as pkl\n\nwith open(\"baseline_output_dictionary_cache.pkl\", \"rb\") as f:\n    baseline_pred_cache = pkl.load(f)\n\n\"\"\"# code to get our baseline prediction patch dictionary\niterator = SHAPPredictionIterator(plan, predictor,\n                                               skip_completed_ids=acc.completed_ids,\n                                               cache_sw_inference=True,\n                                               verbose=True)\nbaseline_pred_cache = iterator._baseline_output_dictionary\nimport pickle as pkl\n# Write to file\nwith open(\"baseline_output_dictionary_cache.pkl\", \"wb\") as f:\n    pkl.dump(baseline_pred_cache, f)\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:25.208219Z","iopub.execute_input":"2025-07-03T09:15:25.208589Z","iopub.status.idle":"2025-07-03T09:15:33.885114Z","shell.execute_reply.started":"2025-07-03T09:15:25.208559Z","shell.execute_reply":"2025-07-03T09:15:33.884213Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'# code to get our baseline prediction patch dictionary\\niterator = SHAPPredictionIterator(plan, predictor,\\n                                               skip_completed_ids=acc.completed_ids,\\n                                               cache_sw_inference=True,\\n                                               verbose=True)\\nbaseline_pred_cache = iterator._baseline_output_dictionary\\nimport pickle as pkl\\n# Write to file\\nwith open(\"baseline_output_dictionary_cache.pkl\", \"wb\") as f:\\n    pkl.dump(baseline_pred_cache, f)\\n\\n'"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"\"\"\"example = baseline_pred_cache[((None, None, None), (0, 72, None), (0, 160, None), (0, 160, None))]\nprint(example.shape)\nC,D,H,W = example.shape\nprint(D*H*W)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.885998Z","iopub.execute_input":"2025-07-03T09:15:33.886206Z","iopub.status.idle":"2025-07-03T09:15:33.891349Z","shell.execute_reply.started":"2025-07-03T09:15:33.886191Z","shell.execute_reply":"2025-07-03T09:15:33.890722Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'example = baseline_pred_cache[((None, None, None), (0, 72, None), (0, 160, None), (0, 160, None))]\\nprint(example.shape)\\nC,D,H,W = example.shape\\nprint(D*H*W)'"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"\"\"\"# (a) load + preprocess the volume  (1, C, D, H, W) – nnU-Net order\nnii_path = \"input_volume_39.nii.gz\"\ndataset_json_path = Path(model_dir_readonly) / \"dataset.json\"\n\nvolume_np = nnunetv2_default_preprocessing(nii_path, predictor, dataset_json_path)\n\nvolume = torch.from_numpy(volume_np).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\n\nprint(\"Volume shape:\", volume.shape)                # (1, C, D, H, W)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.892286Z","iopub.execute_input":"2025-07-03T09:15:33.892563Z","iopub.status.idle":"2025-07-03T09:15:33.905435Z","shell.execute_reply.started":"2025-07-03T09:15:33.892545Z","shell.execute_reply":"2025-07-03T09:15:33.904730Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'# (a) load + preprocess the volume  (1, C, D, H, W) – nnU-Net order\\nnii_path = \"input_volume_39.nii.gz\"\\ndataset_json_path = Path(model_dir_readonly) / \"dataset.json\"\\n\\nvolume_np = nnunetv2_default_preprocessing(nii_path, predictor, dataset_json_path)\\n\\nvolume = torch.from_numpy(volume_np).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\\n\\nprint(\"Volume shape:\", volume.shape)                # (1, C, D, H, W)'"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"\"\"\"# (b) super-voxel / organ-id map  (W, H, D)\norgan_mask_path = join(\n    nnUNet_raw, \"total_segmentator_structures\",\n    \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\",\n)\nsv_np = nib.load(organ_mask_path).get_fdata()\nsv_np = np.transpose(sv_np, (2, 1, 0))                 # match (D,H,W)\n# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\nsv_values, indexes = np.unique(sv_np, return_inverse=True)\n\nsv_np = indexes.reshape(sv_np.shape)\nprint(np.unique(sv_np))\n\n# IMPORTANT 🔸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\nsupervox = torch.from_numpy(sv_np).long().to(device)   # (D,H,W)\n\nprint(\"Mask shape:\", supervox.shape)                      # (D, H, W)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.906319Z","iopub.execute_input":"2025-07-03T09:15:33.906705Z","iopub.status.idle":"2025-07-03T09:15:33.919364Z","shell.execute_reply.started":"2025-07-03T09:15:33.906685Z","shell.execute_reply":"2025-07-03T09:15:33.918729Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'# (b) super-voxel / organ-id map  (W, H, D)\\norgan_mask_path = join(\\n    nnUNet_raw, \"total_segmentator_structures\",\\n    \"AUTOMI_00039_0000\", \"mask_mask_add_input_20_total_segmentator.nii\",\\n)\\nsv_np = nib.load(organ_mask_path).get_fdata()\\nsv_np = np.transpose(sv_np, (2, 1, 0))                 # match (D,H,W)\\n# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\\nsv_values, indexes = np.unique(sv_np, return_inverse=True)\\n\\nsv_np = indexes.reshape(sv_np.shape)\\nprint(np.unique(sv_np))\\n\\n# IMPORTANT 🔸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\\nsupervox = torch.from_numpy(sv_np).long().to(device)   # (D,H,W)\\n\\nprint(\"Mask shape:\", supervox.shape)                      # (D, H, W)'"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"\"\"\"print(sv_np.shape)\nprint(np.prod(sv_np.shape))\nprint(np.sum(sv_np != 0))\nprint(np.sum(sv_np == 0))\nassert np.sum(sv_np != 0) + np.sum(sv_np == 0) == np.prod(sv_np.shape)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.920249Z","iopub.execute_input":"2025-07-03T09:15:33.920547Z","iopub.status.idle":"2025-07-03T09:15:33.936634Z","shell.execute_reply.started":"2025-07-03T09:15:33.920522Z","shell.execute_reply":"2025-07-03T09:15:33.935824Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'print(sv_np.shape)\\nprint(np.prod(sv_np.shape))\\nprint(np.sum(sv_np != 0))\\nprint(np.sum(sv_np == 0))\\nassert np.sum(sv_np != 0) + np.sum(sv_np == 0) == np.prod(sv_np.shape)'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"# ------------------------------------------------------------\n# ❷  Forward wrapper that nnU-Net expects\n# ------------------------------------------------------------\n\n@torch.inference_mode()\ndef forward_segmentation_output_to_explain(\n        input_image:         torch.Tensor,\n        perturbation_mask:   torch.BoolTensor,\n        baseline_prediction_dict: dict\n) -> torch.Tensor:           # returns a scalar per sample\n    \"\"\"\n    Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n    by the network – adapt to your real metric as needed.\n    \"\"\"\n    logits = predictor.predict_sliding_window_return_logits_with_caching(\n        input_image, perturbation_mask, baseline_prediction_dict,\n    )                              # (C, D, H, W)\n    seg_mask = torch.argmax(logits, dim=0)          # (D,H,W)\n    D, H, W = seg_mask.shape\n    aggregate = torch.sum(logits[1].double() * seg_mask) / (D*H*W)  # normalize to avoid overflows in SHAP\n\n    return aggregate\n\n# c) wrap your cached‐forward method:\nexplainer = KernelShapWithMask(\n    forward_func=lambda vol, mask: forward_segmentation_output_to_explain(\n        input_image=vol,\n        perturbation_mask=mask,\n        baseline_prediction_dict=baseline_pred_cache)\n)\n\"\"\"\n# d) compute SHAP\nattr = explainer.attribute(\n    inputs=volume,       # (1,C,D,H,W)\n    baselines=0.0,\n    feature_mask=supervox,\n    n_samples=50,    \n    return_input_shape=True,\n    show_progress=True,\n)\nprint(\"Attributions:\", attr.shape)  # → (1,C,D,H,W)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.937532Z","iopub.execute_input":"2025-07-03T09:15:33.937857Z","iopub.status.idle":"2025-07-03T09:15:33.951925Z","shell.execute_reply.started":"2025-07-03T09:15:33.937835Z","shell.execute_reply":"2025-07-03T09:15:33.951248Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'\\n# d) compute SHAP\\nattr = explainer.attribute(\\n    inputs=volume,       # (1,C,D,H,W)\\n    baselines=0.0,\\n    feature_mask=supervox,\\n    n_samples=50,    \\n    return_input_shape=True,\\n    show_progress=True,\\n)\\nprint(\"Attributions:\", attr.shape)  # → (1,C,D,H,W)\\n'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"\"\"\"for x,y,_ in explainer.dataset:\n    print(x)\n    \nformatted_inputs, baselines = _format_input_baseline(inputs=volume, baselines=0.0)\nfeature_mask, num_interp_features = construct_feature_mask(\n            feature_mask=supervox, formatted_inputs=formatted_inputs\n        )\nprint(num_interp_features)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.952818Z","iopub.execute_input":"2025-07-03T09:15:33.953097Z","iopub.status.idle":"2025-07-03T09:15:33.969625Z","shell.execute_reply.started":"2025-07-03T09:15:33.953079Z","shell.execute_reply":"2025-07-03T09:15:33.968892Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'for x,y,_ in explainer.dataset:\\n    print(x)\\n    \\nformatted_inputs, baselines = _format_input_baseline(inputs=volume, baselines=0.0)\\nfeature_mask, num_interp_features = construct_feature_mask(\\n            feature_mask=supervox, formatted_inputs=formatted_inputs\\n        )\\nprint(num_interp_features)'"},"metadata":{}}],"execution_count":44},{"cell_type":"markdown","source":"### Move back attribution to physical space and save the nifty","metadata":{}},{"cell_type":"code","source":"\"\"\"affine = nib.load(nii_path).affine\nattr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\nattr_img = nib.Nifti1Image(attr_postprocessed, affine)\nnib.save(attr_img, 'attribution_map-n_samples=50.nii.gz')\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.970530Z","iopub.execute_input":"2025-07-03T09:15:33.970838Z","iopub.status.idle":"2025-07-03T09:15:33.982482Z","shell.execute_reply.started":"2025-07-03T09:15:33.970819Z","shell.execute_reply":"2025-07-03T09:15:33.981947Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"\"affine = nib.load(nii_path).affine\\nattr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\\nattr_img = nib.Nifti1Image(attr_postprocessed, affine)\\nnib.save(attr_img, 'attribution_map-n_samples=50.nii.gz')\""},"metadata":{}}],"execution_count":45},{"cell_type":"markdown","source":"# Next step: define regular, fixed size superpixels and try to compute the attributions of each of them\nSo we also need to define a metric to compare, since segmentation explanations, differently from classification, is intrinsically ambiguous. For example, let's select a priori a single region of the segmentation output, and use the average of these pixels to compute the impact of perturbations.","metadata":{}},{"cell_type":"markdown","source":"### Animation Plot util (interactive won't work on a notebook with matplotlib inline-mode","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, FFMpegWriter, PillowWriter\nfrom IPython.display import HTML, display\n\ndef visualize_volume(\n    volume: np.ndarray,\n    mode: str = 'interactive',\n    plane: str = 'axial',\n    step: int = 1,\n    cmap = 'gray',\n    gif_filename: str = 'volume_animation.gif',\n    mp4_filename: str = 'volume_animation.mp4',\n    interval: int = 100\n):\n    \"\"\"\n    Visualize a 3D volume either as an inline (HTML5) animation or as a saved GIF/MP4 file,\n    with optional subsampling of slices (step).\n\n    Parameters\n    ----------\n    volume : np.ndarray\n        3D NumPy array (shape: X × Y × Z).\n    mode : str, optional\n        - 'interactive': render a lightweight HTML5 video inline.\n        - 'gif': save as GIF to `gif_filename` and display.\n        - 'mp4': save as MP4 to `mp4_filename` and display.\n      Default is 'interactive'.\n    plane : str, optional\n        One of {'axial', 'coronal', 'sagittal'}. Determines slicing orientation:\n        - 'axial':   slices = volume[:, :, idx]\n        - 'coronal': slices = volume[:, idx, :]\n        - 'sagittal': slices = volume[idx, :, :]\n      Default is 'axial'.\n    step : int, optional\n        Take every `step`-th slice along the chosen axis. Must be ≥ 1.\n        E.g. step=2 shows slices 0,2,4,… instead of 0,1,2,3,… \n        (reduces #frames if your volume has many slices).\n      Default is 1.\n    gif_filename : str, optional\n        If mode='gif', save the animation to this file.\n      Default is 'volume_animation.gif'.\n    mp4_filename : str, optional\n        If mode='mp4', save the animation to this file.\n      Default is 'volume_animation.mp4'.\n    interval : int, optional\n        Milliseconds between frames in the animation. Lower → faster playback.\n      Default is 100 (i.e. 10 FPS).\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid (e.g. volume not 3D, unknown mode/plane, or step < 1).\n    \"\"\"\n\n    # 1) Validate inputs\n    if not isinstance(volume, np.ndarray) or volume.ndim != 3:\n        raise ValueError(\"`volume` must be a 3D NumPy array.\")\n    if mode not in {'interactive', 'gif', 'mp4'}:\n        raise ValueError(\"`mode` must be one of {'interactive', 'gif', 'mp4'}.\")\n    if plane not in {'axial', 'coronal', 'sagittal'}:\n        raise ValueError(\"`plane` must be one of {'axial', 'coronal', 'sagittal'}.\")\n    if not (isinstance(step, int) and step >= 1):\n        raise ValueError(\"`step` must be an integer ≥ 1.\")\n\n    # 2) Decide how to slice + total number of (subsampled) frames\n    if plane == 'axial':\n        get_slice = lambda vol, idx: np.rot90(vol[:, :, idx])\n        full_num = volume.shape[2]\n    elif plane == 'coronal':\n        get_slice = lambda vol, idx: np.rot90(vol[:, idx, :])\n        full_num = volume.shape[1]\n    else:  # sagittal\n        get_slice = lambda vol, idx: np.rot90(vol[idx, :, :])\n        full_num = volume.shape[0]\n\n    # Build a list of indices: [0, step, 2*step, …] but not exceeding full_num-1\n    indices = list(range(0, full_num, step))\n    num_frames = len(indices)\n\n    # 3) Set up matplotlib figure once\n    fig, ax = plt.subplots(figsize=(6,6))\n    ax.axis('off')\n\n    # Show the first frame\n    first_img = get_slice(volume, indices[0])\n    vmin, vmax = np.percentile(first_img, (1, 99))\n    im = ax.imshow(first_img, cmap=cmap, vmin=vmin, vmax=vmax)\n    title = ax.set_title(f\"{plane.capitalize()} Slice 1/{num_frames}\", fontsize=14)\n\n    def _update(frame_idx):\n        \"\"\"\n        frame_idx runs from 0 to num_frames-1. We map it to the actual voxel index.\n        \"\"\"\n        actual_idx = indices[frame_idx]\n        slice_img = get_slice(volume, actual_idx)\n        vmin, vmax = np.percentile(slice_img, (1, 99))\n        im.set_data(slice_img)\n        im.set_clim(vmin, vmax)\n        title.set_text(f\"{plane.capitalize()} Slice {frame_idx+1}/{num_frames}\")\n        return (im, title)\n\n    # 4) Build the FuncAnimation\n    anim = FuncAnimation(\n        fig,\n        _update,\n        frames=range(num_frames),\n        interval=interval,\n        blit=True\n    )\n\n    # 5) Depending on mode, either render inline or save to file\n    if mode == 'interactive':\n        # Convert to JSHTML and display inline.\n        # This is (relatively) lightweight compared to embedding 300+ full-size frames.\n        html_widget = HTML(anim.to_jshtml())\n        display(html_widget)\n\n    elif mode == 'gif':\n        # Save as GIF. You can adjust fps=1000//interval if desired.\n        gif_writer = PillowWriter(fps=max(1, 1000 // interval))\n        anim.save(gif_filename, writer=gif_writer)\n        plt.close(fig)\n        display(HTML(f'<img src=\"{gif_filename}\" />'))\n\n    else:  # mode == 'mp4'\n        # Save as MP4 using FFMpegWriter (usually more compact than GIF).\n        # Kaggle has ffmpeg installed, so this should work out of the box.\n        fps = max(1, 1000 // interval)\n        mp4_writer = FFMpegWriter(fps=fps, codec='libx264')\n        anim.save(mp4_filename, writer=mp4_writer)\n        plt.close(fig)\n        display(HTML(f'<video controls src=\"{mp4_filename}\" width=\"512\"></video>'))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:33.983276Z","iopub.execute_input":"2025-07-03T09:15:33.983514Z","iopub.status.idle":"2025-07-03T09:15:34.010671Z","shell.execute_reply.started":"2025-07-03T09:15:33.983497Z","shell.execute_reply":"2025-07-03T09:15:34.010002Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import nibabel as nib\n\nct_img_path = join(nnUNet_raw, \"imagesTr\", \"AUTOMI_00039_0000.nii\")\nnii = nib.load(ct_img_path)\nvolume_data = nii.get_fdata()  # shape: (512, 512, 283)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:34.011567Z","iopub.execute_input":"2025-07-03T09:15:34.011853Z","iopub.status.idle":"2025-07-03T09:15:34.198343Z","shell.execute_reply.started":"2025-07-03T09:15:34.011835Z","shell.execute_reply":"2025-07-03T09:15:34.197464Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"#example usage GIF\n\"\"\"visualize_volume(\n    volume=volume_data,\n    mode='gif',\n    plane='coronal',\n    step=2, \n    gif_filename='fast.gif',\n    interval=80     # faster playback (~12.5 FPS)\n)\"\"\"\n#example usage MP4\n\"\"\"visualize_volume(\n    volume=volume_data,\n    mode='mp4',\n    plane='axial',\n    step=1, \n    mp4_filename='axial_full.mp4',\n    interval=80\n)\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:34.199291Z","iopub.execute_input":"2025-07-03T09:15:34.199612Z","iopub.status.idle":"2025-07-03T09:15:34.205556Z","shell.execute_reply.started":"2025-07-03T09:15:34.199589Z","shell.execute_reply":"2025-07-03T09:15:34.204811Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"\"visualize_volume(\\n    volume=volume_data,\\n    mode='mp4',\\n    plane='axial',\\n    step=1, \\n    mp4_filename='axial_full.mp4',\\n    interval=80\\n)\""},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"### We observe out of ROI regions (CT machinery, empty sections) => We need to tessellate the preprocessed volume, otherwise we won't have a match between the network input and our attributions mapping.","metadata":{}},{"cell_type":"code","source":"ct_img_path = \"input_volume_39.nii.gz\"#join(nnUNet_raw, \"imagesTr\", \"AUTOMI_00000_0000.nii\")\nnii = nib.load(ct_img_path)\n#volume_data = nii.get_fdata()  # shape: (512, 512, 283)\n\nplans_manager = predictor.plans_manager\nconfiguration_manager = predictor.configuration_manager\ndataset_json = Path(model_dir_readonly) / \"dataset.json\"\n\npreprocessor = configuration_manager.preprocessor_class(verbose=False)\nrw = plans_manager.image_reader_writer_class()\nif callable(rw) and not hasattr(rw, \"read_images\"):\n    rw = rw()\nimg_np, img_props = rw.read_images([str(ct_img_path)])\n\npreprocessed, _, _ = preprocessor.run_case_npy(\n    img_np, seg=None, properties=img_props,\n    plans_manager=plans_manager,\n    configuration_manager=configuration_manager,\n    dataset_json=dataset_json\n)\n# preprocessed image has batch dimension added, and it's transposed\npreprocessed = preprocessed.squeeze().transpose([2,1,0])\n\n\"\"\"visualize_volume(\n    volume=preprocessed,\n    mode='gif',\n    plane='coronal',\n    step=2, \n    gif_filename='fast.gif',\n    interval=80     # faster playback (~12.5 FPS)\n)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:34.206343Z","iopub.execute_input":"2025-07-03T09:15:34.206621Z","iopub.status.idle":"2025-07-03T09:15:36.650599Z","shell.execute_reply.started":"2025-07-03T09:15:34.206600Z","shell.execute_reply":"2025-07-03T09:15:36.649937Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"\"visualize_volume(\\n    volume=preprocessed,\\n    mode='gif',\\n    plane='coronal',\\n    step=2, \\n    gif_filename='fast.gif',\\n    interval=80     # faster playback (~12.5 FPS)\\n)\""},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"## It seems that actually it just applies a filter that hides some artefact noise at the beginning in this case. In principle, nnunetv2 preprocessing can crop to remove empty background, but it does this only for completely 0 voxels, or when a segmentation is passed in cascade-mode (not our case). Anyway, it's important that we apply tessellation on the preprocessed volume to be sure.","metadata":{}},{"cell_type":"code","source":"print(img_props)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:36.651357Z","iopub.execute_input":"2025-07-03T09:15:36.651567Z","iopub.status.idle":"2025-07-03T09:15:36.656557Z","shell.execute_reply.started":"2025-07-03T09:15:36.651553Z","shell.execute_reply":"2025-07-03T09:15:36.655693Z"}},"outputs":[{"name":"stdout","text":"{'sitk_stuff': {'spacing': (1.171875, 1.171875, 5.0), 'origin': (-300.0, -186.10000610351562, -1739.5), 'direction': (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)}, 'spacing': [5.0, 1.171875, 1.171875], 'shape_before_cropping': (283, 512, 512), 'bbox_used_for_cropping': [[0, 283], [0, 512], [0, 512]], 'shape_after_cropping_and_before_resampling': (283, 512, 512)}\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"## There is not cropping indeed","metadata":{}},{"cell_type":"markdown","source":"## Now create a superpixel mask (several algorithms tested)","metadata":{}},{"cell_type":"markdown","source":"### 1. Cubic","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport math\n\ndef create_supervoxel_mask(volume_shape, side_length):\n    \"\"\"\n    Partition a 3D volume into cubic supervoxels of given side length, and\n    return a mask array where each voxel is labeled by an integer supervoxel ID.\n\n    Parameters\n    ----------\n    volume_shape : tuple of ints (D, H, W)\n        Shape of the 3D volume in voxels (depth, height, width).\n    side_length : int\n        Desired side length (in voxels) of each cubic supervoxel.\n\n    Returns\n    -------\n    supervoxel_mask : np.ndarray, shape (D, H, W), dtype=np.int32\n        Integer mask such that all voxels belonging to the same L×L×L block\n        have the same label. Labels start at 1 and increase to the total number\n        of supervoxels.\n    \"\"\"\n    D, H, W = volume_shape\n    # Compute how many blocks fit along each axis (using ceiling for remainders)\n    n_blocks_d = math.ceil(D / side_length)\n    n_blocks_h = math.ceil(H / side_length)\n    n_blocks_w = math.ceil(W / side_length)\n\n    # Initialize mask\n    supervoxel_mask = np.zeros((D, H, W), dtype=np.int32)\n\n    # Assign labels: iterate over block indices\n    label = 1\n    for bd in range(n_blocks_d):\n        start_d = bd * side_length\n        end_d = min((bd + 1) * side_length, D)\n\n        for bh in range(n_blocks_h):\n            start_h = bh * side_length\n            end_h = min((bh + 1) * side_length, H)\n\n            for bw in range(n_blocks_w):\n                start_w = bw * side_length\n                end_w = min((bw + 1) * side_length, W)\n\n                # Assign this block the current label\n                supervoxel_mask[\n                    start_d:end_d,\n                    start_h:end_h,\n                    start_w:end_w\n                ] = label\n                label += 1\n\n    return supervoxel_mask\n\n# 1. Take the preprocessed CT volume\nct_data = preprocessed.squeeze().transpose([2,1,0])  # shape: (D, H, W)\nprint(\"Shape: \", ct_data.shape)\n\n# 2. Decide on a supervoxel side length (in voxels)\nside_length = 32\n\n# 3. Create the supervoxel mask\n#    The mask will have the same spatial shape as the CT volume.\n#supervoxel_mask = create_supervoxel_mask(ct_data.shape, side_length)","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-03T09:15:36.657431Z","iopub.execute_input":"2025-07-03T09:15:36.657607Z","iopub.status.idle":"2025-07-03T09:15:36.672096Z","shell.execute_reply.started":"2025-07-03T09:15:36.657593Z","shell.execute_reply":"2025-07-03T09:15:36.671283Z"}},"outputs":[{"name":"stdout","text":"Shape:  (283, 512, 512)\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"### 2. Hexagonal prism","metadata":{}},{"cell_type":"code","source":"import numpy as np, math\nfrom scipy.spatial import cKDTree\nimport matplotlib.pyplot as plt\n\ndef hex_prism_mask(shape, r=24, z_thick=None, orientation='flat',\n                   return_edges=False):\n    \"\"\"\n    shape        : (D,H,W) of volume\n    r            : hex side length (voxels)\n    z_thick      : slab height; None -> whole D (prisms)\n    orientation  : 'flat' (flat-top)  or  'pointy' (pointy-top)\n    return_edges : if True also returns a 2-D bool array with cell borders\n    \"\"\"\n    D, H, W = shape\n    z_thick  = D if z_thick is None else int(z_thick)\n\n    if orientation == 'flat':\n        dx, dy = 3*r, math.sqrt(3)*r\n        shift  = lambda col: (dy/2) if (col & 1) else 0\n        n_cols = int(W/dx)+3; n_rows = int(H/dy)+3\n        centres = [(col*dx, row*dy+shift(col))\n                   for col in range(-1, n_cols)\n                   for row in range(-1, n_rows)]\n    else:  # pointy-top\n        dx, dy = math.sqrt(3)*r, 3*r\n        shift  = lambda row: (dx/2) if (row & 1) else 0\n        n_cols = int(W/dx)+3; n_rows = int(H/dy)+3\n        centres = [(col*dx+shift(row), row*dy)\n                   for row in range(-1, n_rows)\n                   for col in range(-1, n_cols)]\n\n    centres = np.array(centres, float)\n    m = r\n    keep = ((centres[:,0] >= -m)&(centres[:,0]<=W-1+m)&\n            (centres[:,1] >= -m)&(centres[:,1]<=H-1+m))\n    centres = centres[keep]\n\n    Y,X = np.mgrid[:H,:W]\n    lab = cKDTree(centres).query(np.c_[X.ravel(),Y.ravel()],1)[1]\n    hex2d = lab.reshape(H,W).astype(np.int32)+1      # labels start at 1\n\n    mask3d = np.zeros((D,H,W), np.int32)\n    off = 0\n    for z0 in range(0, D, z_thick):\n        mask3d[z0:z0+z_thick] = hex2d + off\n        off += hex2d.max()\n\n    if not return_edges:\n        return mask3d\n\n    # simple edge map: a pixel differs from any 4-neighbour\n    edge = np.zeros_like(hex2d, bool)\n    edge[:-1] |= hex2d[:-1] != hex2d[1:]\n    edge[1:]  |= hex2d[1:]  != hex2d[:-1]\n    edge[:,:-1]|= hex2d[:,:-1]!= hex2d[:,1:]\n    edge[:,1:] |= hex2d[:,1:] != hex2d[:,:-1]\n    return mask3d, edge\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-07-03T09:15:36.673067Z","iopub.execute_input":"2025-07-03T09:15:36.673330Z","iopub.status.idle":"2025-07-03T09:15:36.690895Z","shell.execute_reply.started":"2025-07-03T09:15:36.673301Z","shell.execute_reply":"2025-07-03T09:15:36.690079Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"### 3. SLIC algorithm","metadata":{}},{"cell_type":"code","source":"from skimage.segmentation import slic\n\nvolume = preprocessed.squeeze().transpose([2,1,0])  # shape: (D, H, W)\nprint(\"Shape volume:\", volume.shape)  # ad es. (D, H, W)\n\n# Parametri SLIC: n_segments definisce quanti supervoxels circa si voglion\nsegments = slic(volume, n_segments=100, compactness=1, start_label=0, max_num_iter=100, channel_axis=None)\n\nprint(\"Mappa supervoxel shape:\", segments.shape)\nn_supervoxels = len(np.unique(segments))\nprint(\"Numero di supervoxels:\", n_supervoxels)\n#for sup in range(n_supervoxels):\n#   print(\"sup_\",sup, \" number of voxels: \", np.sum(np.where(segments == sup)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T09:15:48.712679Z","iopub.execute_input":"2025-07-03T09:15:48.713558Z","iopub.status.idle":"2025-07-03T09:33:17.988176Z","shell.execute_reply.started":"2025-07-03T09:15:48.713532Z","shell.execute_reply":"2025-07-03T09:33:17.987342Z"}},"outputs":[{"name":"stdout","text":"Shape volume: (283, 512, 512)\nMappa supervoxel shape: (283, 512, 512)\nNumero di supervoxels: 108\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# Optional: Save the result\n\nct_img_path = \"input_volume_39.nii.gz\"\nnii = nib.load(ct_img_path)\naffine = nii.affine\n\noutput_img = nib.Nifti1Image(segments.astype(np.int32), affine=np.eye(4))\nnib.save(output_img, 'slic_map.nii.gz')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"visualize_volume(\n    volume=segments,\n    mode='gif',\n    plane='sagittal',\n    step=2, \n    cmap=\"tab20\",\n    gif_filename='fast.gif',\n    interval=80     # faster playback (~12.5 FPS)\n)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install and configure 3D-SEEDS \nhttps://github.com/Zch0414/3d-seeds/blob/master/README.md","metadata":{}},{"cell_type":"code","source":"\"\"\"# resetta l'ambiente di lavoro\n%cd /kaggle/working\n!rm -rf 3d-seeds-compile\n\n# aggiorna repository di sistema e installa gli header di OpenCV\n!apt-get update -qq\n!apt-get install -y -q libopencv-dev libopencv-contrib-dev ninja-build\n\"\"\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"!git clone https://github.com/Zch0414/3d-seeds.git 3d-seeds-compile   # clona in /kaggle/working/3d-seeds-compile\n%cd 3d-seeds-compile\n\"\"\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import pathlib, re\nsetup_py = pathlib.Path(\"setup.py\").read_text()\n\n# puntiamo agli header/librerie appena installati da apt-get\nsetup_py = re.sub(r'OPENCV_INCLUDE_DIRS\\s*=.*',\n                  'OPENCV_INCLUDE_DIRS = \"/usr/include/opencv4\"', setup_py)\nsetup_py = re.sub(r'OPENCV_LIBRARY_DIRS\\s*=.*',\n                  'OPENCV_LIBRARY_DIRS = \"/usr/lib/x86_64-linux-gnu\"', setup_py)\n\npathlib.Path(\"setup.py\").write_text(setup_py)\n\"\"\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"!python setup.py bdist_wheel -q\n!pip install dist/*.whl\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"%cd /kaggle/working/3d-seeds-compile   # ← root della repo\n\n# 1) esporta il path degli header in CPLUS_INCLUDE_PATH (vale per g++)\nimport os, sys, subprocess, shlex\nos.environ[\"CPLUS_INCLUDE_PATH\"] = \"/usr/include/opencv4\"\nos.environ[\"CPATH\"]              = \"/usr/include/opencv4\"    # (copertura per clang)\n\n# 2) idem per i .so (non strettamente necessario, ma sicuro)\nos.environ[\"LIBRARY_PATH\"] = \"/usr/lib/x86_64-linux-gnu\"\n\n# 3) build + install\n!python setup.py bdist_wheel   # stavolta senza -q per vedere il comando completo\n!pip install dist/*.whl\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"%cd /kaggle/working\"\"\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rapid test","metadata":{}},{"cell_type":"code","source":"\"\"\"import numpy as np, python_3d_seeds as sv, time\n\nvol = np.random.rand(64, 64, 64).astype(np.float32)\nD, H, W = vol.shape\n\n# factory: W, H, D, channels, n_segments, hist_bins, λ, block_iter, pixel_iter\nseeds = sv.createSupervoxelSEEDS(W, H, D, 1,\n                                 200,    # n_segments\n                                 15, 2,  # hist_bins, lambda_boundary\n                                 2, 4)   # block_iters, pixel_iters\n\nt0 = time.time()\nseeds.iterate(vol, num_iterations=4)      # ← senza spacing\nlabels = seeds.getLabels()\n\nprint(\"supervoxels:\", np.unique(labels).size,\n      \"| tempo:\", round(time.time() - t0, 3), \"s\")\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import numpy as np, python_3d_seeds as sv, nibabel as nib\nfrom scipy.ndimage import zoom\n\n# ---------- carica volume ----------\nct_img_path = \"input_volume_39.nii.gz\"#join(nnUNet_raw, \"imagesTr\", \"AUTOMI_00039_0000.nii\")\nnii = nib.load(ct_img_path)\nvol = preprocessed.astype(np.float32)          # già normalizzato (-1..1 o 0..1)\nprint(\"Shape iniziale volume preprocessed: \", vol.shape)\n\n# (facoltativo) resampling “morbido” se dz >> dy,dx\ndz,dy,dx = nii.header.get_zooms()\nif dz > 3*dy:                                  # esempio: slice spesse 5 mm\n    factor = (dy/dz, 1.0, 1.0)                 # porta dz≈dy\n    vol = zoom(vol, factor, order=1)\n\n# ---------- cast a uint8 ----------\nvmin, vmax = vol.min(), vol.max()\nvol8 = np.round(255*(vol - vmin)/(vmax - vmin)).astype(np.uint8, copy=False)\nvol8 = np.ascontiguousarray(vol8)              # C-order garantito\n\n# ---------- SEEDS ----------\nD,H,W = vol8.shape\nseeds = sv.createSupervoxelSEEDS(\n        W, H, D, 1,          # width, height, depth, channels\n        200,                 # num_superpixels\n        2,                   # num_levels\n        2,                   # prior (λ bordo)\n        15,                  # histogram_bins\n        True)                # double_step\n\nseeds.iterate(vol8, num_iterations=4)\nlabels = seeds.getLabels()\nprint(\"Supervoxels:\", np.unique(labels).size)\nprint(\"type: \", type(labels))\nprint(\"shape: \", labels.shape)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"output_img = nib.Nifti1Image(labels, affine=nii.affine)\nnib.save(output_img, \"3d-seeds_supervoxel_map.nii.gz\")\"\"\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Face-centered cubic (FCC) lattice induced supervoxel assignment\n-> more *isotropic* than simple cubes","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nfrom scipy.spatial import cKDTree\n\ndef generate_fcc_centers(W, H, D, S):\n    \"\"\"\n    Generate supervoxel centers on an FCC lattice within the volume.\n\n    Parameters:\n    - W, H, D: Volume dimensions (width, height, depth).\n    - S: Spacing parameter (approximate supervoxel size).\n\n    Returns:\n    - centers: Array of [x, y, z] coordinates for supervoxel centers.\n    \"\"\"\n    # Lattice parameter: adjust so nearest-neighbor distance approximates S\n    a = S * np.sqrt(2)  # In FCC, nearest-neighbor distance is a*sqrt(2)/2\n    centers = []\n\n    # Base grid points (where i+j+k is even)\n    for i in range(int(np.ceil(W / a)) + 1):\n        for j in range(int(np.ceil(H / a)) + 1):\n            for k in range(int(np.ceil(D / a)) + 1):\n                if (i + j + k) % 2 == 0:\n                    x, y, z = i * a, j * a, k * a\n                    if 0 <= x < W and 0 <= y < H and 0 <= z < D:\n                        centers.append([x, y, z])\n\n    # Add face-centered points (simplified; add other offsets as needed)\n    for i in range(int(np.ceil((W - a/2) / a)) + 1):\n        for j in range(int(np.ceil((H - a/2) / a)) + 1):\n            for k in range(int(np.ceil(D / a)) + 1):\n                x, y, z = i * a + a/2, j * a + a/2, k * a\n                if 0 <= x < W and 0 <= y < H and 0 <= z < D:\n                    centers.append([x, y, z])\n                # Add other offsets (e.g., (i+0.5, j, k+0.5), (i, j+0.5, k+0.5)) if needed\n\n    return np.array(centers)\n\ndef assign_voxels_to_centers(volume_shape, centers):\n    \"\"\"\n    Assign each voxel to the nearest supervoxel center.\n\n    Parameters:\n    - volume_shape: Tuple (W, H, D) of volume dimensions.\n    - centers: Array of supervoxel center coordinates.\n\n    Returns:\n    - labels: 3D array of integer labels (int32).\n    \"\"\"\n    W, H, D = volume_shape\n    # Create coordinate grid for all voxels\n    grid_x, grid_y, grid_z = np.mgrid[0:W, 0:H, 0:D]\n    voxels = np.vstack([grid_x.ravel(), grid_y.ravel(), grid_z.ravel()]).T\n\n    # Use KD-tree for efficient nearest-neighbor search\n    tree = cKDTree(centers)\n    _, labels = tree.query(voxels)\n\n    # Cast to int32 for compatibility\n    return labels.reshape(W, H, D).astype(np.int32)\n\ndef create_supervoxel_map(nifti_file, S=10):\n    \"\"\"\n    Produce a supervoxel map from a NIfTI CT scan volume.\n\n    Parameters:\n    - nifti_file: Path to the NIfTI file.\n    - S: Spacing parameter for supervoxel size (default=10).\n\n    Returns:\n    - supervoxel_map: 3D array with integer labels for each supervoxel (int32).\n    - affine: Affine matrix from the input NIfTI file.\n    \"\"\"\n    # Load NIfTI volume\n    img = nib.load(nifti_file)\n    volume = img.get_fdata()\n    affine = img.affine  # Get affine matrix\n    W, H, D = volume.shape\n\n    # Generate supervoxel centers\n    centers = generate_fcc_centers(W, H, D, S)\n    if len(centers) == 0:\n        raise ValueError(\"No centers generated. Reduce S or check volume dimensions.\")\n\n    # Assign voxels to supervoxels\n    supervoxel_map = assign_voxels_to_centers((W, H, D), centers)\n\n    return supervoxel_map, affine\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"nifti_file = \"input_volume_39.nii.gz\"\nsupervoxel_map, affine = create_supervoxel_map(nifti_file, S=30)\nprint(f\"Data type: {supervoxel_map.dtype}\")  # Should print int32\nprint(f\"Number of supervoxels: {len(np.unique(supervoxel_map))}\")\noutput_img = nib.Nifti1Image(supervoxel_map, affine=affine)\nnib.save(output_img, \"supervoxel_map.nii.gz\")\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.1 affine transformation to translate isotropy from voxel space into the original geometrical space (measured in mm)","metadata":{}},{"cell_type":"code","source":"\"\"\"import numpy as np\nimport nibabel as nib\nfrom scipy.spatial import cKDTree\n\n# Load the NIfTI image\nimg = nib.load('input_volume_39.nii.gz')\nvolume = img.get_fdata()\naffine = img.affine\nW, H, D = volume.shape\n\n# Compute correct voxel spacings from affine matrix\nvoxel_spacings = np.array([np.linalg.norm(affine[:3, 0]),\n                           np.linalg.norm(affine[:3, 1]),\n                           np.linalg.norm(affine[:3, 2])])\n\n# Set desired supervoxel size in mm\nS = 300\n\n# Compute grid steps in voxel space\nstep_x = max(1, int(np.round(S / voxel_spacings[0])))\nstep_y = max(1, int(np.round(S / voxel_spacings[1])))\nstep_z = max(1, int(np.round(S / voxel_spacings[2])))\n\n# Generate centers in voxel space\ncenters_voxel = []\nfor i in range(0, W, step_x):\n    for j in range(0, H, step_y):\n        for k in range(0, D, step_z):\n            centers_voxel.append([i, j, k])\ncenters_voxel = np.array(centers_voxel)\n\n# Transform to physical space\ncenters_physical = np.dot(affine, np.hstack((centers_voxel, np.ones((len(centers_voxel), 1)))).T).T[:, :3]\n\n# Verify centers were generated\nif len(centers_physical) == 0:\n    raise ValueError(\"No supervoxel centers generated. Check volume dimensions and affine matrix.\")\n\n# Assign voxels to centers using KD-tree\ndef assign_voxels_to_centers(volume_shape, centers_physical, affine):\n    W, H, D = volume_shape\n    # Generate all voxel coordinates\n    grid_x, grid_y, grid_z = np.mgrid[0:W, 0:H, 0:D]\n    voxels_voxel = np.vstack([grid_x.ravel(), grid_y.ravel(), grid_z.ravel(), np.ones(grid_x.size)]).T\n    # Transform to physical space\n    voxels_physical = np.dot(affine, voxels_voxel.T).T[:, :3]\n    # Find nearest center for each voxel\n    tree = cKDTree(centers_physical)\n    _, labels = tree.query(voxels_physical)\n    return labels.reshape(W, H, D).astype(np.int32)\n\n# Generate supervoxel map\nsupervoxel_map = assign_voxels_to_centers((W, H, D), centers_physical, affine)\n\n# Optional: Save the result\noutput_img = nib.Nifti1Image(supervoxel_map, affine)\nnib.save(output_img, 'supervoxel_map.nii.gz')\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#print(affine)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.1 Try to apply the original algorithm FCC it to an affine transformed volume that has the same proportion as the .nii in the physical space. Transform->apply the algorithm to derive the map-> back transform the map onto the voxel space\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nfrom scipy.spatial import cKDTree\n\ndef generate_supervoxel_map(img, S=200.0):\n    \"\"\"\n    Generate a supervoxel map using FCC tessellation in physical space (original version).\n    \n    Args:\n        img: Nibabel NIfTI image object\n        S (float): Desired supervoxel size in millimeters (default: 200.0)\n    \n    Returns:\n        supervoxel_map: 3D NumPy array with integer labels for supervoxels\n    \"\"\"\n    # Load volume and affine from image\n    volume = img.get_fdata()\n    affine = img.affine\n    W, H, D = volume.shape\n\n    # Compute the physical bounding box of the volume\n    corners_voxel = np.array([\n        [0, 0, 0],\n        [W-1, 0, 0],\n        [0, H-1, 0],\n        [0, 0, D-1],\n        [W-1, H-1, 0],\n        [W-1, 0, D-1],\n        [0, H-1, D-1],\n        [W-1, H-1, D-1]\n    ])\n    corners_hom = np.hstack((corners_voxel, np.ones((8, 1))))\n    corners_physical = (affine @ corners_hom.T).T[:, :3]\n    min_xyz = corners_physical.min(axis=0)\n    max_xyz = corners_physical.max(axis=0)\n\n    # Generate FCC lattice centers in physical space\n    a = S * np.sqrt(2)\n    factor = 2 / a\n    p_min = int(np.floor(factor * min_xyz[0])) - 1\n    p_max = int(np.ceil(factor * max_xyz[0])) + 1\n    q_min = int(np.floor(factor * min_xyz[1])) - 1\n    q_max = int(np.ceil(factor * max_xyz[1])) + 1\n    r_min = int(np.floor(factor * min_xyz[2])) - 1\n    r_max = int(np.ceil(factor * max_xyz[2])) + 1\n\n    # Create grid of possible indices\n    p_vals = np.arange(p_min, p_max + 1)\n    q_vals = np.arange(q_min, q_max + 1)\n    r_vals = np.arange(r_min, r_max + 1)\n    P, Q, R = np.meshgrid(p_vals, q_vals, r_vals, indexing='ij')\n    P = P.flatten()\n    Q = Q.flatten()\n    R = R.flatten()\n\n    # Filter for FCC lattice points (sum of indices is even)\n    mask = (P + Q + R) % 2 == 0\n    P = P[mask]\n    Q = Q[mask]\n    R = R[mask]\n\n    # Compute physical coordinates of centers\n    centers = np.column_stack((P * a / 2, Q * a / 2, R * a / 2))\n\n    # Keep only centers within the bounding box\n    inside = ((centers[:, 0] >= min_xyz[0]) & (centers[:, 0] <= max_xyz[0]) &\n              (centers[:, 1] >= min_xyz[1]) & (centers[:, 1] <= max_xyz[1]) &\n              (centers[:, 2] >= min_xyz[2]) & (centers[:, 2] <= max_xyz[2]))\n    centers = centers[inside]\n\n    # Check if any centers were generated\n    print(f\"Number of supervoxel centers: {len(centers)}\")\n    if len(centers) == 0:\n        raise ValueError(\"No supervoxel centers generated. Try reducing S.\")\n\n    # Generate voxel indices and transform to physical coordinates\n    voxel_indices = np.indices((W, H, D)).reshape(3, -1).T  # shape (W*H*D, 3)\n    voxel_indices_hom = np.hstack((voxel_indices, np.ones((voxel_indices.shape[0], 1))))  # shape (W*H*D, 4)\n    physical_coords = (affine @ voxel_indices_hom.T).T[:, :3]  # shape (W*H*D, 3)\n\n    # Assign each voxel to the nearest supervoxel center\n    tree = cKDTree(centers)\n    _, labels = tree.query(physical_coords)\n\n    # Create the supervoxel map\n    supervoxel_map = labels.reshape((W, H, D)).astype(np.int32)\n\n    return supervoxel_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import numpy as np\nimport nibabel as nib\nimport time\n\n# Load the image\nimg = nib.load('input_volume_39.nii.gz')\n\n# Generate and save supervoxel map using the original method with timing\nstart_time = time.time()\nsupervoxel_map = generate_supervoxel_map(img, S=100.0)\nend_time = time.time()\noriginal_time = end_time - start_time\nprint(f\"Execution time: {original_time:.2f} seconds\")\n\noutput_img = nib.Nifti1Image(supervoxel_map, img.affine)\nnib.save(output_img, 'supervoxel_map.nii.gz')\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply modified KernelShap to this supervoxel tessellation","metadata":{}},{"cell_type":"code","source":"# 2) Initialise predictor ------------------------\npredictor = CustomNNUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=False, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=False,\n    verbose_preprocessing=False,\n    allow_tqdm=False #it interfere with SHAP loading bar\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir_readonly,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (a) load + preprocess the volume  (1, C, D, H, W) – nnU-Net order\nnii_path = \"input_volume_39.nii.gz\"\ndataset_json_path = Path(model_dir_readonly) / \"dataset.json\"\n\nvolume_np = nnunetv2_default_preprocessing(nii_path, predictor, dataset_json_path)\n\nvolume = torch.from_numpy(volume_np).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\n\nprint(\"Volume shape:\", volume.shape)                # (1, C, D, H, W)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"# (b) super-voxel / organ-id map  (W, H, D)\n# Load the image\nimg = nib.load('input_volume_39.nii.gz')\n\n# Generate and save supervoxel map using the \nsupervoxel_map = generate_supervoxel_map(img, S=100.0)\n                    # (W, H, D)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\n# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\nsv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\n\nsupervoxel_map = indexes.reshape(supervoxel_map.shape)\nprint(np.unique(supervoxel_map))\n\n# IMPORTANT 🔸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\nsupervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\n\nprint(\"Mask shape:\", supervoxel_map.shape)  \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"print(supervoxel_map.shape)\nprint(np.prod(supervoxel_map.shape))\nprint(torch.sum(supervoxel_map != 0))\nprint(torch.sum(supervoxel_map == 0))\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------\n# ❷  Forward wrapper that nnU-Net expects\n# ------------------------------------------------------------\n\n@torch.inference_mode()\ndef forward_segmentation_output_to_explain(\n        input_image:         torch.Tensor,\n        perturbation_mask:   torch.BoolTensor,\n        baseline_prediction_dict: dict\n) -> torch.Tensor:           # returns a scalar per sample\n\n    logits = predictor.predict_sliding_window_return_logits_with_caching(\n        input_image, perturbation_mask, baseline_prediction_dict,\n    )                              # (C, D, H, W)\n    seg_mask = torch.argmax(logits, dim=0)          # (D,H,W)\n    D, H, W = seg_mask.shape\n    aggregate = torch.sum(logits[1].double() * seg_mask) / (D*H*W)  # scaling to avoid overflows in SHAP\n\n    return aggregate\n\n# c) wrap your cached‐forward method:\nexplainer = KernelShapWithMask(\n    forward_func=lambda vol, mask: forward_segmentation_output_to_explain(\n        input_image=vol,\n        perturbation_mask=mask,\n        baseline_prediction_dict=baseline_pred_cache)\n)\n\n\"\"\"# d) compute SHAP\nattr = explainer.attribute(\n    inputs=volume,       # (1,C,D,H,W)\n    baselines=0.0, \n    feature_mask=supervoxel_map,\n    n_samples=1000,    \n    return_input_shape=True,\n    show_progress=True,\n)\nprint(\"Attributions:\", attr.shape)  # → (1,C,D,H,W)\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"for x,y,_ in explainer.dataset:\n    print(x)\n    break\n    \nformatted_inputs, baselines = _format_input_baseline(inputs=volume, baselines=0.0)\nfeature_mask, num_interp_features = construct_feature_mask(\n            feature_mask=supervoxel_map, formatted_inputs=formatted_inputs\n        )\nprint(num_interp_features)\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Move back attributions to physical space and save the nifty","metadata":{}},{"cell_type":"code","source":"\"\"\"affine = nib.load(nii_path).affine\nattr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\nattr_img = nib.Nifti1Image(attr_postprocessed, affine)\nnib.save(attr_img, 'attribution_map-FCC.nii.gz')\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"derive attribution module, ignoring sign","metadata":{}},{"cell_type":"code","source":"\"\"\"attr_postprocessed_module = np.abs(attr_postprocessed)\nattr_img = nib.Nifti1Image(attr_postprocessed_module, affine)\nnib.save(attr_img, 'attribution_map-FCC_module.nii.gz')\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define a ROI to explain segmentation in. \nMaybe this will provide a more useful attribution map, highlighting nearby organs","metadata":{}},{"cell_type":"code","source":"# get the manually derived ROI mask from the dataset, where we manually added it\nROI_mask_path = \"/kaggle/input/automi-seg/segmentation-masked-ROI.nii\"\nROI_mask = nib.load(ROI_mask_path)\n\nprint(ROI_mask.get_fdata().shape)\nprint(ROI_mask.affine)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ROI mask is a binary mask highlighting the lymphnodes of interest. We need a bounding box to crop the volume accordingly","metadata":{}},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\n\ndef get_mask_bbox_slices(mask_nii_path):\n    \"\"\"\n    Load a binary ROI mask NIfTI and compute the minimal 3D bounding\n    box slices containing all positive voxels.\n\n    Parameters\n    ----------\n    mask_nii_path : str or Path\n        Path to the input binary ROI mask NIfTI (.nii or .nii.gz).\n\n    Returns\n    -------\n    bbox_slices : tuple of slice\n        A 3-tuple of Python slice objects (x_slice, y_slice, z_slice)\n        defining the minimal bounding box.\n    \"\"\"\n    # 1) Load mask\n    nii = nib.load(str(mask_nii_path))\n    data = nii.get_fdata()\n    if data.ndim != 3:\n        raise ValueError(\"Input NIfTI must be a 3D volume\")\n    \n    # 2) Find indices of positive voxels\n    pos_voxels = np.argwhere(data > 0)\n    if pos_voxels.size == 0:\n        raise ValueError(\"No positive voxels found in mask\")\n    \n    # 3) Compute min/max per axis\n    x_min, y_min, z_min = pos_voxels.min(axis=0)\n    x_max, y_max, z_max = pos_voxels.max(axis=0)\n    \n    # 4) Build slice objects (end is exclusive, hence +1)\n    bbox_slices = (\n        slice(int(x_min), int(x_max) + 1),\n        slice(int(y_min), int(y_max) + 1),\n        slice(int(z_min), int(z_max) + 1),\n    )\n    \n    return bbox_slices","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_slice, y_slice, z_slice = get_mask_bbox_slices(ROI_mask_path)\nprint(\"Bounding box slices:\")\nprint(\"  x:\", x_slice)\nprint(\"  y:\", y_slice)\nprint(\"  z:\", z_slice)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### to **crop** correctly the volume around the *ROI*, we need to derive the **receptive field** of the sliding window inference, that depends on the *patch size*.","metadata":{}},{"cell_type":"code","source":"patch_size = np.array(predictor.configuration_manager.patch_size)\nprint(\"Patch size: \", patch_size)\n\n# Receptive field is twice the patch size-1\nRF = 2*(patch_size-1)\nprint(\"Receptive field: \", RF)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Consider the receptive field to compute the final slices for cropping\nRemember that model metadata are related to transposed volume (nnunetv2 takes (D, H, W) shape)","metadata":{}},{"cell_type":"code","source":"volume_path = \"input_volume_39.nii.gz\"\nvolume_shape = nib.load(volume_path).shape # (W, H, D) -> x, y, z\n# RF shape -> (D, H, W) -> z, y, x (model input shape)\nprint(\"Original volume shape:\", volume_shape)\n\nW, H, D = volume_shape\n\n# backward sorted receptive field axes\nRF_x, RF_y, RF_z = RF[2],RF[1],RF[0]\n\nx_slice_RF = slice(int(max(x_slice.start - RF_x/2, 0)), int(min(x_slice.stop + RF_x/2, W)))\ny_slice_RF = slice(int(max(y_slice.start - RF_y/2, 0)), int(min(y_slice.stop + RF_y/2, H)))\nz_slice_RF = slice(int(max(z_slice.start - RF_z/2, 0)), int(min(z_slice.stop + RF_z/2, D)))\n\nprint(\"new  x:\", x_slice_RF)\nprint(\"new  y:\", y_slice_RF)\nprint(\"new  z:\", z_slice_RF)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\n\ndef crop_volume_and_affine(nii_path, bbox_slices, save_cropped_nii_path=None):\n    \"\"\"\n    Crop a 3D NIfTI volume using the given bounding-box slices and\n    recompute the affine so the cropped volume retains correct world coordinates.\n\n    Parameters\n    ----------\n    nii_path : str or Path\n        Path to the input NIfTI volume (.nii or .nii.gz).\n    bbox_slices : tuple of slice\n        A 3-tuple (x_slice, y_slice, z_slice) as returned by get_mask_bbox_slices().\n    save_cropped_nii_path : str or Path, optional\n        If provided, the cropped volume will be saved here as a new NIfTI.\n\n    Returns\n    -------\n    cropped_data : np.ndarray\n        The volume data cropped to the bounding box.\n    new_affine : np.ndarray\n        The updated 4×4 affine transform for the cropped volume.\n    \"\"\"\n    # 1) Load the original image\n    img = nib.load(str(nii_path))\n    data = img.get_fdata()\n    affine = img.affine\n\n    # 2) Crop the data array\n    cropped_data = data[bbox_slices]\n\n    # 3) Extract the voxel‐offsets for x, y, z from the slice starts\n    x_slice, y_slice, z_slice = bbox_slices\n    z0, y0, x0 = z_slice.start, y_slice.start, x_slice.start\n\n    # 4) Compute the new affine translation: shift the origin by the voxel offsets\n    # Note voxel coordinates are (i, j, k) = (x, y, z)\n    offset_vox = np.array([x0, y0, z0])\n    new_affine = affine.copy()\n    new_affine[:3, 3] += affine[:3, :3].dot(offset_vox)\n\n    # 5) Optionally save the cropped volume\n    if save_cropped_nii_path is not None:\n        cropped_img = nib.Nifti1Image(cropped_data, new_affine)\n        nib.save(cropped_img, str(save_cropped_nii_path))\n\n    return cropped_data, new_affine\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\nslices = (x_slice_RF, y_slice_RF, z_slice_RF)\n\nnii_path = \"input_volume_39.nii.gz\"\n\ncropped_volume, affine_cropped_volume = crop_volume_and_affine(\n    nii_path=nii_path,\n    bbox_slices=slices,\n    save_cropped_nii_path=Path(\"cropped_volume_with_RF.nii.gz\")\n)\n\nprint(\"Cropped data shape:\", cropped_volume.shape)\nprint(\"New affine:\\n\", affine_cropped_volume)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We need a way to check original mask overlapping in the new cropped volume","metadata":{}},{"cell_type":"code","source":"cropped_mask, affine_cropped_mask = crop_volume_and_affine(\n    nii_path=ROI_mask_path,\n    bbox_slices=slices,\n    save_cropped_nii_path=Path(\"cropped_mask_with_RF.nii.gz\")\n)\n\nprint(\"Cropped mask shape:\", cropped_mask.shape)\nprint(\"New mask affine:\\n\", affine_cropped_mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## We execute SHAP on this cropped image, and we only consider our ROI","metadata":{}},{"cell_type":"code","source":"# 2) Initialise predictor ------------------------\npredictor = CustomNNUNetPredictor(\n    tile_step_size=0.5,\n    use_gaussian=True,\n    use_mirroring=False, # == test time augmentation\n    perform_everything_on_device=True,\n    device=torch.device('cuda', 0),\n    verbose=False,\n    verbose_preprocessing=False,\n    allow_tqdm=False #it interfere with SHAP loading bar\n)\n# initializes the network architecture, loads the checkpoint\npredictor.initialize_from_trained_model_folder(\n    model_dir_readonly,\n    use_folds=(0,),\n    checkpoint_name='checkpoint_final.pth',\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (a) load + cropped volume  (1, C, D, H, W) – nnU-Net order\nnii_path_cropped = \"cropped_volume_with_RF.nii.gz\"\ndataset_json_path = Path(model_dir_readonly) / \"dataset.json\"\n\nvolume_np = nnunetv2_default_preprocessing(nii_path_cropped, predictor, dataset_json_path)\n\nvolume = torch.from_numpy(volume_np).unsqueeze(0).to(device)        # torch (1,C,D,H,W)\n\nprint(\"Volume shape:\", volume.shape)                # (1, C, D, H, W)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (b) super-voxel / organ-id map  (W, H, D)\n# Load the image\nimg = nib.load(nii_path_cropped)\n\n# Generate and save supervoxel map using the \nsupervoxel_map = generate_supervoxel_map(img, S=100.0)\n                    # (D, H, W)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"supervoxel_map_path = 'FCC-supervoxel_map.nii.gz'\nsupervoxel_map_img = nib.Nifti1Image(supervoxel_map, affine_cropped_mask)\nnib.save(supervoxel_map_img, supervoxel_map_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"supervoxel_map = np.transpose(supervoxel_map, (2, 1, 0))                 # match (D,H,W)\n# we need features of feature mask ordered from 0 (or 1) to M-1 (M)\nsv_values, indexes = np.unique(supervoxel_map, return_inverse=True)\n\nsupervoxel_map = indexes.reshape(supervoxel_map.shape)\nprint(np.unique(supervoxel_map))\n\n# IMPORTANT 🔸: KernelShapWithMask expects **(X, Y, Z)** without channel axis\nsupervoxel_map = torch.from_numpy(supervoxel_map).long().to(device)   # (D,H,W)\n\nprint(\"Mask shape:\", supervoxel_map.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### derive baseline cached dictionary\nusing planner, iterator, predictor (just temporary solution)","metadata":{}},{"cell_type":"code","source":"import pickle as pkl\n\nwith open(\"baseline_output_dictionary_cache.pkl\", \"rb\") as f:\n    cropped_baseline_pred_cache = pkl.load(f)\n\n\"\"\"# temporarily use a new planner and iterator\n\nplan_tmp = OrganMaskPerturbationPlanner(\n    volume_file=nii_path_cropped,\n    organ_mask_file=supervoxel_map_path,\n)\n\n# code to get our baseline prediction patch dictionary\niterator_tmp = SHAPPredictionIterator(plan_tmp, predictor,\n                                               skip_completed_ids=acc.completed_ids,\n                                               cache_sw_inference=True,\n                                               verbose=True)\ncropped_baseline_pred_cache = iterator_tmp.get_orig_image_output_dictionary()\nimport pickle as pkl\n# Write to file\nwith open(\"cropped_baseline_output_dictionary_cache.pkl\", \"wb\") as f:\n    pkl.dump(cropped_baseline_pred_cache, f)\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(cropped_baseline_pred_cache[(None, None, None), (0, 72, None), (0, 160, None), (0, 160, None)].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### get our cropped ROI mask","metadata":{}},{"cell_type":"code","source":"ROI_mask = np.transpose(cropped_mask, (2, 1, 0))\nROI_mask = torch.from_numpy(ROI_mask).to(device)\n\nprint(ROI_mask.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Include masking in the forward function","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------------------\n# ❷  Forward wrapper that nnU-Net expects\n# ------------------------------------------------------------\n\n@torch.inference_mode()\ndef forward_segmentation_output_to_explain(\n        input_image:         torch.Tensor,\n        perturbation_mask:   torch.BoolTensor,\n        ROI_mask:            torch.Tensor,   # remember that must be cropped to the same size of the other tensors\n        baseline_prediction_dict: dict\n) -> torch.Tensor:           # returns a scalar per sample\n    \"\"\"\n    Example aggregate: sum of lymph-node logits (class 1) in the mask produced\n    by the network – adapt to your real metric as needed.\n    \"\"\"\n    logits = predictor.predict_sliding_window_return_logits_with_caching(\n        input_image, perturbation_mask, baseline_prediction_dict,\n    )                              # (C, D, H, W)\n    # we now mask both by the segmentation prevalent class, and by ROI\n    seg_mask = torch.argmax(logits, dim=0)          # (D,H,W)\n    seg_mask = seg_mask * ROI_mask\n    D, H, W = seg_mask.shape\n    aggregate = torch.sum(logits[1].double() * seg_mask) / (D*H*W)  # scaling to avoid overflows in SHAP\n\n    return aggregate\n\n# c) wrap your cached‐forward method:\nexplainer = KernelShapWithMask(\n    forward_func=lambda vol, mask: forward_segmentation_output_to_explain(\n        input_image=vol,\n        perturbation_mask=mask,\n        ROI_mask=ROI_mask,\n        baseline_prediction_dict=cropped_baseline_pred_cache)\n)\n\n\"\"\"# d) compute SHAP\nattr = explainer.attribute(\n    inputs=volume,       # (1,C,D,H,W)\n    baselines=0.0, \n    feature_mask=supervoxel_map,\n    n_samples=1000,    \n    return_input_shape=True,\n    show_progress=True,\n)\nprint(\"Attributions:\", attr.shape)  # → (1,C,D,H,W)\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"attr_postprocessed = attr[0][0].detach().cpu().numpy().transpose(2,1,0) # (W, H, D)\nattr_img = nib.Nifti1Image(attr_postprocessed, affine_cropped_volume)\nnib.save(attr_img, 'attribution_map-FCC-ROI.nii.gz')\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}